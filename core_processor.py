# core_processor.py

import os
import json
import time
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
import shutil
import threading
from datetime import datetime, timezone
import time as time_module
import psycopg2
# ç¡®ä¿æ‰€æœ‰ä¾èµ–éƒ½å·²æ­£ç¡®å¯¼å…¥
import handler.emby as emby
import handler.tmdb as tmdb
from tasks.helpers import parse_full_asset_details
import utils
import constants
import logging
import actor_utils
from database.actor_db import ActorDBManager
from database.log_db import LogDBManager
from database.connection import get_db_connection as get_central_db_connection
from cachetools import TTLCache
from ai_translator import AITranslator
from utils import translate_country_list, get_unified_rating
from watchlist_processor import WatchlistProcessor
from handler.douban import DoubanApi

logger = logging.getLogger(__name__)
try:
    from handler.douban import DoubanApi
    DOUBAN_API_AVAILABLE = True
except ImportError:
    DOUBAN_API_AVAILABLE = False
    class DoubanApi:
        def __init__(self, *args, **kwargs): pass
        def get_acting(self, *args, **kwargs): return {}
        def close(self): pass

def _read_local_json(file_path: str) -> Optional[Dict[str, Any]]:
    if not os.path.exists(file_path):
        logger.warning(f"æœ¬åœ°å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"è¯»å–æœ¬åœ°JSONæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return None

def _aggregate_series_cast_from_tmdb_data(series_data: Dict[str, Any], all_episodes_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ã€æ–°ã€‘ä»å†…å­˜ä¸­çš„TMDBæ•°æ®èšåˆä¸€ä¸ªå‰§é›†çš„æ‰€æœ‰æ¼”å‘˜ã€‚
    """
    logger.debug(f"ã€æ¼”å‘˜èšåˆã€‘å¼€å§‹ä¸º '{series_data.get('name')}' ä»å†…å­˜ä¸­çš„TMDBæ•°æ®èšåˆæ¼”å‘˜...")
    aggregated_cast_map = {}

    # 1. ä¼˜å…ˆå¤„ç†ä¸»å‰§é›†çš„æ¼”å‘˜åˆ—è¡¨
    main_cast = series_data.get("credits", {}).get("cast", [])
    for actor in main_cast:
        actor_id = actor.get("id")
        if actor_id:
            aggregated_cast_map[actor_id] = actor
    logger.debug(f"  âœ ä»ä¸»å‰§é›†æ•°æ®ä¸­åŠ è½½äº† {len(aggregated_cast_map)} ä½ä¸»æ¼”å‘˜ã€‚")

    # 2. èšåˆæ‰€æœ‰åˆ†é›†çš„æ¼”å‘˜å’Œå®¢ä¸²æ¼”å‘˜
    for episode_data in all_episodes_data:
        credits_data = episode_data.get("credits", {})
        actors_to_process = credits_data.get("cast", []) + credits_data.get("guest_stars", [])
        
        for actor in actors_to_process:
            actor_id = actor.get("id")
            if actor_id and actor_id not in aggregated_cast_map:
                if 'order' not in actor:
                    actor['order'] = 999  # ä¸ºå®¢ä¸²æ¼”å‘˜è®¾ç½®é«˜orderå€¼
                aggregated_cast_map[actor_id] = actor

    full_aggregated_cast = list(aggregated_cast_map.values())
    full_aggregated_cast.sort(key=lambda x: x.get('order', 999))
    
    logger.info(f"  âœ å…±ä¸º '{series_data.get('name')}' èšåˆäº† {len(full_aggregated_cast)} ä½ç‹¬ç«‹æ¼”å‘˜ã€‚")
    return full_aggregated_cast
class MediaProcessor:
    def __init__(self, config: Dict[str, Any]):
        # â˜…â˜…â˜… ç„¶åï¼Œä»è¿™ä¸ª config å­—å…¸é‡Œï¼Œè§£æå‡ºæ‰€æœ‰éœ€è¦çš„å±æ€§ â˜…â˜…â˜…
        self.config = config

        # åˆå§‹åŒ–æˆ‘ä»¬çš„æ•°æ®åº“ç®¡ç†å‘˜
        self.actor_db_manager = ActorDBManager()
        self.log_db_manager = LogDBManager()

        # ä» config ä¸­è·å–æ‰€æœ‰å…¶ä»–é…ç½®
        self.douban_api = None
        if getattr(constants, 'DOUBAN_API_AVAILABLE', False):
            try:
                # --- âœ¨âœ¨âœ¨ æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ START âœ¨âœ¨âœ¨ ---

                # 1. ä»é…ç½®ä¸­è·å–å†·å´æ—¶é—´ 
                douban_cooldown = self.config.get(constants.CONFIG_OPTION_DOUBAN_DEFAULT_COOLDOWN, 2.0)
                
                # 2. ä»é…ç½®ä¸­è·å– Cookieï¼Œä½¿ç”¨æˆ‘ä»¬åˆšåˆšåœ¨ constants.py ä¸­å®šä¹‰çš„å¸¸é‡
                douban_cookie = self.config.get(constants.CONFIG_OPTION_DOUBAN_COOKIE, "")
                
                # 3. æ·»åŠ ä¸€ä¸ªæ—¥å¿—ï¼Œæ–¹ä¾¿è°ƒè¯•
                if not douban_cookie:
                    logger.debug(f"é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æˆ–æœªè®¾ç½® '{constants.CONFIG_OPTION_DOUBAN_COOKIE}'ã€‚å¦‚æœè±†ç“£APIè¿”å›'need_login'é”™è¯¯ï¼Œè¯·é…ç½®è±†ç“£cookieã€‚")
                else:
                    logger.debug("å·²ä»é…ç½®ä¸­åŠ è½½è±†ç“£ Cookieã€‚")

                # 4. å°†æ‰€æœ‰å‚æ•°ä¼ é€’ç»™ DoubanApi çš„æ„é€ å‡½æ•°
                self.douban_api = DoubanApi(
                    cooldown_seconds=douban_cooldown,
                    user_cookie=douban_cookie  # <--- å°† cookie ä¼ è¿›å»
                )
                logger.trace("DoubanApi å®ä¾‹å·²åœ¨ MediaProcessorAPI ä¸­åˆ›å»ºã€‚")
                
                # --- âœ¨âœ¨âœ¨ æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ END âœ¨âœ¨âœ¨ ---

            except Exception as e:
                logger.error(f"MediaProcessorAPI åˆå§‹åŒ– DoubanApi å¤±è´¥: {e}", exc_info=True)
        else:
            logger.warning("DoubanApi å¸¸é‡æŒ‡ç¤ºä¸å¯ç”¨ï¼Œå°†ä¸ä½¿ç”¨è±†ç“£åŠŸèƒ½ã€‚")
        self.emby_url = self.config.get("emby_server_url")
        self.emby_api_key = self.config.get("emby_api_key")
        self.emby_user_id = self.config.get("emby_user_id")
        self.tmdb_api_key = self.config.get("tmdb_api_key", "")
        self.local_data_path = self.config.get("local_data_path", "").strip()
        
        self.ai_enabled = self.config.get("ai_translation_enabled", False)
        self.ai_translator = AITranslator(self.config) if self.ai_enabled else None
        
        self._stop_event = threading.Event()
        self.processed_items_cache = self._load_processed_log_from_db()
        self.manual_edit_cache = TTLCache(maxsize=10, ttl=600)
        logger.trace("æ ¸å¿ƒå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆã€‚")
    # --- æ›´æ–°åª’ä½“å…ƒæ•°æ®ç¼“å­˜ ---
    def _upsert_media_metadata(
        self,
        cursor: psycopg2.extensions.cursor,
        item_type: str,
        final_processed_cast: List[Dict[str, Any]],
        source_data_package: Optional[Dict[str, Any]],
        item_details_from_emby: Optional[Dict[str, Any]] = None,
        douban_rating: Optional[float] = None
    ):
        """
        - å®æ—¶å…ƒæ•°æ®å†™å…¥ã€‚
        """
        if not item_details_from_emby:
            logger.error("  âœ å†™å…¥å…ƒæ•°æ®ç¼“å­˜å¤±è´¥ï¼šç¼ºå°‘ Emby è¯¦æƒ…æ•°æ®ã€‚")
            return
        def get_representative_runtime(emby_items, tmdb_runtime):
            if not emby_items:
                return tmdb_runtime
            
            # æ”¶é›†æ‰€æœ‰ç‰ˆæœ¬çš„æ—¶é•¿
            runtimes = []
            for item in emby_items:
                if item.get('RunTimeTicks'):
                    runtimes.append(round(item['RunTimeTicks'] / 600000000))
            
            # å¦‚æœæœ‰ Emby æ•°æ®ï¼Œå–æœ€å¤§å€¼ï¼ˆé€šå¸¸å¤§å®¶å¸Œæœ›çœ‹åˆ°åŠ é•¿ç‰ˆ/å¯¼æ¼”å‰ªè¾‘ç‰ˆçš„æ—¶é•¿ï¼‰
            if runtimes:
                return max(runtimes)
            
            # å…œåº•
            return tmdb_runtime
        try:
            from psycopg2.extras import execute_batch
            
            if not source_data_package:
                logger.warning("  âœ å…ƒæ•°æ®å†™å…¥è·³è¿‡ï¼šæœªæä¾›æºæ•°æ®åŒ…ã€‚")
                return

            records_to_upsert = []
            
            if item_type == "Movie":
                movie_record = source_data_package.copy()
                movie_record['item_type'] = 'Movie'
                movie_record['tmdb_id'] = str(movie_record.get('id'))
                final_runtime = get_representative_runtime([item_details_from_emby], movie_record.get('runtime'))
                movie_record['runtime_minutes'] = final_runtime
                actors_relation = [{"tmdb_id": int(p.get("id")), "character": p.get("character"), "order": p.get("order")} for p in final_processed_cast if p.get("id")]
                movie_record['actors_json'] = json.dumps(actors_relation, ensure_ascii=False)
                if douban_rating is not None: movie_record['rating'] = douban_rating
                movie_record['in_library'] = True
                movie_record['subscription_status'] = 'NONE'
                movie_record['emby_item_ids_json'] = json.dumps([item_details_from_emby.get('Id')])
                movie_record['date_added'] = item_details_from_emby.get("DateCreated")
                movie_record['ignore_reason'] = None
                asset_details = parse_full_asset_details(item_details_from_emby)
                movie_record['asset_details_json'] = json.dumps([asset_details], ensure_ascii=False)
                
                records_to_upsert.append(movie_record)

            elif item_type == "Series":
                # â˜…â˜…â˜…  1: æ¢å¤ TMDb å…ƒæ•°æ®åˆ—è¡¨çš„å®šä¹‰ â˜…â˜…â˜…
                series_details = source_data_package.get("series_details", source_data_package)
                seasons_details = source_data_package.get("seasons_details", series_details.get("seasons", []))
                episodes_details = list(source_data_package.get("episodes_details", {}).values()) # <--- æ¢å¤è¿™è¡Œ

                # â˜…â˜…â˜…  2: è·å–å¹¶é¢„å¤„ç†æ‰€æœ‰ Emby åˆ†é›†æ–‡ä»¶ç‰ˆæœ¬ â˜…â˜…â˜…
                emby_episode_versions = []
                series_id = item_details_from_emby.get('Id')
                logger.info(f"  âœ æ­£åœ¨ä¸ºå‰§é›† '{item_details_from_emby.get('Name')}' è·å–æ‰€æœ‰åˆ†é›†æ–‡ä»¶ç‰ˆæœ¬...")
                emby_episode_versions = emby.get_all_library_versions(
                    base_url=self.emby_url, api_key=self.emby_api_key, user_id=self.emby_user_id,
                    media_type_filter="Episode", parent_id=series_id,
                    fields="Id,Type,ParentIndexNumber,IndexNumber,MediaStreams,Container,Size,Path,ProviderIds,RunTimeTicks"
                ) or []

                episodes_grouped_by_number = defaultdict(list)
                for ep_version in emby_episode_versions:
                    s_num = ep_version.get("ParentIndexNumber")
                    e_num = ep_version.get("IndexNumber")
                    if s_num is not None and e_num is not None:
                        episodes_grouped_by_number[(s_num, e_num)].append(ep_version)
                
                # ... (æ„å»º series_record ) ...
                series_record = {
                    "item_type": "Series", "tmdb_id": str(series_details.get('id')), "title": series_details.get('name'),
                    "original_title": series_details.get('original_name'), "overview": series_details.get('overview'),
                    "release_date": series_details.get('first_air_date'), "poster_path": series_details.get('poster_path'),
                    "rating": douban_rating if douban_rating is not None else series_details.get('vote_average'),
                    "asset_details_json": '[]'
                }
                actors_relation = [{"tmdb_id": int(p.get("id")), "character": p.get("character"), "order": p.get("order")} for p in final_processed_cast if p.get("id")]
                series_record['actors_json'] = json.dumps(actors_relation, ensure_ascii=False)
                tmdb_official_rating = None
                if series_details.get('content_ratings', {}).get('results'):
                    for cr in series_details['content_ratings']['results']:
                        if cr.get('iso_3166_1') == 'US' and cr.get('rating'):
                            tmdb_official_rating = cr['rating']; break
                series_record['official_rating'] = tmdb_official_rating
                series_record['unified_rating'] = get_unified_rating(tmdb_official_rating)
                series_record['genres_json'] = json.dumps([g['name'] for g in series_details.get('genres', [])], ensure_ascii=False)
                series_record['studios_json'] = json.dumps([s['name'] for s in series_details.get('production_companies', [])], ensure_ascii=False)
                series_record['directors_json'] = json.dumps([{'id': c.get('id'), 'name': c.get('name')} for c in series_details.get('created_by', [])], ensure_ascii=False)
                series_record['countries_json'] = json.dumps(translate_country_list(series_details.get('origin_country', [])), ensure_ascii=False)
                keywords_data = series_details.get('keywords', {})
                keywords = [k['name'] for k in (keywords_data.get('keywords', []) or keywords_data.get('results', []))]
                series_record['keywords_json'] = json.dumps(keywords, ensure_ascii=False)
                languages_list = series_details.get('languages', [])
                series_record['original_language'] = languages_list[0] if languages_list else None
                series_record['in_library'] = True
                series_record['subscription_status'] = 'NONE'
                series_record['emby_item_ids_json'] = json.dumps([item_details_from_emby.get('Id')])
                series_record['date_added'] = item_details_from_emby.get("DateCreated")
                series_record['ignore_reason'] = None
                records_to_upsert.append(series_record)

                for season in seasons_details:
                    if season.get('season_number', 0) == 0: continue
                    
                    # æ²¡æœ‰å­£æµ·æŠ¥å°±ç”¨çˆ¶å‰§æµ·æŠ¥
                    season_poster = season.get('poster_path')
                    if not season_poster:
                        season_poster = series_details.get('poster_path')

                    records_to_upsert.append({
                        "tmdb_id": str(season.get('id')), 
                        "item_type": "Season", 
                        "parent_series_tmdb_id": str(series_details.get('id')), 
                        "title": season.get('name'), 
                        "overview": season.get('overview'), 
                        "release_date": season.get('air_date'), 
                        "poster_path": season_poster, 
                        "season_number": season.get('season_number')
                    })
                
                # â˜…â˜…â˜…  éå† TMDb å…ƒæ•°æ®åˆ—è¡¨ï¼Œå¹¶ä»ä¸­æŸ¥æ‰¾ Emby ç‰ˆæœ¬è¿›è¡Œèšåˆ â˜…â˜…â˜…
                for episode in episodes_details:
                    # 1. å…ˆæå–å­£å·å’Œé›†å· (æåˆ°å‰é¢æ¥)
                    s_num = episode.get('season_number')
                    e_num = episode.get('episode_number')
                    
                    # 2. å…ˆæŸ¥æ‰¾ Emby ç‰ˆæœ¬ (æåˆ°å‰é¢æ¥)
                    versions_of_episode = episodes_grouped_by_number.get((s_num, e_num))

                    # 3. â˜…â˜…â˜… è®¡ç®—æ—¶é•¿é€»è¾‘ (Emby > TMDB) â˜…â˜…â˜…
                    # é»˜è®¤ä½¿ç”¨ TMDB çš„æ—¶é•¿
                    final_runtime = episode.get('runtime')
                    
                    # å¦‚æœæ‰¾åˆ°äº† Emby æ–‡ä»¶ï¼Œä¸”æ–‡ä»¶é‡Œæœ‰çœŸå®æ—¶é•¿ï¼Œåˆ™è¦†ç›–
                    if versions_of_episode:
                        emby_data = versions_of_episode[0]
                        if emby_data.get('RunTimeTicks'):
                            # 1åˆ†é’Ÿ = 600,000,000 Ticks
                            final_runtime = round(emby_data['RunTimeTicks'] / 600000000)

                    final_runtime = get_representative_runtime(versions_of_episode, episode.get('runtime'))

                    # 4. ç°åœ¨å†åˆ›å»ºè®°å½•ï¼ŒæŠŠç®—å¥½çš„ final_runtime æ”¾è¿›å»
                    episode_record = {
                        "tmdb_id": str(episode.get('id')), 
                        "item_type": "Episode", 
                        "parent_series_tmdb_id": str(series_details.get('id')), 
                        "title": episode.get('name'), 
                        "overview": episode.get('overview'), 
                        "release_date": episode.get('air_date'), 
                        "season_number": s_num, 
                        "episode_number": e_num,
                        "runtime_minutes": final_runtime
                    }
                    
                    # 5. è¡¥å…… Emby èµ„äº§ä¿¡æ¯ (é€»è¾‘ä¸å˜)
                    if versions_of_episode:
                        all_emby_ids = [v.get('Id') for v in versions_of_episode]
                        all_asset_details = [parse_full_asset_details(v) for v in versions_of_episode]
                        
                        episode_record['asset_details_json'] = json.dumps(all_asset_details, ensure_ascii=False)
                        episode_record['emby_item_ids_json'] = json.dumps(all_emby_ids)
                        episode_record['in_library'] = True

                    records_to_upsert.append(episode_record)

            if not records_to_upsert:
                return

            # ... (æ‰¹é‡å†™å…¥æ•°æ®åº“) ...
            all_possible_columns = [
                "tmdb_id", "item_type", "title", "original_title", "overview", "release_date", "release_year",
                "original_language",
                "poster_path", "rating", "actors_json", "parent_series_tmdb_id", "season_number", "episode_number",
                "in_library", "subscription_status", "subscription_sources_json", "emby_item_ids_json", "date_added",
                "official_rating", "unified_rating",
                "genres_json", "directors_json", "studios_json", "countries_json", "keywords_json", "ignore_reason",
                "asset_details_json",
                "runtime_minutes"
            ]
            data_for_batch = []
            for record in records_to_upsert:
                db_row_complete = {col: record.get(col) for col in all_possible_columns}
                
                if db_row_complete['in_library'] is None: db_row_complete['in_library'] = False
                if db_row_complete['subscription_status'] is None: db_row_complete['subscription_status'] = 'NONE'
                if db_row_complete['subscription_sources_json'] is None: db_row_complete['subscription_sources_json'] = '[]'
                if db_row_complete['emby_item_ids_json'] is None: db_row_complete['emby_item_ids_json'] = '[]'

                release_date_str = db_row_complete.get('release_date')
                if release_date_str and len(release_date_str) >= 4:
                    try: db_row_complete['release_year'] = int(release_date_str[:4])
                    except (ValueError, TypeError): pass
                if record.get('item_type') == 'Movie':
                    tmdb_official_rating = None
                    if record.get('release_dates', {}).get('results'):
                        for rd in record['release_dates']['results']:
                            if rd.get('iso_3166_1') == 'US':
                                for release in rd.get('release_dates', []):
                                    if release.get('certification'):
                                        tmdb_official_rating = release['certification']; break
                            if tmdb_official_rating: break
                    db_row_complete['official_rating'] = tmdb_official_rating
                    db_row_complete['unified_rating'] = get_unified_rating(tmdb_official_rating)
                    db_row_complete['genres_json'] = json.dumps([g['name'] for g in record.get('genres', [])], ensure_ascii=False)
                    db_row_complete['studios_json'] = json.dumps([s['name'] for s in record.get('production_companies', [])], ensure_ascii=False)
                    crew = record.get("credits", {}).get('crew', [])
                    db_row_complete['directors_json'] = json.dumps([{'id': p.get('id'), 'name': p.get('name')} for p in crew if p.get('job') == 'Director'], ensure_ascii=False)
                    db_row_complete['countries_json'] = json.dumps(translate_country_list([c.get('iso_3166_1') for c in record.get('production_countries', [])]), ensure_ascii=False)
                    keywords_data = record.get('keywords', {})
                    keywords = [k['name'] for k in (keywords_data.get('keywords', {}) or keywords_data.get('results', []))]
                    db_row_complete['keywords_json'] = json.dumps(keywords, ensure_ascii=False)
                    db_row_complete['original_language'] = record.get('original_language')
                data_for_batch.append(db_row_complete)

            cols_str = ", ".join(all_possible_columns)
            placeholders_str = ", ".join([f"%({col})s" for col in all_possible_columns])
            cols_to_update = [col for col in all_possible_columns if col not in ['tmdb_id', 'item_type']]
            
            cols_to_protect = ['subscription_sources_json']
            timestamp_field = "last_synced_at"
            
            for col in cols_to_protect:
                if col in cols_to_update: cols_to_update.remove(col)

            update_clauses = [f"{col} = EXCLUDED.{col}" for col in cols_to_update]
            update_clauses.append(f"{timestamp_field} = NOW()")

            sql = f"""
                INSERT INTO media_metadata ({cols_str})
                VALUES ({placeholders_str})
                ON CONFLICT (tmdb_id, item_type) DO UPDATE SET {', '.join(update_clauses)};
            """
            
            execute_batch(cursor, sql, data_for_batch)
            logger.info(f"  âœ æˆåŠŸå°† {len(data_for_batch)} æ¡å±‚çº§å…ƒæ•°æ®è®°å½•æ‰¹é‡å†™å…¥æ•°æ®åº“ã€‚")

        except Exception as e:
            logger.error(f"æ‰¹é‡å†™å…¥å±‚çº§å…ƒæ•°æ®åˆ°æ•°æ®åº“æ—¶å¤±è´¥: {e}", exc_info=True)
            raise

    # --- æ ‡è®°ä¸ºå·²å¤„ç† ---
    def _mark_item_as_processed(self, cursor: psycopg2.extensions.cursor, item_id: str, item_name: str, score: float = 10.0):
        """
        ã€é‡æ„ã€‘å°†ä¸€ä¸ªé¡¹ç›®æ ‡è®°ä¸ºâ€œå·²å¤„ç†â€çš„å”¯ä¸€å®˜æ–¹æ–¹æ³•ã€‚
        å®ƒä¼šåŒæ—¶æ›´æ–°æ•°æ®åº“å’Œå†…å­˜ç¼“å­˜ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§ã€‚
        """
        # 1. æ›´æ–°æ•°æ®åº“
        self.log_db_manager.save_to_processed_log(cursor, item_id, item_name, score=score)
        
        # 2. å®æ—¶æ›´æ–°å†…å­˜ç¼“å­˜
        self.processed_items_cache[item_id] = item_name
        
        logger.debug(f"  âœ å·²å°† '{item_name}' æ ‡è®°ä¸ºå·²å¤„ç† (æ•°æ®åº“ & å†…å­˜)ã€‚")
    # --- æ¸…é™¤å·²å¤„ç†è®°å½• ---
    def clear_processed_log(self):
        """
        ã€å·²æ”¹é€ ã€‘æ¸…é™¤æ•°æ®åº“å’Œå†…å­˜ä¸­çš„å·²å¤„ç†è®°å½•ã€‚
        ä½¿ç”¨ä¸­å¤®æ•°æ®åº“è¿æ¥å‡½æ•°ã€‚
        """
        try:
            # 1. â˜…â˜…â˜… è°ƒç”¨ä¸­å¤®å‡½æ•° â˜…â˜…â˜…
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                
                logger.debug("æ­£åœ¨ä»æ•°æ®åº“åˆ é™¤ processed_log è¡¨ä¸­çš„æ‰€æœ‰è®°å½•...")
                cursor.execute("DELETE FROM processed_log")
                # with è¯­å¥ä¼šè‡ªåŠ¨å¤„ç† conn.commit()
            
            logger.info("  âœ æ•°æ®åº“ä¸­çš„å·²å¤„ç†è®°å½•å·²æ¸…é™¤ã€‚")

            # 2. æ¸…ç©ºå†…å­˜ç¼“å­˜
            self.processed_items_cache.clear()
            logger.info("  âœ å†…å­˜ä¸­çš„å·²å¤„ç†è®°å½•ç¼“å­˜å·²æ¸…é™¤ã€‚")

        except Exception as e:
            logger.error(f"æ¸…é™¤æ•°æ®åº“æˆ–å†…å­˜å·²å¤„ç†è®°å½•æ—¶å¤±è´¥: {e}", exc_info=True)
            # 3. â˜…â˜…â˜… é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œé€šçŸ¥ä¸Šæ¸¸è°ƒç”¨è€…æ“ä½œå¤±è´¥ â˜…â˜…â˜…
            raise
    
    # â˜…â˜…â˜… å…¬å¼€çš„ã€ç‹¬ç«‹çš„è¿½å‰§åˆ¤æ–­æ–¹æ³• â˜…â˜…â˜…
    def check_and_add_to_watchlist(self, item_details: Dict[str, Any]):
        """
        æ£€æŸ¥ä¸€ä¸ªåª’ä½“é¡¹ç›®æ˜¯å¦ä¸ºå‰§é›†ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ‰§è¡Œæ™ºèƒ½è¿½å‰§åˆ¤æ–­å¹¶æ·»åŠ åˆ°å¾…çœ‹åˆ—è¡¨ã€‚
        æ­¤æ–¹æ³•è¢«è®¾è®¡ä¸ºç”±å¤–éƒ¨äº‹ä»¶ï¼ˆå¦‚Webhookï¼‰æ˜¾å¼è°ƒç”¨ã€‚
        """
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_details.get('Id')})")
        
        if item_details.get("Type") != "Series":
            # å¦‚æœä¸æ˜¯å‰§é›†ï¼Œç›´æ¥è¿”å›ï¼Œä¸æ‰“å°éå¿…è¦çš„æ—¥å¿—
            return

        logger.info(f"  âœ å¼€å§‹ä¸ºæ–°å…¥åº“å‰§é›† '{item_name_for_log}' è¿›è¡Œè¿½å‰§çŠ¶æ€åˆ¤æ–­...")
        try:
            # å®ä¾‹åŒ– WatchlistProcessor å¹¶æ‰§è¡Œæ·»åŠ æ“ä½œ
            watchlist_proc = WatchlistProcessor(self.config)
            watchlist_proc.add_series_to_watchlist(item_details)
        except Exception as e_watchlist:
            logger.error(f"  âœ åœ¨è‡ªåŠ¨æ·»åŠ  '{item_name_for_log}' åˆ°è¿½å‰§åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e_watchlist}", exc_info=True)

    def signal_stop(self):
        self._stop_event.set()

    def clear_stop_signal(self):
        self._stop_event.clear()

    def get_stop_event(self) -> threading.Event:
        """è¿”å›å†…éƒ¨çš„åœæ­¢äº‹ä»¶å¯¹è±¡ï¼Œä»¥ä¾¿ä¼ é€’ç»™å…¶ä»–å‡½æ•°ã€‚"""
        return self._stop_event

    def is_stop_requested(self) -> bool:
        return self._stop_event.is_set()

    def _load_processed_log_from_db(self) -> Dict[str, str]:
        log_dict = {}
        try:
            # 1. â˜…â˜…â˜… ä½¿ç”¨ with è¯­å¥å’Œä¸­å¤®å‡½æ•° â˜…â˜…â˜…
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                
                # 2. æ‰§è¡ŒæŸ¥è¯¢
                cursor.execute("SELECT item_id, item_name FROM processed_log")
                rows = cursor.fetchall()
                
                # 3. å¤„ç†ç»“æœ
                for row in rows:
                    if row['item_id'] and row['item_name']:
                        log_dict[row['item_id']] = row['item_name']
            
            # 4. with è¯­å¥ä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰äº‹æƒ…ï¼Œä»£ç å¹²å‡€åˆ©è½ï¼

        except Exception as e:
            # 5. â˜…â˜…â˜… è®°å½•æ›´è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯ â˜…â˜…â˜…
            logger.error(f"ä»æ•°æ®åº“è¯»å–å·²å¤„ç†è®°å½•å¤±è´¥: {e}", exc_info=True)
        return log_dict

    # âœ¨ ä» SyncHandler è¿ç§»å¹¶æ”¹é€ ï¼Œç”¨äºåœ¨æœ¬åœ°ç¼“å­˜ä¸­æŸ¥æ‰¾è±†ç“£JSONæ–‡ä»¶
    def _find_local_douban_json(self, imdb_id: Optional[str], douban_id: Optional[str], douban_cache_dir: str) -> Optional[str]:
        """æ ¹æ® IMDb ID æˆ– è±†ç“£ ID åœ¨æœ¬åœ°ç¼“å­˜ç›®å½•ä¸­æŸ¥æ‰¾å¯¹åº”çš„è±†ç“£JSONæ–‡ä»¶ã€‚"""
        if not os.path.exists(douban_cache_dir):
            return None
        
        # ä¼˜å…ˆä½¿ç”¨ IMDb ID åŒ¹é…ï¼Œæ›´å‡†ç¡®
        if imdb_id:
            for dirname in os.listdir(douban_cache_dir):
                if dirname.startswith('0_'): continue
                if imdb_id in dirname:
                    dir_path = os.path.join(douban_cache_dir, dirname)
                    for filename in os.listdir(dir_path):
                        if filename.endswith('.json'):
                            return os.path.join(dir_path, filename)
                            
        # å…¶æ¬¡ä½¿ç”¨è±†ç“£ ID åŒ¹é…
        if douban_id:
            for dirname in os.listdir(douban_cache_dir):
                if dirname.startswith(f"{douban_id}_"):
                    dir_path = os.path.join(douban_cache_dir, dirname)
                    for filename in os.listdir(dir_path):
                        if filename.endswith('.json'):
                            return os.path.join(dir_path, filename)
        return None

    # âœ¨ å°è£…äº†â€œä¼˜å…ˆæœ¬åœ°ç¼“å­˜ï¼Œå¤±è´¥åˆ™åœ¨çº¿è·å–â€çš„é€»è¾‘
    def _get_douban_data_with_local_cache(self, media_info: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], Optional[float]]:
        """
        ã€V3 - æœ€ç»ˆç‰ˆã€‘è·å–è±†ç“£æ•°æ®ï¼ˆæ¼”å‘˜+è¯„åˆ†ï¼‰ã€‚ä¼˜å…ˆæœ¬åœ°ç¼“å­˜ï¼Œå¤±è´¥åˆ™å›é€€åˆ°åŠŸèƒ½å®Œæ•´çš„åœ¨çº¿APIè·¯å¾„ã€‚
        è¿”å›: (æ¼”å‘˜åˆ—è¡¨, è±†ç“£è¯„åˆ†) çš„å…ƒç»„ã€‚
        """
        # 1. å‡†å¤‡æŸ¥æ‰¾æ‰€éœ€çš„ä¿¡æ¯
        provider_ids = media_info.get("ProviderIds", {})
        item_name = media_info.get("Name", "")
        imdb_id = provider_ids.get("Imdb")
        douban_id_from_provider = provider_ids.get("Douban")
        item_type = media_info.get("Type")
        item_year = str(media_info.get("ProductionYear", ""))

        # 2. å°è¯•ä»æœ¬åœ°ç¼“å­˜æŸ¥æ‰¾
        douban_cache_dir_name = "douban-movies" if item_type == "Movie" else "douban-tv"
        douban_cache_path = os.path.join(self.local_data_path, "cache", douban_cache_dir_name)
        local_json_path = self._find_local_douban_json(imdb_id, douban_id_from_provider, douban_cache_path)

        if local_json_path:
            logger.debug(f"  âœ å‘ç°æœ¬åœ°è±†ç“£ç¼“å­˜æ–‡ä»¶ï¼Œå°†ç›´æ¥ä½¿ç”¨: {local_json_path}")
            douban_data = _read_local_json(local_json_path)
            if douban_data:
                cast = douban_data.get('actors', [])
                rating_str = douban_data.get("rating", {}).get("value")
                rating_float = None
                if rating_str:
                    try: rating_float = float(rating_str)
                    except (ValueError, TypeError): pass
                return cast, rating_float
            else:
                logger.warning(f"æœ¬åœ°è±†ç“£ç¼“å­˜æ–‡ä»¶ '{local_json_path}' æ— æ•ˆï¼Œå°†å›é€€åˆ°åœ¨çº¿APIã€‚")
        
        # 3. å¦‚æœæœ¬åœ°æœªæ‰¾åˆ°ï¼Œå›é€€åˆ°åŠŸèƒ½å®Œæ•´çš„åœ¨çº¿APIè·¯å¾„
        logger.info("  âœ æœªæ‰¾åˆ°æœ¬åœ°è±†ç“£ç¼“å­˜ï¼Œå°†é€šè¿‡åœ¨çº¿APIè·å–æ¼”å‘˜å’Œè¯„åˆ†ä¿¡æ¯ã€‚")

        # 3.1 åŒ¹é…è±†ç“£IDå’Œç±»å‹ã€‚ç°åœ¨ match_info è¿”å›çš„ç»“æœæ˜¯å®Œå…¨å¯ä¿¡çš„ã€‚
        match_info_result = self.douban_api.match_info(
            name=item_name, imdbid=imdb_id, mtype=item_type, year=item_year
        )

        if match_info_result.get("error") or not match_info_result.get("id"):
            logger.warning(f"åœ¨çº¿åŒ¹é…è±†ç“£IDå¤±è´¥ for '{item_name}': {match_info_result.get('message', 'æœªæ‰¾åˆ°ID')}")
            return [], None

        douban_id = match_info_result["id"]
        # âœ¨âœ¨âœ¨ ç›´æ¥ä¿¡ä»»ä» douban.py è¿”å›çš„ç±»å‹ âœ¨âœ¨âœ¨
        douban_type = match_info_result.get("type")

        if not douban_type:
            logger.error(f"ä»è±†ç“£åŒ¹é…ç»“æœä¸­æœªèƒ½è·å–åˆ°åª’ä½“ç±»å‹ for ID {douban_id}ã€‚å¤„ç†ä¸­æ­¢ã€‚")
            return [], None

        # 3.2 è·å–æ¼”èŒå‘˜ (ä½¿ç”¨å®Œå…¨å¯ä¿¡çš„ç±»å‹)
        cast_data = self.douban_api.get_acting(
            name=item_name, 
            douban_id_override=douban_id, 
            mtype=douban_type
        )
        douban_cast_raw = cast_data.get("cast", [])

        # 3.3 è·å–è¯¦æƒ…ï¼ˆä¸ºäº†è¯„åˆ†ï¼‰ï¼ŒåŒæ ·ä½¿ç”¨å¯ä¿¡çš„ç±»å‹
        details_data = self.douban_api._get_subject_details(douban_id, douban_type)
        douban_rating = None
        if details_data and not details_data.get("error"):
            rating_str = details_data.get("rating", {}).get("value")
            if rating_str:
                try:
                    douban_rating = float(rating_str)
                    logger.info(f"  âœ åœ¨çº¿è·å–åˆ°è±†ç“£è¯„åˆ† for '{item_name}': {douban_rating}")
                except (ValueError, TypeError):
                    pass

        return douban_cast_raw, douban_rating
    
    # --- é€šè¿‡TmdbIDæŸ¥æ‰¾æ˜ å°„è¡¨ ---
    def _find_person_in_map_by_tmdb_id(self, tmdb_id: str, cursor: psycopg2.extensions.cursor) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ® TMDB ID åœ¨ person_identity_map è¡¨ä¸­æŸ¥æ‰¾å¯¹åº”çš„è®°å½•ã€‚
        """
        if not tmdb_id:
            return None
        try:
            cursor.execute(
                "SELECT * FROM person_identity_map WHERE tmdb_person_id = %s",
                (tmdb_id,)
            )
            return cursor.fetchone()
        except psycopg2.Error as e:
            logger.error(f"é€šè¿‡ TMDB ID '{tmdb_id}' æŸ¥è¯¢ person_identity_map æ—¶å‡ºé”™: {e}")
            return None
    
    # --- é€šè¿‡ API æ›´æ–° Emby ä¸­æ¼”å‘˜åå­— ---
    def _update_emby_person_names_from_final_cast(self, final_cast: List[Dict[str, Any]], item_name_for_log: str):
        """
        æ ¹æ®æœ€ç»ˆå¤„ç†å¥½çš„æ¼”å‘˜åˆ—è¡¨ï¼Œé€šè¿‡ API æ›´æ–° Emby ä¸­â€œæ¼”å‘˜â€é¡¹ç›®çš„åå­—ã€‚
        """
        actors_to_update = [
            actor for actor in final_cast 
            if actor.get("emby_person_id") and utils.contains_chinese(actor.get("name"))
        ]

        if not actors_to_update:
            logger.info(f"  âœ æ— éœ€é€šè¿‡ API æ›´æ–°æ¼”å‘˜åå­— (æ²¡æœ‰æ‰¾åˆ°éœ€è¦ç¿»è¯‘çš„ Emby æ¼”å‘˜)ã€‚")
            return

        logger.info(f"  âœ å¼€å§‹ä¸ºã€Š{item_name_for_log}ã€‹çš„ {len(actors_to_update)} ä½æ¼”å‘˜é€šè¿‡ API æ›´æ–°åå­—...")
        
        # æ‰¹é‡è·å–è¿™äº›æ¼”å‘˜åœ¨ Emby ä¸­çš„å½“å‰ä¿¡æ¯ï¼Œä»¥å‡å°‘ API è¯·æ±‚
        person_ids = [actor["emby_person_id"] for actor in actors_to_update]
        current_person_details = emby.get_emby_items_by_id(
            base_url=self.emby_url,
            api_key=self.emby_api_key,
            user_id=self.emby_user_id,
            item_ids=person_ids,
            fields="Name"
        )
        
        current_names_map = {p["Id"]: p.get("Name") for p in current_person_details} if current_person_details else {}

        updated_count = 0
        for actor in actors_to_update:
            person_id = actor["emby_person_id"]
            new_name = actor["name"]
            current_name = current_names_map.get(person_id)

            # åªæœ‰å½“æ–°åå­—å’Œå½“å‰åå­—ä¸åŒæ—¶ï¼Œæ‰æ‰§è¡Œæ›´æ–°
            if new_name != current_name:
                emby.update_person_details(
                    person_id=person_id,
                    new_data={"Name": new_name},
                    emby_server_url=self.emby_url,
                    emby_api_key=self.emby_api_key,
                    user_id=self.emby_user_id
                )
                updated_count += 1
                # åŠ ä¸ªå°å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.2) 

        logger.info(f"  âœ æˆåŠŸé€šè¿‡ API æ›´æ–°äº† {updated_count} ä½æ¼”å‘˜çš„åå­—ã€‚")
    
    # --- å…¨é‡å¤„ç†çš„å…¥å£ ---
    def process_full_library(self, update_status_callback: Optional[callable] = None, force_full_update: bool = False):
        """
        è¿™æ˜¯æ‰€æœ‰å…¨é‡å¤„ç†çš„å”¯ä¸€å…¥å£ã€‚
        """
        self.clear_stop_signal()
        
        logger.trace(f"è¿›å…¥æ ¸å¿ƒæ‰§è¡Œå±‚: process_full_library, æ¥æ”¶åˆ°çš„ force_full_update = {force_full_update}")

        if force_full_update:
            logger.info("  âœ æ£€æµ‹åˆ°â€œæ·±åº¦æ›´æ–°â€æ¨¡å¼ï¼Œæ­£åœ¨æ¸…ç©ºå·²å¤„ç†æ—¥å¿—...")
            try:
                self.clear_processed_log()
            except Exception as e:
                logger.error(f"åœ¨ process_full_library ä¸­æ¸…ç©ºæ—¥å¿—å¤±è´¥: {e}", exc_info=True)
                if update_status_callback: update_status_callback(-1, "æ¸…ç©ºæ—¥å¿—å¤±è´¥")
                return

        libs_to_process_ids = self.config.get("libraries_to_process", [])
        if not libs_to_process_ids:
            logger.warning("  âœ æœªåœ¨é…ç½®ä¸­æŒ‡å®šè¦å¤„ç†çš„åª’ä½“åº“ã€‚")
            return

        logger.info("  âœ æ­£åœ¨å°è¯•ä»Embyè·å–åª’ä½“é¡¹ç›®...")
        all_emby_libraries = emby.get_emby_libraries(self.emby_url, self.emby_api_key, self.emby_user_id) or []
        library_name_map = {lib.get('Id'): lib.get('Name', 'æœªçŸ¥åº“å') for lib in all_emby_libraries}
        
        movies = emby.get_emby_library_items(self.emby_url, self.emby_api_key, "Movie", self.emby_user_id, libs_to_process_ids, library_name_map=library_name_map) or []
        series = emby.get_emby_library_items(self.emby_url, self.emby_api_key, "Series", self.emby_user_id, libs_to_process_ids, library_name_map=library_name_map) or []
        
        if movies:
            source_movie_lib_names = sorted(list({library_name_map.get(item.get('_SourceLibraryId')) for item in movies if item.get('_SourceLibraryId')}))
            logger.info(f"  âœ ä»åª’ä½“åº“ã€{', '.join(source_movie_lib_names)}ã€‘è·å–åˆ° {len(movies)} ä¸ªç”µå½±é¡¹ç›®ã€‚")

        if series:
            source_series_lib_names = sorted(list({library_name_map.get(item.get('_SourceLibraryId')) for item in series if item.get('_SourceLibraryId')}))
            logger.info(f"  âœ ä»åª’ä½“åº“ã€{', '.join(source_series_lib_names)}ã€‘è·å–åˆ° {len(series)} ä¸ªç”µè§†å‰§é¡¹ç›®ã€‚")

        all_items = movies + series
        total = len(all_items)
        
        if total == 0:
            logger.info("  âœ åœ¨æ‰€æœ‰é€‰å®šçš„åº“ä¸­æœªæ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„é¡¹ç›®ã€‚")
            if update_status_callback: update_status_callback(100, "æœªæ‰¾åˆ°å¯å¤„ç†çš„é¡¹ç›®ã€‚")
            return

        # --- æ–°å¢ï¼šæ¸…ç†å·²åˆ é™¤çš„åª’ä½“é¡¹ ---
        if update_status_callback: update_status_callback(20, "æ­£åœ¨æ£€æŸ¥å¹¶æ¸…ç†å·²åˆ é™¤çš„åª’ä½“é¡¹...")
        
        with get_central_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT item_id, item_name FROM processed_log")
            processed_log_entries = cursor.fetchall()
            
            processed_ids_in_db = {entry['item_id'] for entry in processed_log_entries}
            emby_ids_in_library = {item.get('Id') for item in all_items if item.get('Id')}
            
            # æ‰¾å‡ºåœ¨ processed_log ä¸­ä½†ä¸åœ¨ Emby åª’ä½“åº“ä¸­çš„é¡¹ç›®
            deleted_items_to_clean = processed_ids_in_db - emby_ids_in_library
            
            if deleted_items_to_clean:
                logger.info(f"  âœ å‘ç° {len(deleted_items_to_clean)} ä¸ªå·²ä» Emby åª’ä½“åº“åˆ é™¤çš„é¡¹ç›®ï¼Œæ­£åœ¨ä» 'å·²å¤„ç†' ä¸­ç§»é™¤...")
                for deleted_item_id in deleted_items_to_clean:
                    self.log_db_manager.remove_from_processed_log(cursor, deleted_item_id)
                    # åŒæ—¶ä»å†…å­˜ç¼“å­˜ä¸­ç§»é™¤
                    if deleted_item_id in self.processed_items_cache:
                        del self.processed_items_cache[deleted_item_id]
                    logger.debug(f"  âœ å·²ä» 'å·²å¤„ç†' ä¸­ç§»é™¤ ItemID: {deleted_item_id}")
                conn.commit()
                logger.info("  âœ å·²åˆ é™¤åª’ä½“é¡¹çš„æ¸…ç†å·¥ä½œå®Œæˆã€‚")
            else:
                logger.info("  âœ æœªå‘ç°éœ€è¦ä» 'å·²å¤„ç†' ä¸­æ¸…ç†çš„å·²åˆ é™¤åª’ä½“é¡¹ã€‚")
        
        if update_status_callback: update_status_callback(30, "å·²åˆ é™¤åª’ä½“é¡¹æ¸…ç†å®Œæˆï¼Œå¼€å§‹å¤„ç†ç°æœ‰åª’ä½“...")

        # --- ç°æœ‰åª’ä½“é¡¹å¤„ç†å¾ªç¯ ---
        for i, item in enumerate(all_items):
            if self.is_stop_requested():
                logger.warning("  ğŸš« å…¨åº“æ‰«æä»»åŠ¡å·²è¢«ç”¨æˆ·ä¸­æ­¢ã€‚")
                break # ä½¿ç”¨ break ä¼˜é›…åœ°é€€å‡ºå¾ªç¯
            
            item_id = item.get('Id')
            item_name = item.get('Name', f"ID:{item_id}")

            if not force_full_update and item_id in self.processed_items_cache:
                logger.info(f"  âœ æ­£åœ¨è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®: {item_name}")
                if update_status_callback:
                    # è°ƒæ•´è¿›åº¦æ¡çš„èµ·å§‹ç‚¹ï¼Œä½¿å…¶åœ¨æ¸…ç†åä» 30% å¼€å§‹
                    progress_after_cleanup = 30
                    current_progress = progress_after_cleanup + int(((i + 1) / total) * (100 - progress_after_cleanup))
                    update_status_callback(current_progress, f"è·³è¿‡: {item_name}")
                continue

            if update_status_callback:
                progress_after_cleanup = 30
                current_progress = progress_after_cleanup + int(((i + 1) / total) * (100 - progress_after_cleanup))
                update_status_callback(current_progress, f"å¤„ç†ä¸­ ({i+1}/{total}): {item_name}")
            
            self.process_single_item(
                item_id, 
                force_full_update=force_full_update
            )
            
            time_module.sleep(float(self.config.get("delay_between_items_sec", 0.5)))
        
        if not self.is_stop_requested() and update_status_callback:
            update_status_callback(100, "å…¨é‡å¤„ç†å®Œæˆ")
    
    # --- æ ¸å¿ƒå¤„ç†æ€»ç®¡ ---
    def process_single_item(self, emby_item_id: str, force_full_update: bool = False):
        """
        ã€V-API-Ready æœ€ç»ˆç‰ˆ - å¸¦è·³è¿‡åŠŸèƒ½ã€‘
        å…¥å£å‡½æ•°ï¼Œå®ƒä¼šå…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®ã€‚
        """
        # 1. é™¤éå¼ºåˆ¶ï¼Œå¦åˆ™è·³è¿‡å·²å¤„ç†çš„
        if not force_full_update and emby_item_id in self.processed_items_cache:
            item_name_from_cache = self.processed_items_cache.get(emby_item_id, f"ID:{emby_item_id}")
            logger.info(f"åª’ä½“ '{item_name_from_cache}' è·³è¿‡å·²å¤„ç†è®°å½•ã€‚")
            return True

        # 2. æ£€æŸ¥åœæ­¢ä¿¡å·
        if self.is_stop_requested():
            return False

        # 3. è·å–Embyè¯¦æƒ…ï¼Œè¿™æ˜¯åç»­æ‰€æœ‰æ“ä½œçš„åŸºç¡€
        item_details_precheck = emby.get_emby_item_details(emby_item_id, self.emby_url, self.emby_api_key, self.emby_user_id, fields="Type")
        if not item_details_precheck:
            logger.error(f"process_single_item: æ— æ³•è·å– Emby é¡¹ç›® {emby_item_id} çš„åŸºç¡€è¯¦æƒ…ã€‚")
            return False

        item_type = item_details_precheck.get("Type")
        item_details = None

        if item_type == "Series":
            # å¦‚æœæ˜¯å‰§é›†ï¼Œè°ƒç”¨æˆ‘ä»¬æ–°çš„èšåˆå‡½æ•°
            item_details = emby.get_emby_series_details_with_full_cast(
                series_id=emby_item_id,
                emby_server_url=self.emby_url,
                emby_api_key=self.emby_api_key,
                user_id=self.emby_user_id
            )
        else:
            # å¦‚æœæ˜¯ç”µå½±æˆ–å…¶ä»–ç±»å‹ï¼Œä½¿ç”¨åŸæ¥çš„å‡½æ•°
            item_details = emby.get_emby_item_details(
                emby_item_id, self.emby_url, self.emby_api_key, self.emby_user_id
            )
        if not item_details:
            logger.error(f"process_single_item: æ— æ³•è·å– Emby é¡¹ç›® {emby_item_id} çš„è¯¦æƒ…ã€‚")
            return False

        # 4. å°†ä»»åŠ¡äº¤ç»™æ ¸å¿ƒå¤„ç†å‡½æ•°
        return self._process_item_core_logic(
            item_details_from_emby=item_details,
            force_full_update=force_full_update
        )

    # ---æ ¸å¿ƒå¤„ç†æµç¨‹ ---
    def _process_item_core_logic(self, item_details_from_emby: Dict[str, Any], force_full_update: bool = False):
        """
        ã€V-Final-Architecture-Pro - â€œè®¾è®¡å¸ˆâ€æœ€ç»ˆç‰ˆ + è¯„åˆ†æœºåˆ¶ã€‘
        æœ¬å‡½æ•°ä½œä¸ºâ€œè®¾è®¡å¸ˆâ€ï¼Œåªè´Ÿè´£è®¡ç®—å’Œæ€è€ƒï¼Œäº§å‡ºâ€œè®¾è®¡å›¾â€å’Œâ€œç‰©æ–™æ¸…å•â€ï¼Œç„¶åå…¨æƒå§”æ‰˜ç»™æ–½å·¥é˜Ÿã€‚
        """
        # ======================================================================
        # é˜¶æ®µ 1: å‡†å¤‡å·¥ä½œ
        # ======================================================================
        item_id = item_details_from_emby.get("Id")
        item_name_for_log = item_details_from_emby.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        tmdb_id = item_details_from_emby.get("ProviderIds", {}).get("Tmdb")
        item_type = item_details_from_emby.get("Type")

        logger.info(f"--- å¼€å§‹å¤„ç† '{item_name_for_log}' (TMDb ID: {tmdb_id}) ---")

        all_emby_people_for_count = item_details_from_emby.get("People", [])
        original_emby_actor_count = len([p for p in all_emby_people_for_count if p.get("Type") == "Actor"])

        if not tmdb_id:
            logger.error(f"  âœ '{item_name_for_log}' ç¼ºå°‘ TMDb IDï¼Œæ— æ³•å¤„ç†ã€‚")
            return False
        if not self.local_data_path:
            logger.error(f"  âœ '{item_name_for_log}' å¤„ç†å¤±è´¥ï¼šæœªåœ¨é…ç½®ä¸­è®¾ç½®â€œæœ¬åœ°æ•°æ®æºè·¯å¾„â€ã€‚")
            return False
        
        try:
            authoritative_cast_source = []
            tmdb_details_for_extra = None # ç”¨äºå†…éƒ¨ç¼“å­˜

            # æ­¥éª¤1:æ£€æŸ¥jsonæ˜¯å¦ç¼ºå¤±
            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            source_cache_dir = os.path.join(self.local_data_path, "cache", cache_folder_name, tmdb_id)
            main_json_filename = "all.json" if item_type == "Movie" else "series.json"
            source_json_path = os.path.join(source_cache_dir, main_json_filename)

            if not os.path.exists(source_json_path):
                logger.warning(f"  âœ æ ¸å¿ƒå¤„ç†å‰ç½®æ£€æŸ¥ï¼šæœ¬åœ°å…ƒæ•°æ®æ–‡ä»¶ '{source_json_path}' ä¸å­˜åœ¨ã€‚å¯åŠ¨å¤‡ç”¨æ–¹æ¡ˆ...")
                logger.info(f"  âœ æ­£åœ¨é€šçŸ¥ Emby ä¸º '{item_name_for_log}' åˆ·æ–°å…ƒæ•°æ®ä»¥ç”Ÿæˆç¼“å­˜æ–‡ä»¶...")
                
                emby.refresh_emby_item_metadata(
                    item_emby_id=item_id,
                    emby_server_url=self.emby_url,
                    emby_api_key=self.emby_api_key,
                    user_id_for_ops=self.emby_user_id,
                    replace_all_metadata_param=True,
                    item_name_for_log=item_name_for_log
                )

                # --- æ ¹æ®åª’ä½“ç±»å‹é€‰æ‹©ä¸åŒçš„ç­‰å¾…ç­–ç•¥ ---
                if item_type == "Series":
                    # ç”µè§†å‰§ï¼šæ™ºèƒ½ç­‰å¾…æ¨¡å¼
                    logger.info("  âœ æ£€æµ‹åˆ°ä¸ºç”µè§†å‰§ï¼Œå¯åŠ¨æ™ºèƒ½ç­‰å¾…æ¨¡å¼...")
                    total_wait_time = 0
                    idle_time = 0
                    last_file_count = 0
                    CHECK_INTERVAL = 10  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                    MAX_IDLE_TIME = 60   # è¿ç»­60ç§’æ²¡åŠ¨é™åˆ™è¶…æ—¶
                    MAX_TOTAL_WAIT_MINUTES = 15 # æ€»æœ€é•¿ç­‰å¾…æ—¶é—´15åˆ†é’Ÿ

                    while total_wait_time < MAX_TOTAL_WAIT_MINUTES * 60:
                        time_module.sleep(CHECK_INTERVAL)
                        total_wait_time += CHECK_INTERVAL

                        # æ£€æŸ¥ä¸»æ–‡ä»¶æ˜¯å¦å·²ç”Ÿæˆ
                        if os.path.exists(source_json_path):
                            logger.info(f"  âœ ä¸»æ–‡ä»¶ '{main_json_filename}' å·²ç”Ÿæˆï¼ç­‰å¾…ç»“æŸã€‚")
                            break
                        
                        # æ£€æŸ¥ç›®å½•å†…æ–‡ä»¶æ•°é‡å˜åŒ–
                        try:
                            current_file_count = len(os.listdir(source_cache_dir))
                        except FileNotFoundError:
                            current_file_count = 0

                        if current_file_count > last_file_count:
                            logger.info(f"  âœ ç¼“å­˜ç›®å½•æœ‰æ´»åŠ¨ï¼Œæ£€æµ‹åˆ° {current_file_count - last_file_count} ä¸ªæ–°æ–‡ä»¶ã€‚é‡ç½®ç©ºé—²è®¡æ—¶å™¨ã€‚")
                            idle_time = 0 # æœ‰æ–°æ–‡ä»¶ï¼Œé‡ç½®ç©ºé—²è®¡æ—¶
                            last_file_count = current_file_count
                        else:
                            idle_time += CHECK_INTERVAL
                            logger.info(f"  âœ ç¼“å­˜ç›®å½•æ— æ–°æ–‡ä»¶ï¼Œç©ºé—²æ—¶é—´ç´¯è®¡: {idle_time}/{MAX_IDLE_TIME}ç§’ã€‚")

                        if idle_time >= MAX_IDLE_TIME:
                            logger.warning(f"  âœ ç¼“å­˜ç›®å½•è¿ç»­ {MAX_IDLE_TIME} ç§’æ— æ´»åŠ¨ï¼Œåˆ¤å®šä»»åŠ¡å®Œæˆæˆ–è¶…æ—¶ã€‚")
                            break
                    else: # whileå¾ªç¯æ­£å¸¸ç»“æŸï¼ˆè¾¾åˆ°æ€»æ—¶é•¿ï¼‰
                        logger.warning(f"  âœ å·²è¾¾åˆ°æ€»æœ€é•¿ç­‰å¾…æ—¶é—´ {MAX_TOTAL_WAIT_MINUTES} åˆ†é’Ÿï¼Œåœæ­¢ç­‰å¾…ã€‚")

                else:
                    # ç”µå½±ï¼šç®€å•å®šæ—¶ç­‰å¾…
                    logger.info("  âœ æ£€æµ‹åˆ°ä¸ºç”µå½±ï¼Œå¯åŠ¨ç®€å•ç­‰å¾…æ¨¡å¼...")
                    for attempt in range(10):
                        logger.info(f"  âœ ç­‰å¾…3ç§’åæ£€æŸ¥æ–‡ä»¶... (ç¬¬ {attempt + 1}/10 æ¬¡å°è¯•)")
                        time_module.sleep(3)
                        if os.path.exists(source_json_path):
                            logger.info(f"  âœ æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆï¼")
                            break
            
            # åœ¨æ‰€æœ‰å°è¯•åï¼Œæœ€ç»ˆç¡®è®¤æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(source_json_path):
                logger.error(f"  âœ ç­‰å¾…è¶…æ—¶ï¼Œå…ƒæ•°æ®æ–‡ä»¶ä»æœªç”Ÿæˆã€‚æ— æ³•ç»§ç»­å¤„ç† '{item_name_for_log}'ï¼Œå·²è·³è¿‡ã€‚")
                return False


            # æ­¥éª¤3ï¼šå¦‚æœæ˜¯å¼ºåˆ¶é‡å¤„ç†å°±ä»TMDbæ‹‰å–æœ€æ–°å…ƒæ•°æ®ï¼Œå¦åˆ™ç›´æ¥ç”¨æœ¬åœ°çš„å…ƒæ•°æ®ã€‚
            if force_full_update and self.tmdb_api_key:
                logger.info(f"  âœ [æ·±åº¦æ›´æ–°æ¨¡å¼] æ­£åœ¨ä» TMDB è·å–æœ€æ–°æ¼”å‘˜è¡¨...")
                if item_type == "Movie":
                    movie_details = tmdb.get_movie_details(tmdb_id, self.tmdb_api_key)
                    if movie_details and movie_details.get("credits", {}).get("cast"):
                        tmdb_details_for_extra = movie_details
                        authoritative_cast_source = movie_details["credits"]["cast"]
                        logger.info(f"  âœ æˆåŠŸä» TMDb è·å–åˆ° {len(authoritative_cast_source)} ä½æ¼”å‘˜çš„æœ€æ–°æ•°æ®ã€‚")
                    else:
                        logger.warning(f"  âœ ä» TMDb è·å–æ¼”å‘˜æ•°æ®å¤±è´¥æˆ–è¿”å›ä¸ºç©ºï¼Œå°†å›é€€åˆ°æœ¬åœ°æ•°æ®ã€‚")
                elif item_type == "Series":
                    aggregated_tmdb_data = tmdb.aggregate_full_series_data_from_tmdb(int(tmdb_id), self.tmdb_api_key)
                    if aggregated_tmdb_data:
                        tmdb_details_for_extra = aggregated_tmdb_data.get("series_details")
                        all_episodes = list(aggregated_tmdb_data.get("episodes_details", {}).values())
                        authoritative_cast_source = _aggregate_series_cast_from_tmdb_data(aggregated_tmdb_data["series_details"], all_episodes)
            else:
                # åœ¨æ–‡ä»¶æ¨¡å¼ä¸‹ï¼Œç›´æ¥è¯»å–æˆ‘ä»¬å·²ç»ç¡®è®¤å­˜åœ¨çš„æ–‡ä»¶
                logger.info(f"  âœ æ­£åœ¨ä» cache æ–‡ä»¶ä¸­é¢„è¯»æ¼”å‘˜è¡¨...")
                source_json_data = _read_local_json(source_json_path)
                if source_json_data:
                    tmdb_details_for_extra = source_json_data
                    authoritative_cast_source = (source_json_data.get("casts", {}) or source_json_data.get("credits", {})).get("cast", [])
                else:
                    logger.error(f"  âœ å…ƒæ•°æ®æ–‡ä»¶ '{source_json_path}' æ— æ•ˆæˆ–ä¸ºç©ºï¼Œæ— æ³•å¤„ç† '{item_name_for_log}'ã€‚")
                    return False
                
            # ç§»é™¤æ— å¤´åƒæ¼”å‘˜
            if self.config.get(constants.CONFIG_OPTION_REMOVE_ACTORS_WITHOUT_AVATARS, True) and authoritative_cast_source:
                original_count = len(authoritative_cast_source)
                
                # ä½¿ç”¨ 'profile_path' ä½œä¸ºåˆ¤æ–­ä¾æ®
                actors_with_avatars = [
                    actor for actor in authoritative_cast_source if actor.get("profile_path")
                ]
                
                if len(actors_with_avatars) < original_count:
                    removed_count = original_count - len(actors_with_avatars)
                    logger.info(f"  âœ åœ¨æ ¸å¿ƒå¤„ç†å‰ï¼Œå·²ä»æºæ•°æ®ä¸­ç§»é™¤ {removed_count} ä½æ— å¤´åƒçš„æ¼”å‘˜ã€‚")
                    # ç”¨ç­›é€‰åçš„åˆ—è¡¨è¦†ç›–åŸå§‹åˆ—è¡¨
                    authoritative_cast_source = actors_with_avatars
                else:
                    logger.debug("  âœ (é¢„æ£€æŸ¥) æ‰€æœ‰æºæ•°æ®ä¸­çš„æ¼”å‘˜å‡æœ‰å¤´åƒï¼Œæ— éœ€é¢„å…ˆç§»é™¤ã€‚")
                
            # ======================================================================
            # é˜¶æ®µ 2: æ•°æ®æ¥æºä¸‰é€‰ä¸€
            # ======================================================================
            final_processed_cast = None
            douban_rating = None
            cache_row = None # ç”¨äºåç»­åˆ¤æ–­æ˜¯å¦èµ°äº†å¿«é€Ÿæ¨¡å¼

            # 1.å°è¯•å…ƒæ•°æ®ç¼“å­˜
            if not force_full_update:
                logger.info(f"  âœ [ç¼“å­˜æ¨¡å¼] å°è¯•ä»å…ƒæ•°æ®ç¼“å­˜åŠ è½½ '{item_name_for_log}'...")
                try:
                    with get_central_db_connection() as conn:
                        cursor = conn.cursor()
                        # åªæœ‰å½“ actors_json å­˜åœ¨ (NOT NULL) ä¸”ä¸æ˜¯ä¸€ä¸ªç©ºçš„JSONæ•°ç»„æ—¶ï¼Œæ‰è®¤ä¸ºç¼“å­˜æœ‰æ•ˆ
                        cursor.execute("""
                            SELECT actors_json, rating 
                            FROM media_metadata 
                            WHERE tmdb_id = %s AND item_type = %s
                              AND actors_json IS NOT NULL AND actors_json::text != '[]'
                        """, (tmdb_id, item_type))
                        cache_row = cursor.fetchone()

                        if cache_row:
                            logger.info(f"  âœ [ç¼“å­˜æ¨¡å¼] æˆåŠŸå‘½ä¸­æœ‰æ•ˆç¼“å­˜ï¼å°†ä»æ•°æ®åº“æ¢å¤æ¼”å‘˜æ•°æ®...")
                            slim_actors_from_cache = cache_row["actors_json"]
                            final_processed_cast = self.actor_db_manager.rehydrate_slim_actors(cursor, slim_actors_from_cache)
                            douban_rating = cache_row.get("rating")

                except Exception as e_cache:
                    logger.warning(f"  âœ [ç¼“å­˜æ¨¡å¼] åŠ è½½ç¼“å­˜å¤±è´¥: {e_cache}ã€‚å°†å›é€€åˆ°æ·±åº¦æ¨¡å¼ã€‚")
                    final_processed_cast = None

            # 2.å°è¯•è¦†ç›–ç¼“å­˜
            if final_processed_cast is None and not force_full_update:
                cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
                target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
                main_json_filename = "all.json" if item_type == "Movie" else "series.json"
                override_json_path = os.path.join(target_override_dir, main_json_filename)

                if os.path.exists(override_json_path):
                    logger.info(f"  âœ [åå“ºæ¨¡å¼] æœªå‘½ä¸­æ•°æ®åº“ç¼“å­˜ï¼Œä½†å‘ç°æœ¬åœ°è¦†ç›–ç¼“å­˜æ–‡ä»¶: {override_json_path}")
                    try:
                        override_data = _read_local_json(override_json_path)
                        if override_data:
                            # æå–æ¼”å‘˜è¡¨
                            cast_data = (override_data.get('casts', {}) or override_data.get('credits', {})).get('cast', [])
                            
                            if cast_data:
                                logger.info(f"  âœ [åå“ºæ¨¡å¼] æˆåŠŸä»è¦†ç›–ç¼“å­˜æ–‡ä»¶åŠ è½½ {len(cast_data)} ä½æ¼”å‘˜æ•°æ®ï¼Œå°†åå“ºå›æ•°æ®åº“...")
                                final_processed_cast = cast_data
                                douban_rating = override_data.get('vote_average')
                                
                                # â˜…â˜…â˜… å…³é”® 1ï¼šå°† override æ•°æ®ä½œä¸ºæºæ•°æ®åŒ… â˜…â˜…â˜…
                                # è¿™æ ·åç»­çš„ _upsert_media_metadata å°±ä¼šæŠŠè¿™ä»½å®Œç¾æ•°æ®ï¼ˆç®€ä»‹ã€åˆ†çº§ç­‰ï¼‰å†™å…¥æ•°æ®åº“
                                tmdb_details_for_extra = override_data
                                
                                # â˜…â˜…â˜… å…³é”® 2ï¼šæ ‡è®°ä¸ºå‘½ä¸­ç¼“å­˜ â˜…â˜…â˜…
                                # è¿™æ ·åç»­çš„è´¨æ£€æµç¨‹ä¼šç›´æ¥ç»™ 10.0 åˆ†ï¼Œä¸å†è¿›è¡Œç”»è›‡æ·»è¶³çš„æ£€æŸ¥
                                cache_row = {'source': 'override_file'}

                                # è¡¥å……ï¼šå°è¯•ç®€å•çš„ ID æ˜ å°„ (TMDb -> Emby)ï¼Œè®©å†…å­˜ä¸­çš„æ•°æ®å¯¹è±¡æ›´å®Œæ•´
                                # è™½ç„¶å†™å…¥æ•°æ®åº“ä¸å¼ºä¾èµ–å®ƒï¼Œä½†å¯¹æ—¥å¿—å’Œåç»­é€»è¾‘æœ‰å¥½å¤„
                                tmdb_to_emby_map = {}
                                for person in item_details_from_emby.get("People", []):
                                    pid = (person.get("ProviderIds") or {}).get("Tmdb")
                                    if pid: tmdb_to_emby_map[str(pid)] = person.get("Id")
                                
                                for actor in final_processed_cast:
                                    aid = str(actor.get('id'))
                                    if aid in tmdb_to_emby_map:
                                        actor['emby_person_id'] = tmdb_to_emby_map[aid]
                            else:
                                logger.warning("  âœ [åå“ºæ¨¡å¼] è¦†ç›–ç¼“å­˜æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¼”å‘˜åˆ—è¡¨ï¼Œè·³è¿‡åå“ºã€‚")
                    except Exception as e:
                        logger.warning(f"  âœ [åå“ºæ¨¡å¼] è¯»å–è¦†ç›–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

            # 3.å®Œæ•´æ¨¡å¼
            if final_processed_cast is None:
                logger.info(f"  âœ æœªå‘½ä¸­ç¼“å­˜æˆ–å¼ºåˆ¶é‡å¤„ç†ï¼Œå¼€å§‹å¤„ç†æ¼”å‘˜è¡¨...")
                
                with get_central_db_connection() as conn:
                    cursor = conn.cursor()
                    
                    all_emby_people = item_details_from_emby.get("People", [])
                    current_emby_cast_raw = [p for p in all_emby_people if p.get("Type") == "Actor"]
                    emby_config = {"url": self.emby_url, "api_key": self.emby_api_key, "user_id": self.emby_user_id}
                    enriched_emby_cast = self.actor_db_manager.enrich_actors_with_provider_ids(cursor, current_emby_cast_raw, emby_config)
                    douban_cast_raw, douban_rating_deep = self._get_douban_data_with_local_cache(item_details_from_emby)
                    douban_rating = douban_rating_deep # è¦†ç›–è¯„åˆ†

                    # è°ƒç”¨æ ¸å¿ƒå¤„ç†å™¨å¤„ç†æ¼”å‘˜è¡¨
                    final_processed_cast = self._process_cast_list(
                        tmdb_cast_people=authoritative_cast_source,
                        emby_cast_people=enriched_emby_cast,
                        douban_cast_list=douban_cast_raw,
                        item_details_from_emby=item_details_from_emby,
                        cursor=cursor,
                        tmdb_api_key=self.tmdb_api_key,
                        stop_event=self.get_stop_event()
                    )

            # ======================================================================
            # é˜¶æ®µ 3: ç»Ÿä¸€çš„æ”¶å°¾æµç¨‹ (æ— è®ºæ¥æºï¼Œå¿…é¡»æ‰§è¡Œ)
            # ======================================================================
            if final_processed_cast is None:
                raise ValueError("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æœ€ç»ˆæ¼”å‘˜åˆ—è¡¨ã€‚")

            with get_central_db_connection() as conn:
                cursor = conn.cursor()

                # â˜…â˜…â˜… æ ¸å¿ƒåŒºåˆ†é€»è¾‘ â˜…â˜…â˜…
                # 1. æ•°æ®åº“ç¼“å­˜æ¨¡å¼: cache_row æ˜¯æ•°æ®åº“è¡Œå¯¹è±¡ï¼Œæ²¡æœ‰ 'source' å­—æ®µ
                # 2. åå“ºæ¨¡å¼: cache_row æ˜¯æˆ‘ä»¬æ‰‹åŠ¨åˆ›å»ºçš„ {'source': 'override_file'}
                is_feedback_mode = cache_row and isinstance(cache_row, dict) and cache_row.get('source') == 'override_file'

                if is_feedback_mode:
                    # --- åˆ†æ”¯ A: åå“ºæ¨¡å¼ (æé€Ÿæ¢å¤) ---
                    # æ—¢ç„¶æœ¬åœ°æ–‡ä»¶å­˜åœ¨ä¸”è¢«è®¤ä¸ºæ˜¯å®Œç¾çš„ï¼Œè¯´æ˜ Emby ç«¯çš„æ•°æ®å’Œå›¾ç‰‡ä¹Ÿå·²ç»æ˜¯å¥½çš„
                    # æˆ‘ä»¬åªéœ€è¦æŠŠæ•°æ®å†™å›å·¥å…·çš„æ•°æ®åº“å³å¯ï¼Œè·³è¿‡æ‰€æœ‰ API è°ƒç”¨å’Œæ–‡ä»¶ IO
                    logger.info(f"  âœ [åå“ºæ¨¡å¼] æ£€æµ‹åˆ°å®Œç¾æœ¬åœ°æ•°æ®ï¼Œè·³è¿‡å›¾ç‰‡ä¸‹è½½ã€æ–‡ä»¶å†™å…¥åŠ Emby åˆ·æ–°ã€‚")
                
                else:
                    # --- åˆ†æ”¯ B: æ­£å¸¸å¤„ç†æ¨¡å¼ (æˆ–æ•°æ®åº“ç¼“å­˜æ¨¡å¼) ---
                    # æ­¥éª¤ 3.1: å†™å…¥ override æ–‡ä»¶
                    self.sync_single_item_assets(
                        item_id=item_id,
                        update_description="ä¸»æµç¨‹å¤„ç†å®Œæˆ",
                        final_cast_override=final_processed_cast,
                        douban_rating_override=douban_rating
                    )

                    # æ­¥éª¤ 3.2: é€šè¿‡ API å®æ—¶æ›´æ–° Emby æ¼”å‘˜åº“ä¸­çš„åå­—
                    self._update_emby_person_names_from_final_cast(final_processed_cast, item_name_for_log)

                    # æ­¥éª¤ 3.3: é€šçŸ¥ Emby åˆ·æ–°
                    logger.info(f"  âœ å¤„ç†å®Œæˆï¼Œæ­£åœ¨é€šçŸ¥ Emby åˆ·æ–°...")
                    emby.refresh_emby_item_metadata(
                        item_emby_id=item_id,
                        emby_server_url=self.emby_url,
                        emby_api_key=self.emby_api_key,
                        user_id_for_ops=self.emby_user_id,
                        replace_all_metadata_param=True, 
                        item_name_for_log=item_name_for_log
                    )

                # æ­¥éª¤ 3.4: æ›´æ–°æˆ‘ä»¬è‡ªå·±çš„æ•°æ®åº“ç¼“å­˜ (è¿™æ˜¯åå“ºæ¨¡å¼çš„æ ¸å¿ƒç›®çš„ï¼Œå¿…é¡»æ‰§è¡Œ)
                self._upsert_media_metadata(
                    cursor=cursor,
                    item_type=item_type,
                    item_details_from_emby=item_details_from_emby,
                    final_processed_cast=final_processed_cast,
                    source_data_package=tmdb_details_for_extra,
                    douban_rating=douban_rating
                )
                
                # ======================================================================
                # æ­¥éª¤ 3.5: ç»¼åˆè´¨æ£€ (è§†é¢‘æµæ£€æŸ¥ + æ¼”å‘˜åŒ¹é…åº¦è¯„åˆ†)
                # ======================================================================
                logger.info(f"  âœ æ­£åœ¨è¯„ä¼°ã€Š{item_name_for_log}ã€‹çš„å¤„ç†è´¨é‡...")
                
                # --- 1. è§†é¢‘æµæ•°æ®å®Œæ•´æ€§æ£€æŸ¥ (ä»…é’ˆå¯¹ Movie å’Œ Episode) ---
                stream_check_passed = True
                stream_fail_reason = ""
                
                if item_type in ['Movie', 'Episode']:
                    has_valid_video = False
                    media_sources = item_details_from_emby.get("MediaSources", [])
                    if media_sources:
                        for source in media_sources:
                            for stream in source.get("MediaStreams", []):
                                # åªè¦å‘ç°ä¸€ä¸ªç±»å‹ä¸º Video çš„æµï¼Œå°±è®¤ä¸ºé€šè¿‡
                                if stream.get("Type") == "Video":
                                    has_valid_video = True
                                    break
                            if has_valid_video: break
                    
                    if not has_valid_video:
                        stream_check_passed = False
                        stream_fail_reason = "ç¼ºå¤±è§†é¢‘æµæ•°æ® (å¯èƒ½æ˜¯strmæ–‡ä»¶æœªæå–æˆ–åˆ†ææœªå®Œæˆ)"
                        logger.warning(f"  âœ [è´¨æ£€å¤±è´¥] ã€Š{item_name_for_log}ã€‹æœªæ£€æµ‹åˆ°è§†é¢‘æµã€‚")

                # --- 2. æ¼”å‘˜å¤„ç†è´¨é‡è¯„åˆ† ---
                genres = item_details_from_emby.get("Genres", [])
                is_animation = "Animation" in genres or "åŠ¨ç”»" in genres or "Documentary" in genres or "çºªå½•" in genres
                
                # å¦‚æœèµ°äº†å¿«é€Ÿæ¨¡å¼ï¼Œæˆ‘ä»¬è®¤ä¸ºå¤„ç†è´¨é‡æ˜¯å®Œç¾çš„
                if cache_row:
                    processing_score = 10.0
                    logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] å¤„ç†è´¨é‡è¯„åˆ†ä¸º 10.0 (å®Œç¾)")
                else:
                    # å¦åˆ™ï¼Œè°ƒç”¨å·¥å…·å‡½æ•°è¿›è¡Œå®é™…è¯„ä¼°
                    processing_score = actor_utils.evaluate_cast_processing_quality(
                        final_cast=final_processed_cast, 
                        original_cast_count=original_emby_actor_count,
                        expected_final_count=len(final_processed_cast), 
                        is_animation=is_animation
                    )
                
                min_score_for_review = float(self.config.get("min_score_for_review", constants.DEFAULT_MIN_SCORE_FOR_REVIEW))
                
                # --- 3. æœ€ç»ˆåˆ¤å®šä¸æ—¥å¿—å†™å…¥ ---
                # ä¼˜å…ˆçº§ï¼šè§†é¢‘æµç¼ºå¤± > è¯„åˆ†è¿‡ä½
                if not stream_check_passed:
                    # æƒ…å†µ A: è§†é¢‘æµç¼ºå¤± -> å¼ºåˆ¶å¾…å¤æ ¸
                    logger.warning(f"  âœ [è´¨æ£€]ã€Š{item_name_for_log}ã€‹å› ç¼ºå¤±è§†é¢‘æµæ•°æ®ï¼Œéœ€é‡æ–°å¤„ç†ã€‚")
                    self.log_db_manager.save_to_failed_log(cursor, item_id, item_name_for_log, stream_fail_reason, item_type, score=0.0)
                    # æ ‡è®°ä¸ºå·²å¤„ç†ï¼Œé˜²æ­¢é‡å¤å¾ªç¯ï¼Œä½†åœ¨UIä¸­ä¼šæ˜¾ç¤ºåœ¨â€œå¾…å¤æ ¸â€åˆ—è¡¨
                    self._mark_item_as_processed(cursor, item_id, item_name_for_log, score=0.0)
                    
                elif processing_score < min_score_for_review:
                    # æƒ…å†µ B: è¯„åˆ†è¿‡ä½ -> å¾…å¤æ ¸
                    reason = f"å¤„ç†è¯„åˆ† ({processing_score:.2f}) ä½äºé˜ˆå€¼ ({min_score_for_review})ã€‚"
                    logger.warning(f"  âœ [è´¨æ£€]ã€Š{item_name_for_log}ã€‹å¤„ç†è´¨é‡ä¸ä½³ï¼Œå·²æ ‡è®°ä¸ºã€å¾…å¤æ ¸ã€‘ã€‚åŸå› : {reason}")
                    self.log_db_manager.save_to_failed_log(cursor, item_id, item_name_for_log, reason, item_type, score=processing_score)
                    self._mark_item_as_processed(cursor, item_id, item_name_for_log, score=processing_score)
                    
                else:
                    # æƒ…å†µ C: ä¸€åˆ‡æ­£å¸¸ -> ç§»é™¤å¾…å¤æ ¸æ ‡è®°ï¼ˆå¦‚æœä¹‹å‰æœ‰ï¼‰
                    logger.info(f"  âœ ã€Š{item_name_for_log}ã€‹è´¨æ£€é€šè¿‡ (è¯„åˆ†: {processing_score:.2f})ï¼Œæ ‡è®°ä¸ºå·²å¤„ç†ã€‚")
                    self._mark_item_as_processed(cursor, item_id, item_name_for_log, score=processing_score)
                    self.log_db_manager.remove_from_failed_log(cursor, item_id)
                
                conn.commit()

            logger.info(f"--- å¤„ç†å®Œæˆ '{item_name_for_log}' ---")

        except (ValueError, InterruptedError) as e:
            logger.warning(f"å¤„ç† '{item_name_for_log}' çš„è¿‡ç¨‹ä¸­æ–­: {e}")
            return False
        except Exception as outer_e:
            logger.error(f"æ ¸å¿ƒå¤„ç†æµç¨‹ä¸­å‘ç”ŸæœªçŸ¥ä¸¥é‡é”™è¯¯ for '{item_name_for_log}': {outer_e}", exc_info=True)
            try:
                with get_central_db_connection() as conn_fail:
                    self.log_db_manager.save_to_failed_log(conn_fail.cursor(), item_id, item_name_for_log, f"æ ¸å¿ƒå¤„ç†å¼‚å¸¸: {str(outer_e)}", item_type)
            except Exception as log_e:
                logger.error(f"å†™å…¥å¾…å¤æ ¸æ—¥å¿—æ—¶å†æ¬¡å‘ç”Ÿé”™è¯¯: {log_e}")
            return False

        logger.trace(f"  âœ… å¤„ç†å®Œæˆ '{item_name_for_log}'")
        return True

    # --- æ ¸å¿ƒå¤„ç†å™¨ ---
    def _process_cast_list(self, tmdb_cast_people: List[Dict[str, Any]],
                                    emby_cast_people: List[Dict[str, Any]],
                                    douban_cast_list: List[Dict[str, Any]],
                                    item_details_from_emby: Dict[str, Any],
                                    cursor: psycopg2.extensions.cursor,
                                    tmdb_api_key: Optional[str],
                                    stop_event: Optional[threading.Event]) -> List[Dict[str, Any]]:
        """
        ã€V-Final with Truncation - Full Codeã€‘
        - åœ¨æ­¥éª¤4çš„å¼€å¤´ï¼Œé‡æ–°åŠ å…¥äº†å¯¹æœ€ç»ˆæ¼”å‘˜åˆ—è¡¨è¿›è¡Œæˆªæ–­çš„é€»è¾‘ã€‚
        - ç¡®ä¿åœ¨è¿›è¡ŒAIç¿»è¯‘ç­‰è€—æ—¶æ“ä½œå‰ï¼Œå°†æ¼”å‘˜æ•°é‡é™åˆ¶åœ¨é…ç½®çš„ä¸Šé™å†…ã€‚
        """
        # --- åœ¨æ‰€æœ‰å¤„ç†å¼€å§‹å‰ï¼Œä»æºå¤´æ¸…æ´—åŒåå¼‚äººæ¼”å‘˜ ---
        logger.debug("  âœ é¢„å¤„ç†ï¼šæ¸…æ´—æºæ•°æ®ä¸­çš„åŒåæ¼”å‘˜ï¼Œåªä¿ç•™orderæœ€å°çš„ä¸€ä¸ªã€‚")
        cleaned_tmdb_cast = []
        seen_names = {} # ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨è§è¿‡çš„åå­—åŠå…¶order
        
        # é¦–å…ˆæŒ‰ order æ’åºï¼Œç¡®ä¿ç¬¬ä¸€ä¸ªé‡åˆ°çš„æ˜¯ order æœ€å°çš„
        tmdb_cast_people.sort(key=lambda x: x.get('order', 999))

        for actor in tmdb_cast_people:
            name = actor.get("name")
            if not name or not isinstance(name, str):
                continue
            
            cleaned_name = name.strip()
            
            if cleaned_name not in seen_names:
                cleaned_tmdb_cast.append(actor)
                seen_names[cleaned_name] = actor.get('order', 999)
            else:
                # è®°å½•è¢«ä¸¢å¼ƒçš„æ¼”å‘˜
                role = actor.get("character", "æœªçŸ¥è§’è‰²")
                logger.info(f"  âœ ä¸ºé¿å…å¼ å† ææˆ´ï¼Œåˆ é™¤åŒåå¼‚äººæ¼”å‘˜: '{cleaned_name}' (è§’è‰²: {role}, order: {actor.get('order', 999)})")

        # ä½¿ç”¨æ¸…æ´—åçš„åˆ—è¡¨è¿›è¡Œåç»­æ‰€æœ‰æ“ä½œ
        tmdb_cast_people = cleaned_tmdb_cast

        # â˜…â˜…â˜… åœ¨æµç¨‹å¼€å§‹æ—¶ï¼Œè®°å½•ä¸‹æ¥è‡ªTMDbçš„åŸå§‹æ¼”å‘˜ID â˜…â˜…â˜…
        original_tmdb_ids = {str(actor.get("id")) for actor in tmdb_cast_people if actor.get("id")}
        # ======================================================================
        # æ­¥éª¤ 1: â˜…â˜…â˜… æ•°æ®é€‚é… â˜…â˜…â˜…
        # ======================================================================
        logger.debug("  âœ å¼€å§‹æ¼”å‘˜æ•°æ®é€‚é… (åæŸ¥ç¼“å­˜æ¨¡å¼)...")
        
        tmdb_actor_map_by_id = {str(actor.get("id")): actor for actor in tmdb_cast_people}
        tmdb_actor_map_by_en_name = {str(actor.get("name") or "").lower().strip(): actor for actor in tmdb_cast_people}

        final_cast_list = []
        used_tmdb_ids = set()

        for emby_actor in emby_cast_people:
            emby_person_id = emby_actor.get("Id")
            emby_tmdb_id = emby_actor.get("ProviderIds", {}).get("Tmdb")
            emby_name_lower = str(emby_actor.get("Name") or "").lower().strip()

            tmdb_match = None

            if emby_tmdb_id and str(emby_tmdb_id) in tmdb_actor_map_by_id:
                tmdb_match = tmdb_actor_map_by_id[str(emby_tmdb_id)]
            else:
                if emby_name_lower in tmdb_actor_map_by_en_name:
                    tmdb_match = tmdb_actor_map_by_en_name[emby_name_lower]
                else:
                    cache_entry = self.actor_db_manager.get_translation_from_db(cursor, emby_actor.get("Name"), by_translated_text=True)
                    if cache_entry and cache_entry.get('original_text'):
                        original_en_name = str(cache_entry['original_text']).lower().strip()
                        if original_en_name in tmdb_actor_map_by_en_name:
                            tmdb_match = tmdb_actor_map_by_en_name[original_en_name]

            if tmdb_match:
                tmdb_id_str = str(tmdb_match.get("id"))
                merged_actor = tmdb_match.copy()
                merged_actor["emby_person_id"] = emby_person_id
                if utils.contains_chinese(emby_actor.get("Name")):
                    merged_actor["name"] = emby_actor.get("Name")
                else:
                    merged_actor["name"] = tmdb_match.get("name")
                merged_actor["character"] = emby_actor.get("Role")
                final_cast_list.append(merged_actor)
                used_tmdb_ids.add(tmdb_id_str)

        for tmdb_id, tmdb_actor_data in tmdb_actor_map_by_id.items():
            if tmdb_id not in used_tmdb_ids:
                new_actor = tmdb_actor_data.copy()
                new_actor["emby_person_id"] = None
                final_cast_list.append(new_actor)

        logger.debug(f"  âœ æ•°æ®é€‚é…å®Œæˆï¼Œç”Ÿæˆäº† {len(final_cast_list)} æ¡åŸºå‡†æ¼”å‘˜æ•°æ®ã€‚")
        
        # ======================================================================
        # æ­¥éª¤ 2: â˜…â˜…â˜… â€œä¸€å¯¹ä¸€åŒ¹é…â€é€»è¾‘ â˜…â˜…â˜…
        # ======================================================================
        douban_candidates = actor_utils.format_douban_cast(douban_cast_list)
        unmatched_local_actors = list(final_cast_list)
        merged_actors = []
        unmatched_douban_actors = []
        logger.info(f"  âœ åŒ¹é…é˜¶æ®µ 1: å¯¹å·å…¥åº§")
        for d_actor in douban_candidates:
            douban_name_zh = d_actor.get("Name", "").lower().strip()
            douban_name_en = d_actor.get("OriginalName", "").lower().strip()
            match_found_for_this_douban_actor = False
            for i, l_actor in enumerate(unmatched_local_actors):
                local_name = str(l_actor.get("name") or "").lower().strip()
                local_original_name = str(l_actor.get("original_name") or "").lower().strip()
                is_match = False
                if douban_name_zh and (douban_name_zh == local_name or douban_name_zh == local_original_name):
                    is_match = True
                elif douban_name_en and (douban_name_en == local_name or douban_name_en == local_original_name):
                    is_match = True
                if is_match:
                    l_actor["name"] = d_actor.get("Name")
                    cleaned_douban_character = utils.clean_character_name_static(d_actor.get("Role"))
                    l_actor["character"] = actor_utils.select_best_role(l_actor.get("character"), cleaned_douban_character)
                    
                    douban_id_to_add = d_actor.get("DoubanCelebrityId")
                    if douban_id_to_add:
                        l_actor["douban_id"] = douban_id_to_add
                    
                    douban_avatar = d_actor.get("DoubanAvatarUrl")
                    if not l_actor.get("profile_path") and douban_avatar:
                        # 1. æ›´æ–°å†…å­˜å¯¹è±¡ï¼Œä¾›æœ¬æ¬¡è¿è¡Œä½¿ç”¨
                        l_actor["profile_path"] = douban_avatar
                        logger.debug(f"    âœ æ¼”å‘˜ '{l_actor.get('name')}' ç¼ºå°‘TMDbå¤´åƒï¼Œå·²ä»è±†ç“£ç¼“å­˜è¡¥å……ã€‚")
                        
                    merged_actors.append(unmatched_local_actors.pop(i))
                    match_found_for_this_douban_actor = True
                    break
            if not match_found_for_this_douban_actor:
                unmatched_douban_actors.append(d_actor)

        current_cast_list = merged_actors + unmatched_local_actors
        final_cast_map = {str(actor['id']): actor for actor in current_cast_list if actor.get('id') and str(actor.get('id')) != 'None'}

        # ======================================================================
        # æ­¥éª¤ 3: â˜…â˜…â˜… å¤„ç†è±†ç“£è¡¥å……æ¼”å‘˜ï¼ˆå¸¦ä¸¢å¼ƒé€»è¾‘ å’Œ æ•°é‡ä¸Šé™é€»è¾‘ï¼‰ â˜…â˜…â˜…
        # ======================================================================
        if not unmatched_douban_actors:
            logger.info("  âœ è±†ç“£APIæœªè¿”å›æ¼”å‘˜æˆ–æ‰€æœ‰æ¼”å‘˜å·²åŒ¹é…ï¼Œè·³è¿‡è¡¥å……æ¼”å‘˜æµç¨‹ã€‚")
        else:
            logger.info(f"  âœ å‘ç° {len(unmatched_douban_actors)} ä½æ½œåœ¨çš„è±†ç“£è¡¥å……æ¼”å‘˜ï¼Œå¼€å§‹æ‰§è¡ŒåŒ¹é…ä¸ç­›é€‰...")
            
            limit = self.config.get(constants.CONFIG_OPTION_MAX_ACTORS_TO_PROCESS, 30)
            try:
                limit = int(limit)
                if limit <= 0: limit = 30
            except (ValueError, TypeError):
                limit = 30

            current_actor_count = len(final_cast_map)
            if current_actor_count >= limit:
                logger.info(f"  âœ å½“å‰æ¼”å‘˜æ•° ({current_actor_count}) å·²è¾¾ä¸Šé™ ({limit})ï¼Œå°†è·³è¿‡æ‰€æœ‰è±†ç“£è¡¥å……æ¼”å‘˜çš„æµç¨‹ã€‚")
                still_unmatched_final = unmatched_douban_actors
            else:
                logger.info(f"  âœ å½“å‰æ¼”å‘˜æ•° ({current_actor_count}) ä½äºä¸Šé™ ({limit})ï¼Œè¿›å…¥è¡¥å……æ¨¡å¼ã€‚")
                
                logger.info(f"  âœ åŒ¹é…é˜¶æ®µ 2: ç”¨è±†ç“£IDæŸ¥'æ¼”å‘˜æ˜ å°„è¡¨' ({len(unmatched_douban_actors)} ä½æ¼”å‘˜)")
                still_unmatched = []
                for d_actor in unmatched_douban_actors:
                    if self.is_stop_requested(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                    d_douban_id = d_actor.get("DoubanCelebrityId")
                    match_found = False
                    if d_douban_id:
                        entry = self.actor_db_manager.find_person_by_any_id(cursor, douban_id=d_douban_id)
                        if entry and entry.get("tmdb_person_id") and entry.get("emby_person_id"):
                            tmdb_id_from_map = str(entry.get("tmdb_person_id"))
                            if tmdb_id_from_map not in final_cast_map:
                                logger.info(f"    â”œâ”€ åŒ¹é…æˆåŠŸ (é€šè¿‡ è±†ç“£IDæ˜ å°„): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> åŠ å…¥æœ€ç»ˆæ¼”å‘˜è¡¨")
                                cached_metadata_map = self.actor_db_manager.get_full_actor_details_by_tmdb_ids(cursor, [int(tmdb_id_from_map)])
                                cached_metadata = cached_metadata_map.get(int(tmdb_id_from_map), {})
                                new_actor_entry = {
                                    "id": tmdb_id_from_map, "name": d_actor.get("Name"),
                                    "original_name": cached_metadata.get("original_name") or d_actor.get("OriginalName"),
                                    "character": d_actor.get("Role"), "order": 999,
                                    "imdb_id": entry.get("imdb_id"), "douban_id": d_douban_id,
                                    "emby_person_id": entry.get("emby_person_id")
                                }
                                final_cast_map[tmdb_id_from_map] = new_actor_entry
                            match_found = True
                    if not match_found:
                        still_unmatched.append(d_actor)
                unmatched_douban_actors = still_unmatched

                logger.info(f"  âœ åŒ¹é…é˜¶æ®µ 3: ç”¨IMDb IDè¿›è¡Œæœ€ç»ˆåŒ¹é…å’Œæ–°å¢ ({len(unmatched_douban_actors)} ä½æ¼”å‘˜)")
                still_unmatched_final = []
                for i, d_actor in enumerate(unmatched_douban_actors):
                    if self.is_stop_requested(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                    
                    if len(final_cast_map) >= limit:
                        logger.info(f"  âœ æ¼”å‘˜æ•°å·²è¾¾ä¸Šé™ ({limit})ï¼Œè·³è¿‡å‰©ä½™ {len(unmatched_douban_actors) - i} ä½æ¼”å‘˜çš„APIæŸ¥è¯¢ã€‚")
                        still_unmatched_final.extend(unmatched_douban_actors[i:])
                        break

                    d_douban_id = d_actor.get("DoubanCelebrityId")
                    match_found = False
                    if d_douban_id and self.douban_api and self.tmdb_api_key:
                        if self.is_stop_requested(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                        details = self.douban_api.celebrity_details(d_douban_id)
                        time_module.sleep(0.3)
                        d_imdb_id = None
                        if details and not details.get("error"):
                            try:
                                info_list = details.get("extra", {}).get("info", [])
                                if isinstance(info_list, list):
                                    for item in info_list:
                                        if isinstance(item, list) and len(item) == 2 and item[0] == 'IMDbç¼–å·':
                                            d_imdb_id = item[1]
                                            break
                            except Exception as e_parse:
                                logger.warning(f"  âœ è§£æ IMDb ID æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e_parse}")
                        
                        if d_imdb_id:
                            logger.debug(f"  âœ ä¸º '{d_actor.get('Name')}' è·å–åˆ° IMDb ID: {d_imdb_id}ï¼Œå¼€å§‹åŒ¹é…...")
                            
                            entry_from_map = self.actor_db_manager.find_person_by_any_id(cursor, imdb_id=d_imdb_id)
                            if entry_from_map and entry_from_map.get("tmdb_person_id") and entry_from_map.get("emby_person_id"):
                                tmdb_id_from_map = str(entry_from_map.get("tmdb_person_id"))
                                if tmdb_id_from_map not in final_cast_map:
                                    logger.debug(f"    â”œâ”€ åŒ¹é…æˆåŠŸ (é€šè¿‡ IMDbæ˜ å°„): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> åŠ å…¥æœ€ç»ˆæ¼”å‘˜è¡¨")
                                    new_actor_entry = {
                                        "id": tmdb_id_from_map, "name": d_actor.get("Name"),
                                        "character": d_actor.get("Role"), "order": 999, "imdb_id": d_imdb_id,
                                        "douban_id": d_douban_id, "emby_person_id": entry_from_map.get("emby_person_id")
                                    }
                                    final_cast_map[tmdb_id_from_map] = new_actor_entry
                                match_found = True
                            
                            if not match_found:
                                logger.debug(f"  âœ æ•°æ®åº“æœªæ‰¾åˆ° {d_imdb_id} çš„æ˜ å°„ï¼Œå¼€å§‹é€šè¿‡ TMDb API åæŸ¥...")
                                if self.is_stop_requested(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                                person_from_tmdb = tmdb.find_person_by_external_id(
                                    external_id=d_imdb_id, api_key=self.tmdb_api_key, source="imdb_id"
                                )
                                if person_from_tmdb and person_from_tmdb.get("id"):
                                    tmdb_id_from_find = str(person_from_tmdb.get("id"))
                                    
                                    d_actor['tmdb_id_from_api'] = tmdb_id_from_find
                                    d_actor['imdb_id_from_api'] = d_imdb_id

                                    final_check_row = self.actor_db_manager.find_person_by_any_id(cursor, tmdb_id=tmdb_id_from_find)
                                    if final_check_row and dict(final_check_row).get("emby_person_id"):
                                        emby_pid_from_final_check = dict(final_check_row).get("emby_person_id")
                                        if tmdb_id_from_find not in final_cast_map:
                                            logger.info(f"    â”œâ”€ åŒ¹é…æˆåŠŸ (é€šè¿‡ TMDbåæŸ¥): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> åŠ å…¥æœ€ç»ˆæ¼”å‘˜è¡¨")
                                            new_actor_entry = {
                                                "id": tmdb_id_from_find, "name": d_actor.get("Name"),
                                                "character": d_actor.get("Role"), "order": 999,
                                                "imdb_id": d_imdb_id, "douban_id": d_douban_id,
                                                "emby_person_id": emby_pid_from_final_check
                                            }
                                            final_cast_map[tmdb_id_from_find] = new_actor_entry
                                        match_found = True
                    
                    if not match_found:
                        still_unmatched_final.append(d_actor)

                # --- å¤„ç†æ–°å¢ ---
                if still_unmatched_final:
                    logger.info(f"  âœ æ£€æŸ¥ {len(still_unmatched_final)} ä½æœªåŒ¹é…æ¼”å‘˜ï¼Œå°è¯•åˆå¹¶æˆ–åŠ å…¥æœ€ç»ˆåˆ—è¡¨...")
                    added_count = 0
                    merged_count = 0
                    
                    for d_actor in still_unmatched_final:
                        tmdb_id_to_process = d_actor.get('tmdb_id_from_api')
                        if tmdb_id_to_process:
                            # æƒ…å†µä¸€ï¼šæ¼”å‘˜å·²å­˜åœ¨ï¼Œæ‰§è¡Œåˆå¹¶/æ›´æ–°
                            if tmdb_id_to_process in final_cast_map:
                                existing_actor = final_cast_map[tmdb_id_to_process]
                                original_name = existing_actor.get("name")
                                new_name = d_actor.get("Name")
                                
                                # ä»…å½“è±†ç“£æä¾›äº†æ›´ä¼˜çš„åå­—ï¼ˆå¦‚ä¸­æ–‡åï¼‰æ—¶æ‰æ›´æ–°
                                if new_name and new_name != original_name and utils.contains_chinese(new_name):
                                    existing_actor["name"] = new_name
                                    logger.debug(f"    âœ [åˆå¹¶] å·²å°†æ¼”å‘˜ (TMDb ID: {tmdb_id_to_process}) çš„åå­—ä» '{original_name}' æ›´æ–°ä¸º '{new_name}'")
                                    merged_count += 1
                            
                            # æƒ…å†µäºŒï¼šæ¼”å‘˜ä¸å­˜åœ¨ï¼Œæ‰§è¡Œæ–°å¢
                            else:
                                new_actor_entry = {
                                    "id": tmdb_id_to_process,
                                    "name": d_actor.get("Name"),
                                    "character": d_actor.get("Role"),
                                    "order": 999,
                                    "imdb_id": d_actor.get("imdb_id_from_api"),
                                    "douban_id": d_actor.get("DoubanCelebrityId"),
                                    "emby_person_id": None
                                }
                                final_cast_map[tmdb_id_to_process] = new_actor_entry
                                added_count += 1
                    
                    if merged_count > 0:
                        logger.info(f"  âœ æˆåŠŸåˆå¹¶äº† {merged_count} ä½ç°æœ‰æ¼”å‘˜çš„è±†ç“£ä¿¡æ¯ã€‚")
                    if added_count > 0:
                        logger.info(f"  âœ æˆåŠŸæ–°å¢äº† {added_count} ä½æ¼”å‘˜åˆ°æœ€ç»ˆåˆ—è¡¨ã€‚")
        
        # ======================================================================
        # æ­¥éª¤ 4: â˜…â˜…â˜… ä»TMDbè¡¥å…¨å¤´åƒ â˜…â˜…â˜…
        # ======================================================================
        current_cast_list = list(final_cast_map.values())
        
        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 2/3: ç­›é€‰éœ€è¦è¡¥å…¨çš„æ¼”å‘˜æ—¶ï¼Œæ’é™¤æ‰åŸå§‹TMDbåˆ—è¡¨ä¸­çš„æ¼”å‘˜ â˜…â˜…â˜…
        actors_to_supplement = [
            actor for actor in current_cast_list 
            if str(actor.get("id")) not in original_tmdb_ids and actor.get("id")
        ]
        
        if actors_to_supplement:
            total_to_supplement = len(actors_to_supplement)
            logger.info(f"  âœ å¼€å§‹ä¸º {total_to_supplement} ä½æ–°å¢æ¼”å‘˜æ£€æŸ¥å¹¶è¡¥å…¨å¤´åƒä¿¡æ¯...")

            ids_to_fetch = [actor.get("id") for actor in actors_to_supplement if actor.get("id")]
            all_cached_metadata = self.actor_db_manager.get_full_actor_details_by_tmdb_ids(cursor, ids_to_fetch)
            
            supplemented_count = 0
            for actor in actors_to_supplement:
                if stop_event and stop_event.is_set(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                
                tmdb_id = actor.get("id")
                profile_path = None
                cached_meta = all_cached_metadata.get(tmdb_id)
                if cached_meta and cached_meta.get("profile_path"):
                    profile_path = cached_meta["profile_path"]
                
                elif tmdb_api_key:
                    person_details = tmdb.get_person_details_tmdb(tmdb_id, tmdb_api_key)
                    if person_details:
                        if person_details.get("profile_path"):
                            profile_path = person_details["profile_path"]
                
                if profile_path:
                    actor["profile_path"] = profile_path
                    supplemented_count += 1

            logger.info(f"  âœ æ–°å¢æ¼”å‘˜å¤´åƒä¿¡æ¯è¡¥å…¨å®Œæˆï¼ŒæˆåŠŸä¸º {supplemented_count}/{total_to_supplement} ä½æ¼”å‘˜è¡¥å……äº†å¤´åƒã€‚")
        else:
            logger.info("  âœ æ²¡æœ‰éœ€è¦è¡¥å……å¤´åƒçš„æ–°å¢æ¼”å‘˜ã€‚")

        # ======================================================================
        # æ­¥éª¤ 5: â˜…â˜…â˜… ä»è±†ç“£è¡¥å…¨å¤´åƒ â˜…â˜…â˜…
        # ======================================================================
        if self.douban_api:
            actors_needing_avatar_check = [
                actor for actor in current_cast_list
                if not actor.get("profile_path") and actor.get("id")
            ]
            
            if actors_needing_avatar_check:
                logger.info(f"  âœ å‘ç° {len(actors_needing_avatar_check)} ä½æ¼”å‘˜ä»æ— å¤´åƒï¼Œå¯åŠ¨æœ€ç»ˆæ£€æŸ¥...")
                douban_avatars_found = 0
                for actor in actors_needing_avatar_check:
                    if stop_event and stop_event.is_set(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                    
                    douban_id = actor.get("douban_id")
                    # å¦‚æœæ¼”å‘˜èº«ä¸Šæ²¡æœ‰é¢„å…ˆå…³è”çš„è±†ç“£IDï¼Œå°±å»æ˜ å°„è¡¨é‡ŒæŸ¥ä¸€æ¬¡
                    if not douban_id:
                        tmdb_id_to_check = actor.get("id")
                        if tmdb_id_to_check:
                            map_entry = self.actor_db_manager.find_person_by_any_id(cursor, tmdb_id=tmdb_id_to_check)
                            if map_entry and map_entry.get("douban_celebrity_id"):
                                douban_id = map_entry.get("douban_celebrity_id")
                                logger.debug(f"    âœ ä¸ºæ¼”å‘˜ '{actor.get('name')}' (TMDb ID: {actor.get('id')}) ä»æ˜ å°„è¡¨æ‰¾åˆ°è±†ç“£ID: {douban_id}")

                    if douban_id:
                        try:
                            details = self.douban_api.celebrity_details(douban_id)
                            time_module.sleep(0.3)
                            
                            if details and not details.get("error"):
                                avatar_url = (details.get("avatars", {}) or {}).get("large")
                                if avatar_url:
                                    actor["profile_path"] = avatar_url
                                    douban_avatars_found += 1
                                    
                        except Exception as e_douban_avatar:
                            logger.warning(f"    âœ ä¸ºæ¼”å‘˜ (è±†ç“£ID: {douban_id}) è·å–è±†ç“£å¤´åƒæ—¶å‘ç”Ÿé”™è¯¯: {e_douban_avatar}")

                if douban_avatars_found > 0:
                    logger.info(f"  âœ æˆåŠŸä¸º {douban_avatars_found} ä½æ¼”å‘˜è¡¥å……å¹¶ç¼“å­˜äº†è±†ç“£å¤‡ç”¨å¤´åƒã€‚")
            else:
                logger.info("  âœ æ— éœ€ä»è±†ç“£è¡¥å……å¤‡ç”¨å¤´åƒã€‚")

        # ======================================================================
        # æ­¥éª¤ 6: â˜…â˜…â˜… ä»æ¼”å‘˜è¡¨ç§»é™¤æ— å¤´åƒæ¼”å‘˜ â˜…â˜…â˜…
        # ======================================================================
        if self.config.get(constants.CONFIG_OPTION_REMOVE_ACTORS_WITHOUT_AVATARS, True):
            actors_with_avatars = [actor for actor in current_cast_list if actor.get("profile_path")]
            actors_without_avatars = [actor for actor in current_cast_list if not actor.get("profile_path")]

            if actors_without_avatars:
                removed_names = [a.get('name', f"TMDbID:{a.get('id')}") for a in actors_without_avatars]
                logger.info(f"  âœ å°†ç§»é™¤ {len(actors_without_avatars)} ä½æ— å¤´åƒçš„æ¼”å‘˜: {removed_names}")
                current_cast_list = actors_with_avatars
        else:
            logger.info("  âœ æœªå¯ç”¨ç§»é™¤æ— å¤´åƒæ¼”å‘˜ã€‚")

        # ======================================================================
        # æ­¥éª¤ 7ï¼šæ™ºèƒ½æˆªæ–­é€»è¾‘ (Smart Truncation) â˜…â˜…â˜…
        # ======================================================================
        max_actors = self.config.get(constants.CONFIG_OPTION_MAX_ACTORS_TO_PROCESS, 30)
        try:
            limit = int(max_actors)
            if limit <= 0: limit = 30
        except (ValueError, TypeError):
            limit = 30

        original_count = len(current_cast_list)
        
        if original_count > limit:
            logger.info(f"  âœ æ¼”å‘˜åˆ—è¡¨æ€»æ•° ({original_count}) è¶…è¿‡ä¸Šé™ ({limit})ï¼Œå°†ä¼˜å…ˆä¿ç•™æœ‰å¤´åƒçš„æ¼”å‘˜åè¿›è¡Œæˆªæ–­ã€‚")
            sort_key = lambda x: x.get('order') if x.get('order') is not None and x.get('order') >= 0 else 999
            with_profile = [actor for actor in current_cast_list if actor.get("profile_path")]
            without_profile = [actor for actor in current_cast_list if not actor.get("profile_path")]
            with_profile.sort(key=sort_key)
            without_profile.sort(key=sort_key)
            prioritized_list = with_profile + without_profile
            current_cast_list = prioritized_list[:limit]
            logger.debug(f"  âœ æˆªæ–­åï¼Œä¿ç•™äº† {len(with_profile)} ä½æœ‰å¤´åƒæ¼”å‘˜ä¸­çš„ {len([a for a in current_cast_list if a.get('profile_path')])} ä½ã€‚")
        else:
            # â–¼â–¼â–¼ æ ¸å¿ƒä¿®æ”¹ï¼šç›´æ¥åœ¨ current_cast_list ä¸Šæ’åº â–¼â–¼â–¼
            current_cast_list.sort(key=lambda x: x.get('order') if x.get('order') is not None and x.get('order') >= 0 else 999)

        # ======================================================================
        # æ­¥éª¤ 8: â˜…â˜…â˜… ç¿»è¯‘å’Œæ ¼å¼åŒ– â˜…â˜…â˜…
        # ======================================================================
        logger.info(f"  âœ å°†å¯¹ {len(current_cast_list)} ä½æ¼”å‘˜è¿›è¡Œæœ€ç»ˆçš„ç¿»è¯‘å’Œæ ¼å¼åŒ–å¤„ç†...")

        if not (self.ai_translator and self.config.get(constants.CONFIG_OPTION_AI_TRANSLATION_ENABLED, False)):
            logger.info("  âœ AIç¿»è¯‘æœªå¯ç”¨ï¼Œå°†ä¿ç•™æ¼”å‘˜å’Œè§’è‰²ååŸæ–‡ã€‚")
        else:
            final_translation_map = {}
            terms_to_translate = set()
            for actor in current_cast_list:
                character = actor.get('character')
                if character:
                    cleaned_character = utils.clean_character_name_static(character)
                    if cleaned_character and not utils.contains_chinese(cleaned_character):
                        terms_to_translate.add(cleaned_character)
                name = actor.get('name')
                if name and not utils.contains_chinese(name):
                    terms_to_translate.add(name)
            
            total_terms_count = len(terms_to_translate)
            logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 1. ä»»åŠ¡æ¦‚è§ˆ: å…±æ”¶é›†åˆ° {total_terms_count} ä¸ªç‹¬ç«‹è¯æ¡éœ€è¦ç¿»è¯‘ã€‚")
            if total_terms_count > 0:
                logger.debug(f"    âœ å¾…å¤„ç†è¯æ¡åˆ—è¡¨: {list(terms_to_translate)}")

            remaining_terms = list(terms_to_translate)
            if remaining_terms:
                cached_results = {}
                terms_for_api = []
                for term in remaining_terms:
                    cached = self.actor_db_manager.get_translation_from_db(cursor, term)
                    if cached and cached.get('translated_text'):
                        cached_results[term] = cached['translated_text']
                    else:
                        terms_for_api.append(term)
                
                cached_count = len(cached_results)
                logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 2. ç¼“å­˜æ£€æŸ¥: å‘½ä¸­æ•°æ®åº“ç¼“å­˜ {cached_count} æ¡ã€‚")
                if cached_count > 0:
                    logger.debug("    âœ å‘½ä¸­ç¼“å­˜çš„è¯æ¡ä¸è¯‘æ–‡:")
                    for k, v in sorted(cached_results.items()):
                        logger.debug(f"    â”œâ”€ {k} âœ {v}")

                if cached_results:
                    final_translation_map.update(cached_results)
                if terms_for_api:
                    logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 3. AIå¤„ç† (å¿«é€Ÿæ¨¡å¼): æäº¤ {len(terms_for_api)} æ¡ã€‚")
                    if terms_for_api:
                        logger.debug(f"    âœ æäº¤ç»™[å¿«é€Ÿæ¨¡å¼]çš„è¯æ¡: {terms_for_api}")
                    fast_api_results = self.ai_translator.batch_translate(terms_for_api, mode='fast')
                    for term, translation in fast_api_results.items():
                        final_translation_map[term] = translation
                        self.actor_db_manager.save_translation_to_db(cursor, term, translation, self.ai_translator.provider)
                failed_terms = []
                for term in remaining_terms:
                    if not utils.contains_chinese(final_translation_map.get(term, term)):
                        failed_terms.append(term)
                remaining_terms = failed_terms
            if remaining_terms:
                logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 4. AIå¤„ç† (éŸ³è¯‘æ¨¡å¼): æäº¤ {len(remaining_terms)} æ¡ã€‚")
                if remaining_terms:
                    logger.debug(f"    âœ æäº¤ç»™[éŸ³è¯‘æ¨¡å¼]çš„è¯æ¡: {remaining_terms}")
                transliterate_results = self.ai_translator.batch_translate(remaining_terms, mode='transliterate')
                final_translation_map.update(transliterate_results)
                still_failed_terms = []
                for term in remaining_terms:
                    if not utils.contains_chinese(final_translation_map.get(term, term)):
                        still_failed_terms.append(term)
                remaining_terms = still_failed_terms
            if remaining_terms:
                item_title = item_details_from_emby.get("Name")
                item_year = item_details_from_emby.get("ProductionYear")
                logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 5. AIå¤„ç† (é¡¾é—®æ¨¡å¼): æäº¤ {len(remaining_terms)} æ¡ã€‚")
                if remaining_terms:
                    logger.debug(f"  âœ æäº¤ç»™[é¡¾é—®æ¨¡å¼]çš„è¯æ¡: {remaining_terms}")
                quality_results = self.ai_translator.batch_translate(remaining_terms, mode='quality', title=item_title, year=item_year)
                final_translation_map.update(quality_results)
            
            successfully_translated_terms = {term for term in terms_to_translate if utils.contains_chinese(final_translation_map.get(term, ''))}
            failed_to_translate_terms = terms_to_translate - successfully_translated_terms
            
            logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 6. ç»“æœæ€»ç»“: æˆåŠŸç¿»è¯‘ {len(successfully_translated_terms)}/{total_terms_count} ä¸ªè¯æ¡ã€‚")
            if successfully_translated_terms:
                logger.debug("  âœ ç¿»è¯‘æˆåŠŸåˆ—è¡¨ (åŸæ–‡ âœ è¯‘æ–‡):")
                for term in sorted(list(successfully_translated_terms)):
                    translation = final_translation_map.get(term)
                    logger.debug(f"    â”œâ”€ {term} âœ {translation}")
            if failed_to_translate_terms:
                logger.warning(f"    âœ ç¿»è¯‘å¤±è´¥åˆ—è¡¨ ({len(failed_to_translate_terms)}æ¡): {list(failed_to_translate_terms)}")

            for actor in current_cast_list:
                original_name = actor.get('name')
                if original_name and original_name in final_translation_map:
                    actor['name'] = final_translation_map[original_name]
                original_character = actor.get('character')
                if original_character:
                    cleaned_character = utils.clean_character_name_static(original_character)
                    actor['character'] = final_translation_map.get(cleaned_character, cleaned_character)
                else:
                    actor['character'] = ''

        tmdb_to_emby_id_map = {
            str(actor.get('id')): actor.get('emby_person_id')
            for actor in current_cast_list if actor.get('id') and actor.get('emby_person_id')
        }
        genres = item_details_from_emby.get("Genres", [])
        is_animation = "Animation" in genres or "åŠ¨ç”»" in genres or "Documentary" in genres or "çºªå½•" in genres
        final_cast_perfect = actor_utils.format_and_complete_cast_list(
            current_cast_list, is_animation, self.config, mode='auto'
        )
        for actor in final_cast_perfect:
            tmdb_id_str = str(actor.get("id"))
            if tmdb_id_str in tmdb_to_emby_id_map:
                actor["emby_person_id"] = tmdb_to_emby_id_map[tmdb_id_str]
        for actor in final_cast_perfect:
            actor["provider_ids"] = {
                "Tmdb": str(actor.get("id")),
                "Imdb": actor.get("imdb_id"),
                "Douban": actor.get("douban_id")
            }

        # ======================================================================
        # æ­¥éª¤ 9: â˜…â˜…â˜… æœ€ç»ˆæ•°æ®å›å†™/åå“º â˜…â˜…â˜… 
        # ======================================================================
        logger.info(f"  âœ å¼€å§‹å°† {len(final_cast_perfect)} ä½æœ€ç»ˆæ¼”å‘˜çš„å®Œæ•´ä¿¡æ¯åŒæ­¥å›æ•°æ®åº“...")
        processed_count = 0
        
        # åœ¨å¾ªç¯å¤–å‡†å¤‡ emby_configï¼Œé¿å…é‡å¤åˆ›å»º
        emby_config_for_upsert = {"url": self.emby_url, "api_key": self.emby_api_key, "user_id": self.emby_user_id}

        for actor in final_cast_perfect:
            # ç›´æ¥å°† actor å­—å…¸å’Œ emby_config ä¼ é€’ç»™ upsert_person å‡½æ•°
            map_id, action = self.actor_db_manager.upsert_person(cursor, actor, emby_config_for_upsert)
            
            if action not in ["ERROR", "SKIPPED", "CONFLICT_ERROR", "UNKNOWN_ERROR"]:
                processed_count += 1
            else:
                # å¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œå›æ»šå½“å‰æ¼”å‘˜çš„æ“ä½œï¼Œå¹¶ä¸ºä¸‹ä¸€ä¸ªæ¼”å‘˜å¼€å¯æ–°äº‹åŠ¡
                # è¿™æ˜¯ä¸ºäº†é˜²æ­¢ä¸€ä¸ªæ¼”å‘˜çš„é”™è¯¯å¯¼è‡´æ•´ä¸ªæ‰¹æ¬¡å¤±è´¥
                cursor.connection.rollback()
                cursor.execute("BEGIN")

        logger.info(f"  âœ æˆåŠŸå¤„ç†äº† {processed_count} ä½æ¼”å‘˜çš„æ•°æ®åº“å›å†™/æ›´æ–°ã€‚")

        return final_cast_perfect
    
    # --- ä¸€é”®ç¿»è¯‘ ---
    def translate_cast_list_for_editing(self, 
                                    cast_list: List[Dict[str, Any]], 
                                    title: Optional[str] = None, 
                                    year: Optional[int] = None,
                                    tmdb_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        ã€V14 - çº¯AIç¿»è¯‘ç‰ˆã€‘ä¸ºæ‰‹åŠ¨ç¼–è¾‘é¡µé¢æä¾›çš„ä¸€é”®ç¿»è¯‘åŠŸèƒ½ã€‚
        - å½»åº•ç§»é™¤ä¼ ç»Ÿç¿»è¯‘å¼•æ“çš„é™çº§é€»è¾‘ã€‚
        - å¦‚æœAIç¿»è¯‘æœªå¯ç”¨æˆ–å¤±è´¥ï¼Œåˆ™ç›´æ¥æ”¾å¼ƒç¿»è¯‘ã€‚
        """
        if not cast_list:
            return []

        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 1: æ£€æŸ¥AIç¿»è¯‘æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœæœªå¯ç”¨åˆ™ç›´æ¥è¿”å› â˜…â˜…â˜…
        if not self.ai_translator or not self.config.get(constants.CONFIG_OPTION_AI_TRANSLATION_ENABLED, False):
            logger.info("æ‰‹åŠ¨ç¼–è¾‘-ä¸€é”®ç¿»è¯‘ï¼šAIç¿»è¯‘æœªå¯ç”¨ï¼Œä»»åŠ¡è·³è¿‡ã€‚")
            # å¯ä»¥åœ¨è¿™é‡Œè¿”å›ä¸€ä¸ªæç¤ºç»™å‰ç«¯ï¼Œæˆ–è€…ç›´æ¥è¿”å›åŸå§‹åˆ—è¡¨
            # ä¸ºäº†å‰ç«¯ä½“éªŒï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç¬¬ä¸€ä¸ªéœ€è¦ç¿»è¯‘çš„æ¼”å‘˜ä¸ŠåŠ ä¸€ä¸ªçŠ¶æ€
            translated_cast_for_status = [dict(actor) for actor in cast_list]
            for actor in translated_cast_for_status:
                name_needs_translation = actor.get('name') and not utils.contains_chinese(actor.get('name'))
                role_needs_translation = actor.get('role') and not utils.contains_chinese(actor.get('role'))
                if name_needs_translation or role_needs_translation:
                    actor['matchStatus'] = 'AIæœªå¯ç”¨'
                    break # åªæ ‡è®°ç¬¬ä¸€ä¸ªå³å¯
            return translated_cast_for_status

        # ä»é…ç½®ä¸­è¯»å–æ¨¡å¼
        translation_mode = self.config.get(constants.CONFIG_OPTION_AI_TRANSLATION_MODE, "fast")
        
        context_log = f" (ä¸Šä¸‹æ–‡: {title} {year})" if title and translation_mode == 'quality' else ""
        logger.info(f"æ‰‹åŠ¨ç¼–è¾‘-ä¸€é”®ç¿»è¯‘ï¼šå¼€å§‹æ‰¹é‡å¤„ç† {len(cast_list)} ä½æ¼”å‘˜ (æ¨¡å¼: {translation_mode}){context_log}ã€‚")
        
        translated_cast = [dict(actor) for actor in cast_list]
        
        # --- çº¯AIæ‰¹é‡ç¿»è¯‘é€»è¾‘ ---
        try:
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                
                translation_cache = {} # æœ¬æ¬¡è¿è¡Œçš„å†…å­˜ç¼“å­˜
                texts_to_translate = set()

                # 1. æ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„è¯æ¡
                texts_to_collect = set()
                for actor in translated_cast:
                    for field_key in ['name', 'role']:
                        text = actor.get(field_key, '').strip()
                        if field_key == 'role':
                            text = utils.clean_character_name_static(text)
                        if text and not utils.contains_chinese(text):
                            texts_to_collect.add(text)

                # 2. æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦ä½¿ç”¨ç¼“å­˜
                if translation_mode == 'fast':
                    logger.debug("[å¿«é€Ÿæ¨¡å¼] æ­£åœ¨æ£€æŸ¥å…¨å±€ç¿»è¯‘ç¼“å­˜...")
                    for text in texts_to_collect:
                        cached_entry = self.actor_db_manager.get_translation_from_db(cursor=cursor, text=text)
                        if cached_entry:
                            translation_cache[text] = cached_entry.get("translated_text")
                        else:
                            texts_to_translate.add(text)
                else: # 'quality' mode
                    logger.debug("[é¡¾é—®æ¨¡å¼] è·³è¿‡ç¼“å­˜æ£€æŸ¥ï¼Œç›´æ¥ç¿»è¯‘æ‰€æœ‰è¯æ¡ã€‚")
                    texts_to_translate = texts_to_collect

                # 3. å¦‚æœæœ‰éœ€è¦ç¿»è¯‘çš„è¯æ¡ï¼Œè°ƒç”¨AI
                if texts_to_translate:
                    logger.info(f"æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šå°† {len(texts_to_translate)} ä¸ªè¯æ¡æäº¤ç»™AI (æ¨¡å¼: {translation_mode})ã€‚")
                    translation_map_from_api = self.ai_translator.batch_translate(
                        texts=list(texts_to_translate),
                        mode=translation_mode,
                        title=title,
                        year=year
                    )
                    if translation_map_from_api:
                        translation_cache.update(translation_map_from_api)
                        
                        if translation_mode == 'fast':
                            for original, translated in translation_map_from_api.items():
                                self.actor_db_manager.save_translation_to_db(
                                    cursor=cursor,
                                    original_text=original, 
                                    translated_text=translated, 
                                    engine_used=self.ai_translator.provider
                                )
                    else:
                        logger.warning("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šAIæ‰¹é‡ç¿»è¯‘æœªè¿”å›ä»»ä½•ç»“æœã€‚")
                else:
                    logger.info("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šæ‰€æœ‰è¯æ¡å‡åœ¨ç¼“å­˜ä¸­æ‰¾åˆ°ï¼Œæ— éœ€è°ƒç”¨APIã€‚")

                # 4. å›å¡«æ‰€æœ‰ç¿»è¯‘ç»“æœ
                if translation_cache:
                    for i, actor in enumerate(translated_cast):
                        original_name = actor.get('name', '').strip()
                        if original_name in translation_cache:
                            translated_cast[i]['name'] = translation_cache[original_name]
                        
                        original_role_raw = actor.get('role', '').strip()
                        cleaned_original_role = utils.clean_character_name_static(original_role_raw)
                        
                        if cleaned_original_role in translation_cache:
                            translated_cast[i]['role'] = translation_cache[cleaned_original_role]
                        
                        if translated_cast[i].get('name') != actor.get('name') or translated_cast[i].get('role') != actor.get('role'):
                            translated_cast[i]['matchStatus'] = 'å·²ç¿»è¯‘'
        
        except Exception as e:
            logger.error(f"ä¸€é”®ç¿»è¯‘æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            # å¯ä»¥åœ¨è¿™é‡Œç»™å‡ºä¸€ä¸ªé”™è¯¯æç¤º
            for actor in translated_cast:
                actor['matchStatus'] = 'ç¿»è¯‘å‡ºé”™'
                break
            return translated_cast

        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 2: å½»åº•åˆ é™¤é™çº§é€»è¾‘ â˜…â˜…â˜…
        # åŸæœ‰çš„ if not ai_translation_succeeded: ... else ... ä»£ç å—å·²å…¨éƒ¨ç§»é™¤ã€‚

        logger.info("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘å®Œæˆã€‚")
        return translated_cast
    
    # âœ¨âœ¨âœ¨æ‰‹åŠ¨å¤„ç†âœ¨âœ¨âœ¨
    def process_item_with_manual_cast(self, item_id: str, manual_cast_list: List[Dict[str, Any]], item_name: str) -> bool:
        """
        ã€V2.5 - ç»ˆæä¿®å¤ç‰ˆã€‘
        1. å¢åŠ äº†å®Œæ•´çš„æ—¥å¿—è®°å½•ï¼Œè®©æ¯ä¸€æ­¥æ“ä½œéƒ½æ¸…æ™°å¯è§ã€‚
        2. ä¿®å¤å¹¶å¼ºåŒ–äº†â€œç¿»è¯‘ç¼“å­˜åå“ºâ€åŠŸèƒ½ã€‚
        3. å¢åŠ äº†åœ¨å†™å…¥æ–‡ä»¶å‰çš„å¼ºåˆ¶â€œæœ€ç»ˆæ ¼å¼åŒ–â€æ­¥éª¤ï¼Œç¡®ä¿å‰ç¼€æ°¸è¿œæ­£ç¡®ã€‚
        """
        logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†æµç¨‹å¯åŠ¨ï¼šItemID: {item_id} ('{item_name}')")
        
        try:
            item_details = emby.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id)
            if not item_details: raise ValueError(f"æ— æ³•è·å–é¡¹ç›® {item_id} çš„è¯¦æƒ…ã€‚")
            
            raw_emby_actors = [p for p in item_details.get("People", []) if p.get("Type") == "Actor"]
            emby_config = {"url": self.emby_url, "api_key": self.emby_api_key, "user_id": self.emby_user_id}

            # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹: åœ¨æ‰€æœ‰æ“ä½œå¼€å§‹å‰ï¼Œä¸€æ¬¡æ€§è·å–æ‰€æœ‰ enriched_actors â˜…â˜…â˜…
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                enriched_actors = self.actor_db_manager.enrich_actors_with_provider_ids(cursor, raw_emby_actors, emby_config)

            # ======================================================================
            # æ­¥éª¤ 1: æ•°æ®å‡†å¤‡ä¸å®šä½ (ç°åœ¨åªè´Ÿè´£æ„å»ºæ˜ å°„)
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 1/6: æ„å»ºTMDbä¸Embyæ¼”å‘˜çš„IDæ˜ å°„...")
            tmdb_to_emby_map = {}
            for person in enriched_actors:
                person_tmdb_id = (person.get("ProviderIds") or {}).get("Tmdb")
                if person_tmdb_id:
                    tmdb_to_emby_map[str(person_tmdb_id)] = person.get("Id")
            logger.info(f"  âœ æˆåŠŸæ„å»ºäº† {len(tmdb_to_emby_map)} æ¡IDæ˜ å°„ã€‚")
            
            item_type = item_details.get("Type")
            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            if not tmdb_id: raise ValueError(f"é¡¹ç›® {item_id} ç¼ºå°‘ TMDb IDã€‚")

            # --- æ–°å¢ï¼šè·å– TMDb è¯¦æƒ…ç”¨äºåˆ†çº§æ•°æ®æå– ---
            tmdb_details_for_manual_extra = None
            if self.tmdb_api_key:
                if item_type == "Movie":
                    tmdb_details_for_manual_extra = tmdb.get_movie_details(tmdb_id, self.tmdb_api_key)
                    if not tmdb_details_for_manual_extra:
                        logger.warning(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ— æ³•ä» TMDb è·å–ç”µå½± '{item_name}' ({tmdb_id}) çš„è¯¦æƒ…ã€‚")
                elif item_type == "Series":
                    aggregated_tmdb_data = tmdb.aggregate_full_series_data_from_tmdb(int(tmdb_id), self.tmdb_api_key)
                    if aggregated_tmdb_data:
                        tmdb_details_for_manual_extra = aggregated_tmdb_data.get("series_details")
                    else:
                        logger.warning(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ— æ³•ä» TMDb è·å–å‰§é›† '{item_name}' ({tmdb_id}) çš„è¯¦æƒ…ã€‚")
            else:
                logger.warning("  âœ æ‰‹åŠ¨å¤„ç†ï¼šæœªé…ç½® TMDb API Keyï¼Œæ— æ³•è·å– TMDb è¯¦æƒ…ç”¨äºåˆ†çº§æ•°æ®ã€‚")

            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
            main_json_filename = "all.json" if item_type == "Movie" else "series.json"
            main_json_path = os.path.join(target_override_dir, main_json_filename)

            if not os.path.exists(main_json_path):
                raise FileNotFoundError(f"æ‰‹åŠ¨å¤„ç†å¤±è´¥ï¼šæ‰¾ä¸åˆ°ä¸»å…ƒæ•°æ®æ–‡ä»¶ '{main_json_path}'ã€‚")

            # ======================================================================
            # æ­¥éª¤ 2: æ›´æ–°AIç¿»è¯‘ç¼“å­˜
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 2/5: æ£€æŸ¥å¹¶æ›´æ–°AIç¿»è¯‘ç¼“å­˜...")
            try:
                # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ â‘ : ä»ç¼“å­˜è·å–çš„æ˜¯ tmdbId -> åŸå§‹è§’è‰²å çš„å­—å…¸ â˜…â˜…â˜…
                original_roles_map = self.manual_edit_cache.get(item_id)
                if original_roles_map:
                    with get_central_db_connection() as conn:
                        cursor = conn.cursor()
                        updated_count = 0
                        
                        # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ â‘¡: éå†å‰ç«¯æäº¤çš„åˆ—è¡¨ â˜…â˜…â˜…
                        for actor_from_frontend in manual_cast_list:
                            tmdb_id_str = str(actor_from_frontend.get("tmdbId"))
                            if not tmdb_id_str: continue
                            
                            # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ â‘¢: ç”¨ tmdbId ç²¾å‡†æ‰¾åˆ°ä¿®æ”¹å‰çš„è§’è‰²å â˜…â˜…â˜…
                            original_role = original_roles_map.get(tmdb_id_str)
                            if original_role is None: # å¦‚æœåŸå§‹è®°å½•é‡Œå°±æ²¡æœ‰è¿™ä¸ªæ¼”å‘˜ï¼Œå°±è·³è¿‡
                                continue

                            new_role = actor_from_frontend.get('role', '')
                            
                            cleaned_new_role = utils.clean_character_name_static(new_role)
                            cleaned_original_role = utils.clean_character_name_static(original_role)

                            if cleaned_new_role and cleaned_new_role != cleaned_original_role:
                                cache_entry = self.actor_db_manager.get_translation_from_db(text=cleaned_original_role, by_translated_text=True, cursor=cursor)
                                if cache_entry and 'original_text' in cache_entry:
                                    original_text_key = cache_entry['original_text']
                                    self.actor_db_manager.save_translation_to_db(
                                        cursor=cursor, original_text=original_text_key,
                                        translated_text=cleaned_new_role, engine_used="manual"
                                    )
                                    logger.debug(f"  âœ AIç¿»è¯‘ç¼“å­˜å·²æ›´æ–°: '{original_text_key}' ('{cleaned_original_role}' -> '{cleaned_new_role}')")
                                    updated_count += 1
                        if updated_count > 0:
                            logger.info(f"  âœ æˆåŠŸæ›´æ–°äº† {updated_count} æ¡ç¿»è¯‘ç¼“å­˜ã€‚")
                        else:
                            logger.info(f"  âœ æ— éœ€æ›´æ–°ç¿»è¯‘ç¼“å­˜ (è§’è‰²åæœªå‘ç”Ÿæœ‰æ•ˆå˜æ›´)ã€‚")
                        conn.commit()
                else:
                    logger.warning(f"  âœ æ— æ³•æ›´æ–°ç¿»è¯‘ç¼“å­˜ï¼šå†…å­˜ä¸­æ‰¾ä¸åˆ° ItemID {item_id} çš„åŸå§‹æ¼”å‘˜æ•°æ®ä¼šè¯ã€‚")
            except Exception as e:
                logger.error(f"  âœ æ‰‹åŠ¨å¤„ç†æœŸé—´æ›´æ–°ç¿»è¯‘ç¼“å­˜æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", exc_info=True)
            
            # ======================================================================
            # æ­¥éª¤ 3: APIå‰ç½®æ“ä½œ (æ›´æ–°æ¼”å‘˜å)
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 3/6: é€šè¿‡APIæ›´æ–°ç°æœ‰æ¼”å‘˜çš„åå­—...")
            # æ„å»º TMDb ID -> Emby Person ID å’Œ Emby Person ID -> å½“å‰åå­—çš„æ˜ å°„
            emby_id_to_name_map = {}
            for person in enriched_actors: # â˜…â˜…â˜… ç›´æ¥ä½¿ç”¨ enriched_actors
                person_emby_id = person.get("Id")
                if person_emby_id:
                    emby_id_to_name_map[person_emby_id] = person.get("Name")
            
            tmdb_to_emby_map = {}
            emby_id_to_name_map = {}
            for person in enriched_actors:
                person_tmdb_id = (person.get("ProviderIds") or {}).get("Tmdb")
                person_emby_id = person.get("Id")
                if person_tmdb_id and person_emby_id:
                    tmdb_to_emby_map[str(person_tmdb_id)] = person_emby_id
                    emby_id_to_name_map[person_emby_id] = person.get("Name")

            updated_names_count = 0
            for actor_from_frontend in manual_cast_list:
                tmdb_id_str = str(actor_from_frontend.get("tmdbId"))
                
                # åªå¤„ç†åœ¨æ˜ å°„ä¸­èƒ½æ‰¾åˆ°çš„ã€å·²å­˜åœ¨çš„æ¼”å‘˜
                actor_emby_id = tmdb_to_emby_map.get(tmdb_id_str)
                if not actor_emby_id: continue

                new_name = actor_from_frontend.get("name")
                original_name = emby_id_to_name_map.get(actor_emby_id)
                
                if new_name and original_name and new_name != original_name:
                    emby.update_person_details(
                        person_id=actor_emby_id, new_data={"Name": new_name},
                        emby_server_url=self.emby_url, emby_api_key=self.emby_api_key, user_id=self.emby_user_id
                    )
                    updated_names_count += 1
            
            if updated_names_count > 0:
                logger.info(f"  âœ æˆåŠŸé€šè¿‡ API æ›´æ–°äº† {updated_names_count} ä½æ¼”å‘˜çš„åå­—ã€‚")

            # ======================================================================
            # æ­¥éª¤ 4: æ–‡ä»¶è¯»ã€æ”¹ã€å†™ (åŒ…å«æœ€ç»ˆæ ¼å¼åŒ–)
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 4/6: è¯»å–åŸå§‹æ•°æ®ï¼Œè¯†åˆ«å¹¶è¡¥å…¨æ–°å¢æ¼”å‘˜çš„å…ƒæ•°æ®...")
            with open(main_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            original_cast_data = (data.get('casts', {}) or data.get('credits', {})).get('cast', [])
            original_cast_map = {str(actor.get('id')): actor for actor in original_cast_data if actor.get('id')}

            new_actor_tmdb_ids = [
                int(actor.get("tmdbId")) for actor in manual_cast_list 
                if str(actor.get("tmdbId")) not in original_cast_map
            ]

            all_new_actors_metadata = {}
            if new_actor_tmdb_ids:
                with get_central_db_connection() as conn_new:
                    cursor_new = conn_new.cursor()
                    all_new_actors_metadata = self.actor_db_manager.get_full_actor_details_by_tmdb_ids(cursor_new, new_actor_tmdb_ids)

            new_cast_built = []
            
            with get_central_db_connection() as conn:
                cursor = conn.cursor()

                for actor_from_frontend in manual_cast_list:
                    tmdb_id_str = str(actor_from_frontend.get("tmdbId"))
                    if not tmdb_id_str: continue
                    
                    # --- A. å¤„ç†ç°æœ‰æ¼”å‘˜ ---
                    if tmdb_id_str in original_cast_map:
                        updated_actor_entry = original_cast_map[tmdb_id_str].copy()
                        updated_actor_entry['name'] = actor_from_frontend.get('name')
                        updated_actor_entry['character'] = actor_from_frontend.get('role')
                        new_cast_built.append(updated_actor_entry)
                    
                    # --- B. å¤„ç†æ–°å¢æ¼”å‘˜ ---
                    else:
                        logger.info(f"    â”œâ”€ å‘ç°æ–°æ¼”å‘˜: '{actor_from_frontend.get('name')}' (TMDb ID: {tmdb_id_str})ï¼Œå¼€å§‹è¡¥å…¨å…ƒæ•°æ®...")
                        
                        # B1: ä¼˜å…ˆä» å†…å­˜ ç¼“å­˜è·å–
                        person_details = all_new_actors_metadata.get(int(tmdb_id_str))
                        
                        # B2: å¦‚æœç¼“å­˜æ²¡æœ‰ï¼Œåˆ™ä» TMDb API è·å–å¹¶åå“º
                        if not person_details:
                            logger.debug(f"  âœ ç¼“å­˜æœªå‘½ä¸­ï¼Œä» TMDb API è·å–è¯¦æƒ…...")
                            person_details_from_api = tmdb.get_person_details_tmdb(tmdb_id_str, self.tmdb_api_key)
                            if person_details_from_api:
                                self.actor_db_manager.update_actor_metadata_from_tmdb(cursor, tmdb_id_str, person_details_from_api)
                                person_details = person_details_from_api # ä½¿ç”¨APIè¿”å›çš„æ•°æ®
                            else:
                                logger.warning(f"  âœ æ— æ³•è·å–TMDb ID {tmdb_id_str} çš„è¯¦æƒ…ï¼Œå°†ä½¿ç”¨åŸºç¡€ä¿¡æ¯è·³è¿‡ã€‚")
                                # å³ä½¿å¤±è´¥ï¼Œä¹Ÿåˆ›å»ºä¸€ä¸ªåŸºç¡€å¯¹è±¡ï¼Œé¿å…ä¸¢å¤±
                                person_details = {} 
                        else:
                            logger.debug(f"  âœ æˆåŠŸä»æ•°æ®åº“ç¼“å­˜å‘½ä¸­å…ƒæ•°æ®ã€‚")

                        # B3: æ„å»ºä¸€ä¸ªä¸ override æ–‡ä»¶æ ¼å¼ä¸€è‡´çš„æ–°æ¼”å‘˜å¯¹è±¡
                        new_actor_entry = {
                            "id": int(tmdb_id_str),
                            "name": actor_from_frontend.get('name'),
                            "character": actor_from_frontend.get('role'),
                            "original_name": person_details.get("original_name"),
                            "profile_path": person_details.get("profile_path"),
                            "adult": person_details.get("adult", False),
                            "gender": person_details.get("gender", 0),
                            "known_for_department": person_details.get("known_for_department", "Acting"),
                            "popularity": person_details.get("popularity", 0.0),
                            # æ–°å¢æ¼”å‘˜æ²¡æœ‰è¿™äº›ç”µå½±ç‰¹å®šçš„IDï¼Œè®¾ä¸ºNone
                            "cast_id": None, 
                            "credit_id": None,
                            "order": 999 # æ”¾åˆ°æœ€åï¼Œåç»­æ ¼å¼åŒ–æ­¥éª¤ä¼šé‡æ–°æ’åº
                        }
                        new_cast_built.append(new_actor_entry)

            # ======================================================================
            # æ­¥éª¤ 5: æœ€ç»ˆæ ¼å¼åŒ–å¹¶å†™å…¥æ–‡ä»¶ (é€»è¾‘ä¸å˜)
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 5/6: é‡å»ºæ¼”å‘˜åˆ—è¡¨å¹¶æ‰§è¡Œæœ€ç»ˆæ ¼å¼åŒ–...")
            genres = item_details.get("Genres", [])
            is_animation = "Animation" in genres or "åŠ¨ç”»" in genres or "Documentary" in genres or "çºªå½•" in genres
            final_formatted_cast = actor_utils.format_and_complete_cast_list(
                new_cast_built, is_animation, self.config, mode='manual'
            )
            # _build_cast_from_final_data ç¡®ä¿äº†æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ï¼Œå³ä½¿æ˜¯None
            final_cast_for_json = self._build_cast_from_final_data(final_formatted_cast)

            if 'casts' in data:
                data['casts']['cast'] = final_cast_for_json
            elif 'credits' in data:
                data['credits']['cast'] = final_cast_for_json
            else:
                data.setdefault('credits', {})['cast'] = final_cast_for_json
            
            with open(main_json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            if item_type == "Series":
                self._inject_cast_to_series_files(
                    target_dir=target_override_dir, cast_list=final_cast_for_json,
                    series_details=item_details, source_dir=os.path.join(self.local_data_path, "cache", cache_folder_name, tmdb_id)
                )

            # ======================================================================
            # æ­¥éª¤ 6: è§¦å‘åˆ·æ–°å¹¶æ›´æ–°æ—¥å¿—
            # ======================================================================
            logger.info("  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 6/6: è§¦å‘ Emby åˆ·æ–°å¹¶æ›´æ–°å†…éƒ¨æ—¥å¿—...")
            
            emby.refresh_emby_item_metadata(
                item_emby_id=item_id,
                emby_server_url=self.emby_url,
                emby_api_key=self.emby_api_key,
                user_id_for_ops=self.emby_user_id,
                replace_all_metadata_param=True,
                item_name_for_log=item_name
            )

            # æ›´æ–°æˆ‘ä»¬è‡ªå·±çš„æ•°æ®åº“æ—¥å¿—å’Œç¼“å­˜
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                
                # ======================================================================
                # â˜…â˜…â˜… è°ƒç”¨ç»Ÿä¸€çš„ã€å·²è§„èŒƒåŒ–çš„ç¼“å­˜å†™å…¥å‡½æ•° â˜…â˜…â˜…
                # ======================================================================
                self._upsert_media_metadata(
                    cursor=cursor,
                    item_type=item_type,
                    item_details_from_emby=item_details,
                    final_processed_cast=final_formatted_cast, 
                    source_data_package=tmdb_details_for_manual_extra, 
                )
                
                logger.info(f"  âœ æ­£åœ¨å°†æ‰‹åŠ¨å¤„ç†å®Œæˆçš„ã€Š{item_name}ã€‹å†™å…¥å·²å¤„ç†æ—¥å¿—...")
                self.log_db_manager.save_to_processed_log(cursor, item_id, item_name, score=10.0)
                self.log_db_manager.remove_from_failed_log(cursor, item_id)
                conn.commit()

            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç† '{item_name}' æµç¨‹å®Œæˆã€‚")
            return True

        except Exception as e:
            logger.error(f"  âœ æ‰‹åŠ¨å¤„ç† '{item_name}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
            return False
        finally:
            if item_id in self.manual_edit_cache:
                del self.manual_edit_cache[item_id]
                logger.trace(f"å·²æ¸…ç† ItemID {item_id} çš„æ‰‹åŠ¨ç¼–è¾‘ä¼šè¯ç¼“å­˜ã€‚")
    
    # --- ä¸ºå‰ç«¯å‡†å¤‡æ¼”å‘˜åˆ—è¡¨ç”¨äºç¼–è¾‘ ---
    def get_cast_for_editing(self, item_id: str) -> Optional[Dict[str, Any]]:
        """
        ã€V2 - Overrideæ–‡ä»¶ä¸­å¿ƒåŒ–ç‰ˆã€‘
        é‡æ„æ•°æ®æºï¼Œç¡®ä¿å‰ç«¯è·å–å’Œç¼–è¾‘çš„æ¼”å‘˜åˆ—è¡¨ï¼Œä¸ override æ–‡ä»¶ä¸­çš„â€œçœŸç†ä¹‹æºâ€å®Œå…¨ä¸€è‡´ã€‚
        - æ¼”å‘˜è¡¨ä¸»ä½“(åå­—, è§’è‰², é¡ºåº) æ¥è‡ª override ä¸»JSONæ–‡ä»¶ã€‚
        - é€šè¿‡ä¸€æ¬¡ Emby API è°ƒç”¨æ¥è·å– emby_person_id å¹¶è¿›è¡Œæ˜ å°„ã€‚
        """
        logger.info(f"  âœ ä¸ºç¼–è¾‘é¡µé¢å‡†å¤‡æ•°æ®ï¼šItemID {item_id}")
        
        try:
            # æ­¥éª¤ 1: è·å– Emby åŸºç¡€è¯¦æƒ… å’Œ ç”¨äºIDæ˜ å°„çš„Peopleåˆ—è¡¨
            emby_details = emby.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id)
            if not emby_details:
                raise ValueError(f"åœ¨Embyä¸­æœªæ‰¾åˆ°é¡¹ç›® {item_id}")

            item_name_for_log = emby_details.get("Name", f"æœªçŸ¥(ID:{item_id})")
            tmdb_id = emby_details.get("ProviderIds", {}).get("Tmdb")
            item_type = emby_details.get("Type")
            if not tmdb_id:
                raise ValueError(f"é¡¹ç›® '{item_name_for_log}' ç¼ºå°‘ TMDb IDï¼Œæ— æ³•å®šä½å…ƒæ•°æ®æ–‡ä»¶ã€‚")

            # æ­¥éª¤ 2: è¯»å– override æ–‡ä»¶ï¼Œè·å–æƒå¨æ¼”å‘˜è¡¨
            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
            main_json_filename = "all.json" if item_type == "Movie" else "series.json"
            main_json_path = os.path.join(target_override_dir, main_json_filename)

            if not os.path.exists(main_json_path):
                raise FileNotFoundError(f"æ— æ³•ä¸º '{item_name_for_log}' å‡†å¤‡ç¼–è¾‘æ•°æ®ï¼šæ‰¾ä¸åˆ°ä¸»å…ƒæ•°æ®æ–‡ä»¶ '{main_json_path}'ã€‚è¯·ç¡®ä¿è¯¥é¡¹ç›®å·²è¢«è‡³å°‘å¤„ç†è¿‡ä¸€æ¬¡ã€‚")

            with open(main_json_path, 'r', encoding='utf-8') as f:
                override_data = json.load(f)
            
            cast_from_override = (override_data.get('casts', {}) or override_data.get('credits', {})).get('cast', [])
            logger.debug(f"  âœ æˆåŠŸä» override æ–‡ä»¶ä¸º '{item_name_for_log}' åŠ è½½äº† {len(cast_from_override)} ä½æ¼”å‘˜ã€‚")

            # æ­¥éª¤ 3: æ„å»º TMDb ID -> emby_person_id çš„æ˜ å°„
            tmdb_to_emby_map = {}
            for person in emby_details.get("People", []):
                person_tmdb_id = (person.get("ProviderIds") or {}).get("Tmdb")
                if person_tmdb_id:
                    tmdb_to_emby_map[str(person_tmdb_id)] = person.get("Id")
            
            # æ­¥éª¤ 4: ç»„è£…æœ€ç»ˆæ•°æ® (åˆå¹¶ override å†…å®¹ å’Œ emby_person_id)
            cast_for_frontend = []
            session_cache_map = {}
            
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                for actor_data in cast_from_override:
                    actor_tmdb_id = actor_data.get('id')
                    if not actor_tmdb_id: continue
                    
                    emby_person_id = tmdb_to_emby_map.get(str(actor_tmdb_id))
                    
                    # ä»æœ¬åœ°æ•°æ®åº“è·å–å¤´åƒ
                    image_url = None
                    # actor_data å°±æ˜¯ä» override æ–‡ä»¶é‡Œè¯»å‡ºçš„é‚£æ¡è®°å½•ï¼Œå®ƒåŒ…å«äº†æœ€å‡†ç¡®çš„ profile_path
                    profile_path = actor_data.get("profile_path")
                    if profile_path:
                        # å¦‚æœæ˜¯å®Œæ•´çš„ URL (æ¥è‡ªè±†ç“£)ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
                        if profile_path.startswith('http'):
                            image_url = profile_path
                        # å¦åˆ™ï¼Œè®¤ä¸ºæ˜¯ TMDb çš„ç›¸å¯¹è·¯å¾„ï¼Œè¿›è¡Œæ‹¼æ¥
                        else:
                            image_url = f"https://image.tmdb.org/t/p/w185{profile_path}"
                    
                    # æ¸…ç†è§’è‰²å
                    original_role = actor_data.get('character', '')
                    session_cache_map[str(actor_tmdb_id)] = original_role
                    cleaned_role_for_display = utils.clean_character_name_static(original_role)

                    # ä¸ºå‰ç«¯å‡†å¤‡çš„æ•°æ®
                    cast_for_frontend.append({
                        "tmdbId": actor_tmdb_id,
                        "name": actor_data.get('name'),
                        "role": cleaned_role_for_display,
                        "imageUrl": image_url,
                        "emby_person_id": emby_person_id
                    })
                    
            # æ­¥éª¤ 5: ç¼“å­˜ä¼šè¯æ•°æ®å¹¶å‡†å¤‡æœ€ç»ˆå“åº”
            self.manual_edit_cache[item_id] = session_cache_map
            logger.debug(f"å·²ä¸º ItemID {item_id} ç¼“å­˜äº† {len(session_cache_map)} æ¡ç”¨äºæ‰‹åŠ¨ç¼–è¾‘ä¼šè¯çš„æ¼”å‘˜æ•°æ®ã€‚")

            failed_log_info = {}
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT error_message, score FROM failed_log WHERE item_id = %s", (item_id,))
                row = cursor.fetchone()
                if row: failed_log_info = dict(row)

            response_data = {
                "item_id": item_id,
                "item_name": emby_details.get("Name"),
                "item_type": emby_details.get("Type"),
                "image_tag": emby_details.get('ImageTags', {}).get('Primary'),
                "original_score": failed_log_info.get("score"),
                "review_reason": failed_log_info.get("error_message"),
                "current_emby_cast": cast_for_frontend,
                "search_links": {
                    "baidu": utils.generate_search_url('baike', emby_details.get("Name"), emby_details.get("ProductionYear")),
                    "wikipedia": utils.generate_search_url('wikipedia', emby_details.get("Name"), emby_details.get("ProductionYear")),
                    "google": utils.generate_search_url('google', emby_details.get("Name"), emby_details.get("ProductionYear"))
                }
            }
            return response_data

        except Exception as e:
            logger.error(f"  âœ è·å–ç¼–è¾‘æ•°æ®å¤±è´¥ for ItemID {item_id}: {e}", exc_info=True)
            return None
    
    # --- å®æ—¶è¦†ç›–ç¼“å­˜åŒæ­¥ ---
    def sync_single_item_assets(self, item_id: str, 
                                update_description: Optional[str] = None, 
                                sync_timestamp_iso: Optional[str] = None,
                                final_cast_override: Optional[List[Dict[str, Any]]] = None,
                                episode_ids_to_sync: Optional[List[str]] = None,
                                douban_rating_override: Optional[float] = None):
        """
        çº¯ç²¹çš„é¡¹ç›®ç»ç†ï¼Œè´Ÿè´£æ¥æ”¶è®¾è®¡å¸ˆçš„æ‰€æœ‰ææ–™ï¼Œå¹¶åˆ†å‘ç»™æ–½å·¥é˜Ÿã€‚
        """
        log_prefix = f"å®æ—¶è¦†ç›–ç¼“å­˜åŒæ­¥"
        logger.trace(f"--- {log_prefix} å¼€å§‹æ‰§è¡Œ (ItemID: {item_id}) ---")

        if not self.local_data_path:
            logger.warning(f"  âœ {log_prefix} ä»»åŠ¡è·³è¿‡ï¼Œå› ä¸ºæœªé…ç½®æœ¬åœ°æ•°æ®æºè·¯å¾„ã€‚")
            return

        try:
            item_details = emby.get_emby_item_details(
                item_id, self.emby_url, self.emby_api_key, self.emby_user_id,
                fields="ProviderIds,Type,Name,IndexNumber,ParentIndexNumber"
            )
            if not item_details:
                raise ValueError("åœ¨Embyä¸­æ‰¾ä¸åˆ°è¯¥é¡¹ç›®ã€‚")

            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            if not tmdb_id:
                logger.warning(f"{log_prefix} é¡¹ç›® '{item_details.get('Name')}' ç¼ºå°‘TMDb IDï¼Œæ— æ³•åŒæ­¥ã€‚")
                return

            # 1. è°ƒåº¦å¤–å¢™æ–½å·¥é˜Ÿ
            self.sync_item_images(item_details, update_description, episode_ids_to_sync=episode_ids_to_sync)
            
            # 2. è°ƒåº¦ç²¾è£…ä¿®æ–½å·¥é˜Ÿï¼Œå¹¶æŠŠæ‰€æœ‰å›¾çº¸å’Œææ–™éƒ½ç»™ä»–
            self.sync_item_metadata(
                item_details, 
                tmdb_id, 
                final_cast_override=final_cast_override, 
                episode_ids_to_sync=episode_ids_to_sync,
                douban_rating_override=douban_rating_override
            )

            # 3. è®°å½•å·¥æ—¶
            timestamp_to_log = sync_timestamp_iso or datetime.now(timezone.utc).isoformat()
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                self.log_db_manager.mark_assets_as_synced(
                    cursor, 
                    item_id, 
                    timestamp_to_log
                )
                conn.commit()
            
            logger.trace(f"--- {log_prefix} æˆåŠŸå®Œæˆ (ItemID: {item_id}) ---")

        except Exception as e:
            logger.error(f"{log_prefix} æ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯ (ItemID: {item_id}): {e}", exc_info=True)

    # --- å¤‡ä»½å›¾ç‰‡ ---
    def sync_item_images(self, item_details: Dict[str, Any], update_description: Optional[str] = None, episode_ids_to_sync: Optional[List[str]] = None) -> bool:
        """
        ã€æ–°å¢-é‡æ„ã€‘è¿™ä¸ªæ–¹æ³•è´Ÿè´£åŒæ­¥ä¸€ä¸ªåª’ä½“é¡¹ç›®çš„æ‰€æœ‰ç›¸å…³å›¾ç‰‡ã€‚
        å®ƒä» _process_item_core_logic ä¸­æå–å‡ºæ¥ï¼Œä»¥ä¾¿å¤ç”¨ã€‚
        """
        item_id = item_details.get("Id")
        item_type = item_details.get("Type")
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        
        if not all([item_id, item_type, self.local_data_path]):
            logger.error(f"  âœ è·³è¿‡ '{item_name_for_log}'ï¼Œå› ä¸ºç¼ºå°‘IDã€ç±»å‹æˆ–æœªé…ç½®æœ¬åœ°æ•°æ®è·¯å¾„ã€‚")
            return False

        try:
            # --- å‡†å¤‡å·¥ä½œ (ç›®å½•ã€TMDb IDç­‰) ---
            log_prefix = "è¦†ç›–ç¼“å­˜-å›¾ç‰‡å¤‡ä»½ï¼š"
            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            if not tmdb_id:
                logger.warning(f"  âœ {log_prefix} é¡¹ç›® '{item_name_for_log}' ç¼ºå°‘TMDb IDï¼Œæ— æ³•ç¡®å®šè¦†ç›–ç›®å½•ï¼Œè·³è¿‡ã€‚")
                return False
            
            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            base_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
            image_override_dir = os.path.join(base_override_dir, "images")
            os.makedirs(image_override_dir, exist_ok=True)

            # --- å®šä¹‰æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡æ˜ å°„ ---
            full_image_map = {"Primary": "poster.jpg", "Backdrop": "fanart.jpg", "Logo": "clearlogo.png"}
            if item_type == "Movie":
                full_image_map["Thumb"] = "landscape.jpg"

            # â˜…â˜…â˜… å…¨æ–°é€»è¾‘åˆ†å‘ â˜…â˜…â˜…
            images_to_sync = {}
            
            # æ¨¡å¼ä¸€ï¼šç²¾å‡†åŒæ­¥ (å½“æè¿°å­˜åœ¨æ—¶)
            if update_description:
                log_prefix = "[è¦†ç›–ç¼“å­˜-å›¾ç‰‡å¤‡ä»½]"
                logger.trace(f"{log_prefix} æ­£åœ¨è§£ææè¿°: '{update_description}'")
                
                # å®šä¹‰å…³é”®è¯åˆ°Embyå›¾ç‰‡ç±»å‹çš„æ˜ å°„ (ä½¿ç”¨å°å†™ä»¥æ–¹ä¾¿åŒ¹é…)
                keyword_map = {
                    "primary": "Primary",
                    "backdrop": "Backdrop",
                    "logo": "Logo",
                    "thumb": "Thumb", # ç”µå½±ç¼©ç•¥å›¾
                    "banner": "Banner" # å‰§é›†æ¨ªå¹… (å¦‚æœéœ€è¦å¯ä»¥æ·»åŠ )
                }
                
                desc_lower = update_description.lower()
                found_specific_image = False
                for keyword, image_type_api in keyword_map.items():
                    if keyword in desc_lower and image_type_api in full_image_map:
                        images_to_sync[image_type_api] = full_image_map[image_type_api]
                        logger.trace(f"{log_prefix} åŒ¹é…åˆ°å…³é”®è¯ '{keyword}'ï¼Œå°†åªåŒæ­¥ {image_type_api} å›¾ç‰‡ã€‚")
                        found_specific_image = True
                        break # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…å°±åœæ­¢ï¼Œé¿å…é‡å¤
                
                if not found_specific_image:
                    logger.trace(f"{log_prefix} æœªèƒ½åœ¨æè¿°ä¸­æ‰¾åˆ°å¯è¯†åˆ«çš„å›¾ç‰‡å…³é”®è¯ï¼Œå°†å›é€€åˆ°å®Œå…¨åŒæ­¥ã€‚")
                    images_to_sync = full_image_map # å›é€€
            
            # æ¨¡å¼äºŒï¼šå®Œå…¨åŒæ­¥ (é»˜è®¤æˆ–å›é€€)
            else:
                log_prefix = "[è¦†ç›–ç¼“å­˜-å›¾ç‰‡å¤‡ä»½]"
                logger.trace(f"  âœ {log_prefix} æœªæä¾›æ›´æ–°æè¿°ï¼Œå°†åŒæ­¥æ‰€æœ‰ç±»å‹çš„å›¾ç‰‡ã€‚")
                images_to_sync = full_image_map

            # --- æ‰§è¡Œä¸‹è½½ ---
            if not episode_ids_to_sync:
                logger.info(f"  âœ {log_prefix} å¼€å§‹ä¸º '{item_name_for_log}' ä¸‹è½½ {len(images_to_sync)} å¼ ä¸»å›¾ç‰‡è‡³è¦†ç›–ç¼“å­˜")
                for image_type, filename in images_to_sync.items():
                    if self.is_stop_requested():
                        logger.warning(f"  ğŸš« {log_prefix} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ­¢å›¾ç‰‡ä¸‹è½½ã€‚")
                        return False
                    emby.download_emby_image(item_id, image_type, os.path.join(image_override_dir, filename), self.emby_url, self.emby_api_key)
            
            # --- åˆ†é›†å›¾ç‰‡é€»è¾‘ ---
            if item_type == "Series":
                children_to_process = []
                # è·å–æ‰€æœ‰å­é¡¹ä¿¡æ¯ï¼Œç”¨äºæŸ¥æ‰¾
                all_children = emby.get_series_children(item_id, self.emby_url, self.emby_api_key, self.emby_user_id, series_name_for_log=item_name_for_log) or []
                
                if episode_ids_to_sync:
                    # æ¨¡å¼ä¸€ï¼šåªå¤„ç†æŒ‡å®šçš„åˆ†é›†
                    logger.info(f"  âœ {log_prefix} å°†åªåŒæ­¥ {len(episode_ids_to_sync)} ä¸ªæŒ‡å®šåˆ†é›†çš„å›¾ç‰‡ã€‚")
                    id_set = set(episode_ids_to_sync)
                    children_to_process = [child for child in all_children if child.get("Id") in id_set]
                elif images_to_sync == full_image_map:
                    # æ¨¡å¼äºŒï¼šå¤„ç†æ‰€æœ‰å­é¡¹ï¼ˆåŸé€»è¾‘ï¼‰
                    children_to_process = all_children

                for child in children_to_process:
                    if self.is_stop_requested():
                        logger.warning(f"  ğŸš« {log_prefix} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ­¢å­é¡¹ç›®å›¾ç‰‡ä¸‹è½½ã€‚")
                        return False
                    child_type, child_id = child.get("Type"), child.get("Id")
                    if child_type == "Season":
                        season_number = child.get("IndexNumber")
                        if season_number is not None:
                            emby.download_emby_image(child_id, "Primary", os.path.join(image_override_dir, f"season-{season_number}.jpg"), self.emby_url, self.emby_api_key)
                    elif child_type == "Episode":
                        season_number, episode_number = child.get("ParentIndexNumber"), child.get("IndexNumber")
                        if season_number is not None and episode_number is not None:
                            emby.download_emby_image(child_id, "Primary", os.path.join(image_override_dir, f"season-{season_number}-episode-{episode_number}.jpg"), self.emby_url, self.emby_api_key)
            
            logger.trace(f"  âœ {log_prefix} æˆåŠŸå®Œæˆ '{item_name_for_log}' çš„è¦†ç›–ç¼“å­˜-å›¾ç‰‡å¤‡ä»½ã€‚")
            return True
        except Exception as e:
            logger.error(f"{log_prefix} ä¸º '{item_name_for_log}' å¤‡ä»½å›¾ç‰‡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return False
    
    # --- å¤‡ä»½å…ƒæ•°æ® ---
    def sync_item_metadata(self, item_details: Dict[str, Any], tmdb_id: str,
                       final_cast_override: Optional[List[Dict[str, Any]]] = None,
                       episode_ids_to_sync: Optional[List[str]] = None,
                       douban_rating_override: Optional[float] = None):
        """
        ã€V4 - ç²¾è£…ä¿®æ–½å·¥é˜Ÿæœ€ç»ˆç‰ˆã€‘
        æœ¬å‡½æ•°æ˜¯å”¯ä¸€çš„æ–½å·¥é˜Ÿï¼Œè´Ÿè´£æ‰€æœ‰ override æ–‡ä»¶çš„è¯»å†™æ“ä½œã€‚
        """
        item_id = item_details.get("Id")
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        item_type = item_details.get("Type")
        log_prefix = "[è¦†ç›–ç¼“å­˜-å…ƒæ•°æ®å†™å…¥]"

        # å®šä¹‰æ ¸å¿ƒè·¯å¾„
        cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
        source_cache_dir = os.path.join(self.local_data_path, "cache", cache_folder_name, tmdb_id)
        target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
        main_json_filename = "all.json" if item_type == "Movie" else "series.json"
        main_json_path = os.path.join(target_override_dir, main_json_filename)

        # æ­¥éª¤ 1: è¿›åœºæ–½å·¥ï¼Œæ‰“å¥½åŸºç¡€ (å¤åˆ¶æ¯›å¯æˆ¿)
        # åªæœ‰åœ¨éœ€è¦è¿›è¡Œä¸»ä½“è£…ä¿®æ—¶ï¼ˆä¸»æµç¨‹è°ƒç”¨ï¼‰ï¼Œæ‰éœ€è¦å¤åˆ¶ã€‚è¿½æ›´ç­‰é›¶æ´»ä¸éœ€è¦ã€‚
        if final_cast_override is not None:
            logger.info(f"  âœ {log_prefix} å¼€å§‹ä¸º '{item_name_for_log}' å†™å…¥è¦†ç›–ç¼“å­˜...")
            if not os.path.exists(source_cache_dir):
                logger.error(f"  âœ {log_prefix} æ‰¾ä¸åˆ°æºç¼“å­˜ç›®å½•ï¼è·¯å¾„: {source_cache_dir}")
                return
            try:
                shutil.copytree(source_cache_dir, target_override_dir, dirs_exist_ok=True)
            except Exception as e:
                logger.error(f"  âœ {log_prefix} å¤åˆ¶å…ƒæ•°æ®æ—¶å¤±è´¥: {e}", exc_info=True)
                return

        perfect_cast_for_injection = []
        if final_cast_override is not None:
            # --- è§’è‰²ä¸€ï¼šä¸»ä½“ç²¾è£…ä¿® ---
            new_cast_for_json = self._build_cast_from_final_data(final_cast_override)
            
            perfect_cast_for_injection = new_cast_for_json

            # æ­¥éª¤ 2: ä¿®æ”¹ä¸»æ–‡ä»¶
            with open(main_json_path, 'r+', encoding='utf-8') as f:
                data = json.load(f)
                if douban_rating_override is not None: data['vote_average'] = douban_rating_override
                if 'casts' in data: data['casts']['cast'] = perfect_cast_for_injection
                else: data.setdefault('credits', {})['cast'] = perfect_cast_for_injection
                
                f.seek(0)
                json.dump(data, f, ensure_ascii=False, indent=2)
                f.truncate()
        else:
            # --- è§’è‰²äºŒï¼šé›¶æ´»å¤„ç† (è¿½æ›´) ---
            logger.info(f"  âœ {log_prefix} [è¿½æ›´] å¼€å§‹ä¸º '{item_name_for_log}' çš„æ–°åˆ†é›†å†™å…¥è¦†ç›–ç¼“å­˜...")
            if not os.path.exists(main_json_path):
                logger.error(f"  âœ {log_prefix} è¿½æ›´ä»»åŠ¡å¤±è´¥ï¼šæ‰¾ä¸åˆ°ä¸»å…ƒæ•°æ®æ–‡ä»¶ '{main_json_path}'ã€‚")
                return
            try:
                with open(main_json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    perfect_cast_for_injection = (data.get('casts', {}) or data.get('credits', {})).get('cast', [])
            except Exception as e:
                logger.error(f"  âœ {log_prefix} è¯»å–ä¸»å…ƒæ•°æ®æ–‡ä»¶ '{main_json_path}' æ—¶å¤±è´¥: {e}", exc_info=True)
                return

        # æ­¥éª¤ 3: å…¬å…±æ–½å·¥ - æ³¨å…¥åˆ†é›†æ–‡ä»¶
        if item_type == "Series" and perfect_cast_for_injection:
            self._inject_cast_to_series_files(
                target_dir=target_override_dir, 
                cast_list=perfect_cast_for_injection, 
                series_details=item_details, 
                source_dir=source_cache_dir,  
                episode_ids_to_sync=episode_ids_to_sync
            )

    # --- è¾…åŠ©å‡½æ•°ï¼šä»ä¸åŒæ•°æ®æºæ„å»ºæ¼”å‘˜åˆ—è¡¨ ---
    def _build_cast_from_final_data(self, final_cast_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¾…åŠ©å‡½æ•°ï¼šä»ä¸»æµç¨‹çš„æœ€ç»ˆç»“æœæ„å»ºæ¼”å‘˜åˆ—è¡¨"""
        cast_list = []
        for i, actor_info in enumerate(final_cast_data):
            if not actor_info.get("id"): continue
            cast_list.append({
                "id": actor_info.get("id"), "name": actor_info.get("name"), "character": actor_info.get("character"),
                "original_name": actor_info.get("original_name"), "profile_path": actor_info.get("profile_path"),
                "adult": actor_info.get("adult", False), "gender": actor_info.get("gender", 0),
                "known_for_department": actor_info.get("known_for_department", "Acting"),
                "popularity": actor_info.get("popularity", 0.0), "cast_id": actor_info.get("cast_id"),
                "credit_id": actor_info.get("credit_id"), "order": actor_info.get("order", i)
            })
        return cast_list

    # --- è¾…åŠ©å‡½æ•°ï¼šå°†æ¼”å‘˜è¡¨æ³¨å…¥å‰§é›†çš„å­£/é›†JSONæ–‡ä»¶ ---
    def _inject_cast_to_series_files(self, target_dir: str, cast_list: List[Dict[str, Any]], series_details: Dict[str, Any], 
                                     # â–¼â–¼â–¼ æ ¸å¿ƒä¿®æ”¹ 1/3: å¢åŠ  source_dir å‚æ•° â–¼â–¼â–¼
                                     source_dir: str, 
                                     episode_ids_to_sync: Optional[List[str]] = None):
        """
        è¾…åŠ©å‡½æ•°ï¼šå°†æ¼”å‘˜è¡¨æ³¨å…¥å‰§é›†çš„å­£/é›†JSONæ–‡ä»¶ã€‚
        """
        log_prefix = "[è¦†ç›–ç¼“å­˜-å…ƒæ•°æ®å†™å…¥]"
        if cast_list is not None:
            logger.info(f"  âœ {log_prefix} å¼€å§‹å°†æ¼”å‘˜è¡¨æ³¨å…¥æ‰€æœ‰å­£/é›†å¤‡ä»½æ–‡ä»¶...")
        else:
            logger.info(f"  âœ {log_prefix} å¼€å§‹å°†å®æ—¶å…ƒæ•°æ®ï¼ˆæ ‡é¢˜/ç®€ä»‹ï¼‰åŒæ­¥åˆ°æ‰€æœ‰å­£/é›†å¤‡ä»½æ–‡ä»¶...")
        
        children_from_emby = emby.get_series_children(
            series_id=series_details.get("Id"), base_url=self.emby_url,
            api_key=self.emby_api_key, user_id=self.emby_user_id,
            series_name_for_log=series_details.get("Name")
        ) or []

        child_data_map = {}
        for child in children_from_emby:
            key = None
            if child.get("Type") == "Season": key = f"season-{child.get('IndexNumber')}"
            elif child.get("Type") == "Episode": key = f"season-{child.get('ParentIndexNumber')}-episode-{child.get('IndexNumber')}"
            if key: child_data_map[key] = child

        updated_children_count = 0
        try:
            files_to_process = []
            if episode_ids_to_sync:
                id_set = set(episode_ids_to_sync)
                for child in children_from_emby:
                    if child.get("Id") in id_set and child.get("Type") == "Episode":
                        s_num = child.get('ParentIndexNumber')
                        e_num = child.get('IndexNumber')
                        if s_num is not None and e_num is not None:
                            files_to_process.append(f"season-{s_num}-episode-{e_num}.json")
            else:
                for filename in os.listdir(target_dir):
                    if filename.startswith("season-") and filename.endswith(".json") and filename != "series.json":
                        files_to_process.append(filename)

            for filename in files_to_process:
                child_json_path = os.path.join(target_dir, filename)
                
                # â–¼â–¼â–¼ æ ¸å¿ƒä¿®æ”¹ 2/3: æ£€æŸ¥-å¤åˆ¶-ä¿®æ”¹ é€»è¾‘ â–¼â–¼â–¼
                if not os.path.exists(child_json_path):
                    source_json_path = os.path.join(source_dir, filename)
                    if os.path.exists(source_json_path):
                        logger.debug(f"  âœ æ­£åœ¨å¤åˆ¶å…ƒæ•°æ®æ–‡ä»¶ '{filename}'")
                        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                        os.makedirs(os.path.dirname(child_json_path), exist_ok=True)
                        shutil.copy2(source_json_path, child_json_path)
                    else:
                        logger.warning(f"  âœ è·³è¿‡æ³¨å…¥ '{filename}'ï¼Œå› ä¸ºå®ƒåœ¨æºç¼“å­˜å’Œè¦†ç›–ç¼“å­˜ä¸­éƒ½ä¸å­˜åœ¨ã€‚")
                        continue
                
                try:
                    with open(child_json_path, 'r+', encoding='utf-8') as f_child:
                        child_data = json.load(f_child)
                        
                        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šæ¡ä»¶æ€§åœ°æ›´æ–°æ¼”å‘˜è¡¨ â˜…â˜…â˜…
                        if cast_list is not None and 'credits' in child_data and 'cast' in child_data['credits']:
                            child_data['credits']['cast'] = cast_list
                        
                        # æ— è®ºå¦‚ä½•éƒ½æ›´æ–°å…ƒæ•°æ®
                        file_key = os.path.splitext(filename)[0]
                        fresh_data = child_data_map.get(file_key)
                        if fresh_data:
                            child_data['name'] = fresh_data.get('Name', child_data.get('name'))
                            child_data['overview'] = fresh_data.get('Overview', child_data.get('overview'))
                        
                        f_child.seek(0)
                        json.dump(child_data, f_child, ensure_ascii=False, indent=2)
                        f_child.truncate()
                        updated_children_count += 1
                except Exception as e_child:
                    logger.warning(f"  âœ æ›´æ–°å­æ–‡ä»¶ '{filename}' æ—¶å¤±è´¥: {e_child}")
            logger.info(f"  âœ {log_prefix} æˆåŠŸå°†å…ƒæ•°æ®æ³¨å…¥äº† {updated_children_count} ä¸ªå­£/é›†æ–‡ä»¶ã€‚")
        except Exception as e_list:
            logger.error(f"  âœ {log_prefix} éå†å¹¶æ›´æ–°å­£/é›†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e_list}", exc_info=True)

    # --- ä¸ºä¸€ä¸ªåª’ä½“é¡¹åŒæ­¥å…ƒæ•°æ®ç¼“å­˜ ---
    def sync_single_item_to_metadata_cache(self, item_id: str, item_name: Optional[str] = None, episode_ids_to_add: Optional[List[str]] = None):
        """
        ã€V11 - ç»Ÿä¸€ç‰ˆæœ¬æ„ŸçŸ¥ä¿®å¤ç‰ˆã€‘
        - å¸¸è§„æ¨¡å¼åœ¨å¤„ç†å‰§é›†æ—¶ï¼Œä¼šè°ƒç”¨ get_all_library_versions è·å–æ‰€æœ‰åˆ†é›†ç‰ˆæœ¬å¹¶è¿›è¡Œèšåˆã€‚
        - åˆ†é›†è¿½åŠ æ¨¡å¼ä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒå¤„ç†çš„æ˜¯ç‰¹å®šçš„æ–°åˆ†é›†IDï¼Œé€»è¾‘å¤©ç„¶æ­£ç¡®ã€‚
        """
        log_prefix = f"å®æ—¶åŒæ­¥åª’ä½“æ•°æ® '{item_name}'"
        sync_mode = "ç²¾å‡†åˆ†é›†è¿½åŠ " if episode_ids_to_add else "å¸¸è§„å…ƒæ•°æ®åˆ·æ–°"
        logger.info(f"  âœ {log_prefix} å¼€å§‹æ‰§è¡Œ ({sync_mode}æ¨¡å¼)")
        
        try:
            if episode_ids_to_add:
                # --- æ¨¡å¼ä¸€ï¼šç²¾å‡†åˆ†é›†è¿½åŠ   ---
                series_details = emby.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id, fields="ProviderIds,Name")
                if not series_details:
                    logger.error(f"  ğŸš« {log_prefix} [å¢é‡æ¨¡å¼] æ— æ³•è·å–çˆ¶å‰§é›† {item_id} çš„è¯¦æƒ…ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚")
                    return
                
                series_tmdb_id = series_details.get("ProviderIds", {}).get("Tmdb")
                if not series_tmdb_id:
                    logger.error(f"  âœ {log_prefix} [å¢é‡æ¨¡å¼] çˆ¶å‰§é›† '{series_details.get('Name')}' ç¼ºå°‘ TMDb IDï¼Œæ— æ³•å…³è”åˆ†é›†ã€‚")
                    return

                new_episodes_details = emby.get_emby_items_by_id(
                    base_url=self.emby_url, api_key=self.emby_api_key, user_id=self.emby_user_id,
                    item_ids=episode_ids_to_add, 
                    fields="Id,Type,ParentIndexNumber,IndexNumber,Name,OriginalTitle,PremiereDate,ProviderIds,MediaStreams,Container,Size,Path,DateCreated,RunTimeTicks"
                )
                
                if not new_episodes_details:
                    logger.warning(f"  ğŸš« {log_prefix} [å¢é‡æ¨¡å¼] æ— æ³•ä»Embyè·å–æ–°åˆ†é›†çš„è¯¦æƒ…ï¼Œä»»åŠ¡ä¸­æ­¢ã€‚")
                    return
                
                # åˆ†é›†è§†é¢‘æµè´¨æ£€
                for ep in new_episodes_details:
                    has_valid_video = False
                    media_sources = ep.get("MediaSources", []) # æ³¨æ„ï¼šget_emby_items_by_id è¿”å›çš„ç»“æ„å¯èƒ½ç›´æ¥åŒ…å« MediaStreamsï¼Œä¹Ÿå¯èƒ½åœ¨ MediaSources é‡Œï¼Œè§† Emby ç‰ˆæœ¬è€Œå®šã€‚
                    # é€šå¸¸ get_emby_items_by_id å¦‚æœæŒ‡å®šäº† MediaStreams å­—æ®µï¼Œä¼šç›´æ¥è¿”å›åœ¨æ ¹å¯¹è±¡æˆ– MediaSources ä¸­
                    # è¿™é‡Œåšä¸€ä¸ªå…¼å®¹æ€§æ£€æŸ¥
                    streams = ep.get("MediaStreams", [])
                    if not streams and media_sources:
                        streams = media_sources[0].get("MediaStreams", [])
                    
                    for stream in streams:
                        if stream.get("Type") == "Video":
                            has_valid_video = True
                            break
                    
                    if not has_valid_video:
                        s_num = ep.get("ParentIndexNumber", "?")
                        e_num = ep.get("IndexNumber", "?")
                        ep_name = ep.get("Name", "æœªçŸ¥åˆ†é›†")
                        
                        # æ„é€ æ˜ç¡®çš„é”™è¯¯åŸå› 
                        fail_reason = f"S{s_num}E{e_num} ({ep_name}) ç¼ºå¤±è§†é¢‘æµæ•°æ®"
                        logger.warning(f"  âœ [è´¨æ£€å¤±è´¥] å‰§é›†ã€Š{series_details.get('Name')}ã€‹çš„åˆ†é›† {fail_reason}ã€‚")
                        
                        # â˜…â˜…â˜… å…³é”®ï¼šè®°å½•åœ¨çˆ¶å‰§é›† ID ä¸Š â˜…â˜…â˜…
                        # è¿™æ ·åœ¨å¾…å¤æ ¸åˆ—è¡¨ä¸­ï¼Œä½ ä¼šçœ‹åˆ°è¿™éƒ¨å‰§ï¼ŒåŸå› æ˜¯â€œS01E05 ç¼ºå¤±è§†é¢‘æµ...â€
                        with get_central_db_connection() as conn:
                            self.log_db_manager.save_to_failed_log(
                                conn.cursor(), 
                                item_id,  # ä½¿ç”¨çˆ¶å‰§é›† ID
                                series_details.get('Name'), 
                                fail_reason, 
                                "Series", 
                                score=0.0
                            )
                            # åŒæ—¶ä¹Ÿæ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆé˜²æ­¢é‡å¤ï¼‰ï¼Œä½†åœ¨UIä¸­å¯è§
                            self._mark_item_as_processed(conn.cursor(), item_id, series_details.get('Name'), score=0.0)

                metadata_batch = []
                episodes_by_season = defaultdict(list)
                for episode in new_episodes_details:
                    if season_num := episode.get("ParentIndexNumber"):
                        episodes_by_season[season_num].append(episode)

                for season_num, emby_episodes_in_season in episodes_by_season.items():
                    season_details_from_tmdb = tmdb.get_season_details_tmdb(
                        tv_id=series_tmdb_id, season_number=season_num,
                        api_key=self.tmdb_api_key, item_name=series_details.get('Name')
                    )
                    if not season_details_from_tmdb or not season_details_from_tmdb.get("episodes"):
                        continue
                    tmdb_episode_map = {ep.get("episode_number"): ep for ep in season_details_from_tmdb["episodes"]}

                    for emby_episode in emby_episodes_in_season:
                        e_num = emby_episode.get("IndexNumber")
                        tmdb_details = tmdb_episode_map.get(e_num)
                        if not tmdb_details or not tmdb_details.get("id"):
                            continue
                        
                        asset_details = parse_full_asset_details(emby_episode)
                        emby_runtime = round(emby_episode['RunTimeTicks'] / 600000000) if emby_episode.get('RunTimeTicks') else None
                        metadata_to_add = {
                            "tmdb_id": str(tmdb_details.get("id")), "item_type": "Episode",
                            "parent_series_tmdb_id": str(series_tmdb_id),
                            "season_number": season_num, "episode_number": e_num,
                            "in_library": True, "subscription_status": "NONE",
                            "emby_item_ids_json": json.dumps([emby_episode.get("Id")]),
                            "title": tmdb_details.get("name"), "overview": tmdb_details.get("overview"),
                            "release_date": tmdb_details.get("air_date"),
                            "runtime_minutes": emby_runtime if emby_runtime else tmdb_details.get("runtime"),
                            "asset_details_json": json.dumps([asset_details], ensure_ascii=False)
                        }
                        metadata_batch.append(metadata_to_add)
                
                if metadata_batch:
                    with get_central_db_connection() as conn:
                        with conn.cursor() as cursor:
                            for metadata in metadata_batch:
                                columns = list(metadata.keys())
                                update_clauses = [f"{col} = EXCLUDED.{col}" for col in columns if col not in ['tmdb_id', 'item_type']]
                                update_clauses.append("last_synced_at = NOW()")
                                sql = f"""
                                    INSERT INTO media_metadata ({', '.join(columns)}, last_synced_at)
                                    VALUES ({', '.join(['%s'] * len(columns))}, NOW())
                                    ON CONFLICT (tmdb_id, item_type) DO UPDATE SET {', '.join(update_clauses)}
                                """
                                cursor.execute(sql, tuple(metadata.values()))
                            conn.commit()
                    logger.info(f"  âœ {log_prefix} [å¢é‡æ¨¡å¼] æˆåŠŸå°† {len(metadata_batch)} ä¸ªæ–°åˆ†é›†è®°å½•åŒæ­¥åˆ°æ•°æ®åº“ã€‚")
                return

            else:
                # --- æ¨¡å¼äºŒï¼šå¸¸è§„å…ƒæ•°æ®åˆ·æ–° ---
                fields_to_get = "ProviderIds,Type,Name,OriginalTitle,Overview,Tags,OfficialRating,MediaStreams,Container,Size,Path"
                item_details = emby.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id, fields=fields_to_get)
                if not item_details:
                    logger.warning(f"  âœ {log_prefix} æ— æ³•è·å–é¡¹ç›® {item_id} çš„è¯¦æƒ…ï¼Œè·³è¿‡ã€‚")
                    return
                
                tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
                item_type = item_details.get("Type")
                if not tmdb_id or item_type not in ['Movie', 'Series']:
                    logger.warning(f"  âœ {log_prefix} é¡¹ç›® '{item_details.get('Name')}' ä¸æ˜¯ç”µå½±æˆ–å‰§é›†ï¼Œæˆ–ç¼ºå°‘TMDb IDï¼Œè·³è¿‡ã€‚")
                    return

                with get_central_db_connection() as conn:
                    with conn.cursor() as cursor:
                        updates = {
                            "title": item_details.get('Name'), "original_title": item_details.get('OriginalTitle'),
                            "overview": item_details.get('Overview'), "official_rating": item_details.get('OfficialRating'),
                            "unified_rating": get_unified_rating(item_details.get('OfficialRating')),
                            "pre_cached_tags_json": json.dumps(item_details.get('Tags', []), ensure_ascii=False),
                            "last_synced_at": datetime.now(timezone.utc)
                        }
                        
                        if item_type == 'Movie':
                            asset_details = parse_full_asset_details(item_details)
                            updates["asset_details_json"] = json.dumps([asset_details], ensure_ascii=False)
                        
                        set_clauses = [f"{key} = %s" for key in updates.keys()]
                        sql = f"UPDATE media_metadata SET {', '.join(set_clauses)} WHERE tmdb_id = %s AND item_type = %s"
                        cursor.execute(sql, tuple(updates.values()) + (tmdb_id, item_type))
                        
                        if item_type == 'Series':
                            logger.info(f"  âœ {log_prefix} [å¸¸è§„æ¨¡å¼] æ£€æµ‹åˆ°å‰§é›†ï¼Œå¼€å§‹åŒæ­¥æ‰€æœ‰åˆ†é›†çš„èšåˆèµ„äº§è¯¦æƒ…...")
                            all_episode_versions = emby.get_all_library_versions(
                                base_url=self.emby_url, api_key=self.emby_api_key, user_id=self.emby_user_id,
                                media_type_filter="Episode", parent_id=item_id,
                                fields="Id,ProviderIds,MediaStreams,Container,Size,Path,DateCreated"
                            ) or []

                            episodes_grouped_by_tmdb_id = defaultdict(list)
                            for ep_version in all_episode_versions:
                                if ep_tmdb_id := ep_version.get("ProviderIds", {}).get("Tmdb"):
                                    episodes_grouped_by_tmdb_id[str(ep_tmdb_id)].append(ep_version)

                            if episodes_grouped_by_tmdb_id:
                                for ep_tmdb_id, versions in episodes_grouped_by_tmdb_id.items():
                                    asset_details = [parse_full_asset_details(v) for v in versions]
                                    asset_json = json.dumps(asset_details, ensure_ascii=False)
                                    
                                    cursor.execute(
                                        "UPDATE media_metadata SET asset_details_json = %s, last_synced_at = NOW() WHERE tmdb_id = %s AND item_type = 'Episode'",
                                        (asset_json, ep_tmdb_id)
                                    )
                                logger.info(f"  âœ {log_prefix} [å¸¸è§„æ¨¡å¼] æˆåŠŸæ›´æ–°äº† {len(episodes_grouped_by_tmdb_id)} ä¸ªåˆ†é›†çš„èšåˆèµ„äº§è¯¦æƒ…ã€‚")
                        conn.commit()
                logger.info(f"  âœ {log_prefix} [å¸¸è§„æ¨¡å¼] æˆåŠŸæ›´æ–°äº†é¡¹ç›®çš„æ ¸å¿ƒå…ƒæ•°æ®åŠèµ„äº§è¯¦æƒ…ã€‚")

        except Exception as e:
            logger.error(f"{log_prefix} æ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

    # --- å°†æ¥è‡ª Emby çš„å®æ—¶å…ƒæ•°æ®æ›´æ–°åŒæ­¥åˆ° override ç¼“å­˜æ–‡ä»¶ ---
    def sync_emby_updates_to_override_files(self, item_details: Dict[str, Any]):
        """
        å°†æ¥è‡ª Emby çš„å®æ—¶å…ƒæ•°æ®æ›´æ–°åŒæ­¥åˆ° override ç¼“å­˜æ–‡ä»¶ã€‚
        è¿™æ˜¯ä¸€ä¸ª "è¯»-æ”¹-å†™" æ“ä½œï¼Œç”¨äºæŒä¹…åŒ–ç”¨æˆ·åœ¨ Emby UI ä¸Šçš„ä¿®æ”¹ã€‚
        """
        item_id = item_details.get("Id")
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        item_type = item_details.get("Type")
        tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
        log_prefix = "[è¦†ç›–ç¼“å­˜-å…ƒæ•°æ®æŒä¹…åŒ–]"

        if not all([item_id, item_type, tmdb_id, self.local_data_path]):
            logger.warning(f"  âœ {log_prefix} è·³è¿‡ '{item_name_for_log}'ï¼Œç¼ºå°‘å…³é”®IDæˆ–è·¯å¾„é…ç½®ã€‚")
            return

        logger.info(f"  âœ {log_prefix} å¼€å§‹ä¸º '{item_name_for_log}' æ›´æ–°è¦†ç›–ç¼“å­˜æ–‡ä»¶...")

        # --- å®šä½ä¸»æ–‡ä»¶ ---
        cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
        target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
        main_json_filename = "all.json" if item_type == "Movie" else "series.json"
        main_json_path = os.path.join(target_override_dir, main_json_filename)

        # --- å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœ override æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯´æ˜ä»æœªè¢«å®Œæ•´å¤„ç†è¿‡ï¼Œä¸åº”ç»§ç»­ ---
        if not os.path.exists(main_json_path):
            logger.warning(f"  âœ {log_prefix} æ— æ³•æŒä¹…åŒ–ä¿®æ”¹ï¼šä¸»è¦†ç›–æ–‡ä»¶ '{main_json_path}' ä¸å­˜åœ¨ã€‚è¯·å…ˆå¯¹è¯¥é¡¹ç›®è¿›è¡Œä¸€æ¬¡å®Œæ•´å¤„ç†ã€‚")
            return

        try:
            # --- æ ¸å¿ƒçš„ "è¯»-æ”¹-å†™" é€»è¾‘ ---
            with open(main_json_path, 'r+', encoding='utf-8') as f:
                data = json.load(f)

                # å®šä¹‰è¦ä» Emby åŒæ­¥çš„å­—æ®µ
                fields_to_update = {
                    "Name": "title",
                    "OriginalTitle": "original_title",
                    "Overview": "overview",
                    "Tagline": "tagline",
                    "CommunityRating": "vote_average", # ç”¨æˆ·è¯„åˆ†
                    "OfficialRating": "official_rating",
                    "Genres": "genres",
                    "Studios": "production_companies",
                    "Tags": "keywords"
                }
                
                updated_count = 0
                for emby_key, json_key in fields_to_update.items():
                    if emby_key in item_details:
                        new_value = item_details[emby_key]
                        # ç‰¹æ®Šå¤„ç† Studios å’Œ Genres
                        if emby_key in ["Studios", "Genres"]:
                            # å‡è®¾æºæ•°æ®æ˜¯ [{ "Name": "Studio A" }] æˆ– ["Action"]
                            if isinstance(new_value, list):
                                if emby_key == "Studios":
                                     data[json_key] = [{"name": s.get("Name")} for s in new_value if s.get("Name")]
                                else: # Genres
                                     data[json_key] = new_value
                                updated_count += 1
                        else:
                            data[json_key] = new_value
                            updated_count += 1
                
                # å¤„ç†æ—¥æœŸ
                if 'PremiereDate' in item_details:
                    data['release_date'] = (item_details['PremiereDate'] or '').split('T')[0]
                    updated_count += 1

                logger.info(f"  âœ {log_prefix} å‡†å¤‡å°† {updated_count} é¡¹æ›´æ–°å†™å…¥ '{main_json_filename}'ã€‚")

                f.seek(0)
                json.dump(data, f, ensure_ascii=False, indent=2)
                f.truncate()

            # å¦‚æœæ˜¯å‰§é›†ï¼Œè¿˜éœ€è¦æ›´æ–°æ‰€æœ‰å­æ–‡ä»¶çš„ name å’Œ overview
            if item_type == "Series":
                logger.info(f"  âœ {log_prefix} æ£€æµ‹åˆ°ä¸ºå‰§é›†ï¼Œå¼€å§‹åŒæ­¥æ›´æ–°å­é¡¹ï¼ˆå­£/é›†ï¼‰çš„å…ƒæ•°æ®...")
                self._inject_cast_to_series_files(
                    target_dir=target_override_dir,
                    cast_list=None, # â˜…â˜…â˜… å…³é”®ï¼šä¼ å…¥ None è¡¨ç¤ºæˆ‘ä»¬åªæ›´æ–°å…ƒæ•°æ®ï¼Œä¸ç¢°æ¼”å‘˜è¡¨ â˜…â˜…â˜…
                    series_details=item_details,
                    source_dir=os.path.join(self.local_data_path, "cache", cache_folder_name, tmdb_id)
                )

            logger.info(f"  âœ {log_prefix} æˆåŠŸä¸º '{item_name_for_log}' æŒä¹…åŒ–äº†å…ƒæ•°æ®ä¿®æ”¹ã€‚")

        except Exception as e:
            logger.error(f"  âœ {log_prefix} ä¸º '{item_name_for_log}' æ›´æ–°è¦†ç›–ç¼“å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


    def close(self):
        if self.douban_api: self.douban_api.close()
