# core_processor.py

import os
import json
import time
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict
import shutil
import threading
from datetime import datetime, timezone
import time as time_module
import psycopg2
# ç¡®ä¿æ‰€æœ‰ä¾èµ–éƒ½å·²æ­£ç¡®å¯¼å…¥
import handler.emby as emby
import handler.tmdb as tmdb
from tasks.helpers import parse_full_asset_details, calculate_ancestor_ids
import utils
import constants
import logging
import actor_utils
from database.actor_db import ActorDBManager
from database.log_db import LogDBManager
from database.connection import get_db_connection as get_central_db_connection
from cachetools import TTLCache
from ai_translator import AITranslator
from watchlist_processor import WatchlistProcessor
from handler.douban import DoubanApi

logger = logging.getLogger(__name__)
try:
    from handler.douban import DoubanApi
    DOUBAN_API_AVAILABLE = True
except ImportError:
    DOUBAN_API_AVAILABLE = False
    class DoubanApi:
        def __init__(self, *args, **kwargs): pass
        def get_acting(self, *args, **kwargs): return {}
        def close(self): pass

def extract_tag_names(item_data):
    """
    å…¼å®¹æ–°æ—§ç‰ˆ Emby API æå–æ ‡ç­¾åã€‚
    """
    tags_set = set()
    # 1. TagItems
    tag_items = item_data.get('TagItems')
    if isinstance(tag_items, list):
        for t in tag_items:
            if isinstance(t, dict):
                name = t.get('Name')
                if name: tags_set.add(name)
            elif isinstance(t, str) and t:
                tags_set.add(t)
    # 2. Tags
    tags = item_data.get('Tags')
    if isinstance(tags, list):
        for t in tags:
            if t: tags_set.add(str(t))
    return list(tags_set)

def _read_local_json(file_path: str) -> Optional[Dict[str, Any]]:
    if not os.path.exists(file_path):
        logger.warning(f"æœ¬åœ°å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"è¯»å–æœ¬åœ°JSONæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return None

def _aggregate_series_cast_from_tmdb_data(series_data: Dict[str, Any], all_episodes_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ã€æ–°ã€‘ä»å†…å­˜ä¸­çš„TMDBæ•°æ®èšåˆä¸€ä¸ªå‰§é›†çš„æ‰€æœ‰æ¼”å‘˜ã€‚
    """
    logger.debug(f"ã€æ¼”å‘˜èšåˆã€‘å¼€å§‹ä¸º '{series_data.get('name')}' ä»å†…å­˜ä¸­çš„TMDBæ•°æ®èšåˆæ¼”å‘˜...")
    aggregated_cast_map = {}

    # 1. ä¼˜å…ˆå¤„ç†ä¸»å‰§é›†çš„æ¼”å‘˜åˆ—è¡¨
    main_cast = series_data.get("credits", {}).get("cast", [])
    for actor in main_cast:
        actor_id = actor.get("id")
        if actor_id:
            aggregated_cast_map[actor_id] = actor
    logger.debug(f"  âœ ä»ä¸»å‰§é›†æ•°æ®ä¸­åŠ è½½äº† {len(aggregated_cast_map)} ä½ä¸»æ¼”å‘˜ã€‚")

    # 2. èšåˆæ‰€æœ‰åˆ†é›†çš„æ¼”å‘˜å’Œå®¢ä¸²æ¼”å‘˜
    for episode_data in all_episodes_data:
        credits_data = episode_data.get("credits", {})
        actors_to_process = credits_data.get("cast", []) + credits_data.get("guest_stars", [])
        
        for actor in actors_to_process:
            actor_id = actor.get("id")
            if actor_id and actor_id not in aggregated_cast_map:
                if 'order' not in actor:
                    actor['order'] = 999  # ä¸ºå®¢ä¸²æ¼”å‘˜è®¾ç½®é«˜orderå€¼
                aggregated_cast_map[actor_id] = actor

    full_aggregated_cast = list(aggregated_cast_map.values())
    full_aggregated_cast.sort(key=lambda x: x.get('order', 999))
    
    logger.info(f"  âœ å…±ä¸º '{series_data.get('name')}' èšåˆäº† {len(full_aggregated_cast)} ä½ç‹¬ç«‹æ¼”å‘˜ã€‚")
    return full_aggregated_cast
class MediaProcessor:
    def __init__(self, config: Dict[str, Any]):
        # â˜…â˜…â˜… ç„¶åï¼Œä»è¿™ä¸ª config å­—å…¸é‡Œï¼Œè§£æå‡ºæ‰€æœ‰éœ€è¦çš„å±æ€§ â˜…â˜…â˜…
        self.config = config

        # åˆå§‹åŒ–æˆ‘ä»¬çš„æ•°æ®åº“ç®¡ç†å‘˜
        self.actor_db_manager = ActorDBManager()
        self.log_db_manager = LogDBManager()

        # ä» config ä¸­è·å–æ‰€æœ‰å…¶ä»–é…ç½®
        self.douban_api = None
        if getattr(constants, 'DOUBAN_API_AVAILABLE', False):
            try:
                # --- âœ¨âœ¨âœ¨ æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ START âœ¨âœ¨âœ¨ ---

                # 1. ä»é…ç½®ä¸­è·å–å†·å´æ—¶é—´ 
                douban_cooldown = self.config.get(constants.CONFIG_OPTION_DOUBAN_DEFAULT_COOLDOWN, 2.0)
                
                # 2. ä»é…ç½®ä¸­è·å– Cookieï¼Œä½¿ç”¨æˆ‘ä»¬åˆšåˆšåœ¨ constants.py ä¸­å®šä¹‰çš„å¸¸é‡
                douban_cookie = self.config.get(constants.CONFIG_OPTION_DOUBAN_COOKIE, "")
                
                # 3. æ·»åŠ ä¸€ä¸ªæ—¥å¿—ï¼Œæ–¹ä¾¿è°ƒè¯•
                if not douban_cookie:
                    logger.debug(f"é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æˆ–æœªè®¾ç½® '{constants.CONFIG_OPTION_DOUBAN_COOKIE}'ã€‚å¦‚æœè±†ç“£APIè¿”å›'need_login'é”™è¯¯ï¼Œè¯·é…ç½®è±†ç“£cookieã€‚")
                else:
                    logger.debug("å·²ä»é…ç½®ä¸­åŠ è½½è±†ç“£ Cookieã€‚")

                # 4. å°†æ‰€æœ‰å‚æ•°ä¼ é€’ç»™ DoubanApi çš„æ„é€ å‡½æ•°
                self.douban_api = DoubanApi(
                    cooldown_seconds=douban_cooldown,
                    user_cookie=douban_cookie  # <--- å°† cookie ä¼ è¿›å»
                )
                logger.trace("DoubanApi å®ä¾‹å·²åœ¨ MediaProcessorAPI ä¸­åˆ›å»ºã€‚")
                
                # --- âœ¨âœ¨âœ¨ æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ END âœ¨âœ¨âœ¨ ---

            except Exception as e:
                logger.error(f"MediaProcessorAPI åˆå§‹åŒ– DoubanApi å¤±è´¥: {e}", exc_info=True)
        else:
            logger.warning("DoubanApi å¸¸é‡æŒ‡ç¤ºä¸å¯ç”¨ï¼Œå°†ä¸ä½¿ç”¨è±†ç“£åŠŸèƒ½ã€‚")
        self.emby_url = self.config.get("emby_server_url")
        self.emby_api_key = self.config.get("emby_api_key")
        self.emby_user_id = self.config.get("emby_user_id")
        self.tmdb_api_key = self.config.get("tmdb_api_key", "")
        self.local_data_path = self.config.get("local_data_path", "").strip()
        
        self.ai_enabled = self.config.get("ai_translation_enabled", False)
        self.ai_translator = AITranslator(self.config) if self.ai_enabled else None
        
        self._stop_event = threading.Event()
        self.processed_items_cache = self._load_processed_log_from_db()
        self.manual_edit_cache = TTLCache(maxsize=10, ttl=600)
        self._global_lib_guid_map = {}
        self._last_lib_map_update = 0
        logger.trace("æ ¸å¿ƒå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆã€‚")

    def _refresh_lib_guid_map(self):
        """ä» Emby å®æ—¶è·å–æ‰€æœ‰åª’ä½“åº“çš„ ID åˆ° GUID æ˜ å°„"""
        try:
            # è°ƒç”¨ emby.py ä¸­çš„å‡½æ•°
            libs_data = emby.get_all_libraries_with_paths(self.emby_url, self.emby_api_key)
            new_map = {}
            for lib in libs_data:
                info = lib.get('info', {})
                l_id = str(info.get('Id'))
                l_guid = str(info.get('Guid'))
                if l_id and l_guid:
                    new_map[l_id] = l_guid
            
            self._global_lib_guid_map = new_map
            self._last_lib_map_update = time.time()
            logger.debug(f"  âœ å·²åˆ·æ–°åª’ä½“åº“ GUID æ˜ å°„è¡¨ï¼Œå…±åŠ è½½ {len(new_map)} ä¸ªåº“ã€‚")
        except Exception as e:
            logger.error(f"åˆ·æ–°åª’ä½“åº“ GUID æ˜ å°„å¤±è´¥: {e}")

    # --- å®æ—¶è·å–é¡¹ç›®çš„ç¥–å…ˆåœ°å›¾å’Œåº“ GUID ---
    def _get_realtime_ancestor_context(self, item_id: str, source_lib_id: str) -> Tuple[Dict[str, str], Optional[str]]:
        """
        å®æ—¶è·å–é¡¹ç›®çš„ç¥–å…ˆåœ°å›¾å’Œåº“ GUIDã€‚
        """
        id_to_parent_map = {}
        # 1. è·å– GUID æ˜ å°„ (ä¿æŒä¸å˜)
        if not self._global_lib_guid_map or (time.time() - self._last_lib_map_update > 3600):
            self._refresh_lib_guid_map()
        lib_guid = self._global_lib_guid_map.get(str(source_lib_id))

        # 3. å‘ä¸Šçˆ¬æ ‘æ„å»ºçˆ¶å­å…³ç³»ï¼ˆç”¨äºè®¡ç®— ancestor_idsï¼‰
        try:
            curr_id = item_id
            for _ in range(10):
                # å®æ—¶å…¥åº“åªéœ€è¦ ParentId å³å¯ï¼Œä¸éœ€è¦å†è¯·æ±‚ Guid å­—æ®µ
                details = emby.get_emby_item_details(
                    curr_id, 
                    self.emby_url, 
                    self.emby_api_key, 
                    self.emby_user_id,
                    fields="ParentId",
                    silent_404=True
                )
                if not details: break
                
                p_id = details.get('ParentId')
                if p_id == str(source_lib_id) and lib_guid:
                    # æ„é€  Emby ç‰¹æœ‰çš„å¤åˆ ID: GUID_æ•°å­—ID
                    composite_id = f"{lib_guid}_{p_id}"
                    id_to_parent_map[curr_id] = composite_id
                    # å¤åˆ ID çš„çˆ¶çº§æ˜¯ç³»ç»Ÿæ ¹èŠ‚ç‚¹ "1"
                    id_to_parent_map[composite_id] = "1"
                    break 
                
                if p_id and p_id != '1':
                    id_to_parent_map[str(curr_id)] = p_id
                    curr_id = p_id
                else:
                    break
        except Exception as e:
            logger.error(f"å®æ—¶æ„å»ºçˆ¬æ ‘åœ°å›¾å¤±è´¥: {e}")

        return id_to_parent_map, lib_guid

    # --- æ›´æ–°åª’ä½“å…ƒæ•°æ®ç¼“å­˜ ---
    def _upsert_media_metadata(
        self,
        cursor: psycopg2.extensions.cursor,
        item_type: str,
        final_processed_cast: List[Dict[str, Any]],
        source_data_package: Optional[Dict[str, Any]],
        item_details_from_emby: Optional[Dict[str, Any]] = None
    ):
        """
        - å®æ—¶å…ƒæ•°æ®å†™å…¥ã€‚
        ã€å¢å¼ºä¿®å¤ç‰ˆ V2ã€‘
        1. å…³é”®è¯æå–é‡‡ç”¨æ··åˆç­–ç•¥ï¼ŒåŒæ—¶æŸ¥æ‰¾ results å’Œ keywordsï¼Œé˜²æ­¢ç»“æ„ä¸ä¸€è‡´å¯¼è‡´ä¸¢å¤±ã€‚
        2. å‰§é›†å·¥ä½œå®¤ä¼˜å…ˆä½¿ç”¨ networksã€‚
        """
        if not item_details_from_emby:
            logger.error("  âœ å†™å…¥å…ƒæ•°æ®ç¼“å­˜å¤±è´¥ï¼šç¼ºå°‘ Emby è¯¦æƒ…æ•°æ®ã€‚")
            return
        item_id = str(item_details_from_emby.get('Id'))
        source_lib_id = str(item_details_from_emby.get('_SourceLibraryId'))

        id_to_parent_map, lib_guid = self._get_realtime_ancestor_context(item_id, source_lib_id)

        def get_representative_runtime(emby_items, tmdb_runtime):
            if not emby_items: return tmdb_runtime
            runtimes = [round(item['RunTimeTicks'] / 600000000) for item in emby_items if item.get('RunTimeTicks')]
            return max(runtimes) if runtimes else tmdb_runtime
        
        # â˜…â˜…â˜… å†…éƒ¨è¾…åŠ©å‡½æ•°ï¼šå¼ºåŠ›æå–é€šç”¨ JSON å­—æ®µ (ä¿®å¤ç‰ˆ) â˜…â˜…â˜…
        def _extract_common_json_fields(details: Dict[str, Any], m_type: str):
            # 1. Genres (ç±»å‹)
            genres_raw = details.get('genres', [])
            genres_list = []
            for g in genres_raw:
                if isinstance(g, dict): genres_list.append(g.get('name'))
                elif isinstance(g, str): genres_list.append(g)
            genres_json = json.dumps([n for n in genres_list if n], ensure_ascii=False)

            # 2. Studios (å·¥ä½œå®¤/åˆ¶ä½œå…¬å¸/ç”µè§†ç½‘)
            # â˜… åŸºç¡€ï¼šè·å–åˆ¶ä½œå…¬å¸ (ä½¿ç”¨ or [] é˜²æ­¢ None)
            raw_studios = details.get('production_companies') or []
            # ç¡®ä¿æ˜¯åˆ—è¡¨å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸæ•°æ®
            if isinstance(raw_studios, list):
                raw_studios = list(raw_studios)
            else:
                raw_studios = []

            if m_type == 'Series':
                # â˜… å‰§é›†ï¼šè¿½åŠ  networks (æ’­å‡ºå¹³å°)
                networks = details.get('networks') or []
                if isinstance(networks, list):
                    raw_studios.extend(networks)
            
            # å»é‡ (ä½¿ç”¨å­—å…¸ä»¥ ID ä¸ºé”®)
            unique_studios_map = {}
            for s in raw_studios:
                if isinstance(s, dict):
                    s_id = s.get('id')
                    s_name = s.get('name')
                    if s_name:
                        # åæ¥çš„è¦†ç›–å‰é¢çš„ï¼ˆé€šå¸¸ Networks åœ¨åï¼Œä¿ç•™ Networks æ›´åˆç†ï¼‰
                        unique_studios_map[s_id] = {'id': s_id, 'name': s_name}
                elif isinstance(s, str) and s:
                    unique_studios_map[s] = {'id': None, 'name': s}
            
            studios_json = json.dumps(list(unique_studios_map.values()), ensure_ascii=False)

            # 3. Keywords (å…³é”®è¯)
            # å…¼å®¹ keywords (dict/list) å’Œ tags (list)
            keywords_data = details.get('keywords') or details.get('tags') or []
            raw_k_list = []
            
            if isinstance(keywords_data, dict):
                # â˜…â˜…â˜… æ··åˆç­–ç•¥ï¼šä¼˜å…ˆæ ¹æ®ç±»å‹å–å€¼ï¼Œå–ä¸åˆ°å†å°è¯•å¦ä¸€ç§ â˜…â˜…â˜…
                if m_type == 'Series':
                    # å‰§é›†é€šå¸¸åœ¨ 'results' ä¸­
                    raw_k_list = keywords_data.get('results')
                else:
                    # ç”µå½±é€šå¸¸åœ¨ 'keywords' ä¸­
                    raw_k_list = keywords_data.get('keywords')
                
                # å…œåº•ï¼šå¦‚æœé¦–é€‰é”®æ²¡æœ‰æ•°æ®ï¼Œå°è¯•å¦ä¸€ä¸ª (é˜²æ­¢æ•°æ®ç»“æ„æ··ä¹±)
                if not raw_k_list:
                    raw_k_list = keywords_data.get('results') or keywords_data.get('keywords') or []
            elif isinstance(keywords_data, list):
                # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ (å¯èƒ½æ˜¯æœ¬åœ°ç¼“å­˜è¢«æ‰å¹³åŒ–è¿‡)ï¼Œç›´æ¥ä½¿ç”¨
                raw_k_list = keywords_data
            
            keywords = []
            for k in raw_k_list:
                if isinstance(k, dict) and k.get('name'):
                    keywords.append({'id': k.get('id'), 'name': k.get('name')})
                elif isinstance(k, str) and k:
                    keywords.append({'id': None, 'name': k})
            keywords_json = json.dumps(keywords, ensure_ascii=False)

            # 4. Countries (å›½å®¶)
            countries_raw = details.get('production_countries') or details.get('origin_country') or []
            country_codes = []
            for c in countries_raw:
                if isinstance(c, dict): 
                    code = c.get('iso_3166_1')
                    if code: country_codes.append(code)
                elif isinstance(c, str) and c: 
                    country_codes.append(c)
            
            countries_json = json.dumps(country_codes, ensure_ascii=False)

            return genres_json, studios_json, keywords_json, countries_json

        try:
            from psycopg2.extras import execute_batch
            
            if not source_data_package:
                logger.warning("  âœ å…ƒæ•°æ®å†™å…¥è·³è¿‡ï¼šæœªæä¾›æºæ•°æ®åŒ…ã€‚")
                return

            records_to_upsert = []

            # ç”Ÿæˆå‘é‡é€»è¾‘
            overview_embedding_json = None
            if item_type in ["Movie", "Series"] and self.ai_translator:
                overview_text = source_data_package.get('overview') or item_details_from_emby.get('Overview')
                if overview_text and self.config.get("ai_translation_enabled", False):
                    try:
                        embedding = self.ai_translator.generate_embedding(overview_text)
                        if embedding:
                            overview_embedding_json = json.dumps(embedding)
                    except Exception as e_embed:
                        logger.warning(f"  âœ ç”Ÿæˆå‘é‡å¤±è´¥: {e_embed}")
            
            # ==================================================================
            # å¤„ç†ç”µå½± (Movie)
            # ==================================================================
            if item_type == "Movie":
                movie_record = source_data_package.copy()
                movie_record['item_type'] = 'Movie'
                movie_record['tmdb_id'] = str(movie_record.get('id'))
                movie_record['runtime_minutes'] = get_representative_runtime([item_details_from_emby], movie_record.get('runtime'))
                movie_record['rating'] = movie_record.get('vote_average')
                asset_details = parse_full_asset_details(
                    item_details_from_emby, 
                    id_to_parent_map=id_to_parent_map, 
                    library_guid=lib_guid
                )
                asset_details['source_library_id'] = source_lib_id
                
                movie_record['asset_details_json'] = json.dumps([asset_details], ensure_ascii=False)
                movie_record['emby_item_ids_json'] = json.dumps([item_id])
                movie_record['actors_json'] = json.dumps([{"tmdb_id": int(p.get("id")), "character": p.get("character"), "order": p.get("order")} for p in final_processed_cast if p.get("id")], ensure_ascii=False)
                movie_record['in_library'] = True
                movie_record['subscription_status'] = 'NONE'
                movie_record['date_added'] = item_details_from_emby.get("DateCreated")
                movie_record['overview_embedding'] = overview_embedding_json

                # â˜…â˜…â˜… æå–é€šç”¨å­—æ®µ (ä¼ å…¥ 'Movie') â˜…â˜…â˜…
                g_json, s_json, k_json, c_json = _extract_common_json_fields(source_data_package, 'Movie')
                movie_record['genres_json'] = g_json
                movie_record['studios_json'] = s_json
                movie_record['keywords_json'] = k_json
                movie_record['countries_json'] = c_json

                # â˜…â˜…â˜… æå–åˆ†çº§ (Rating) â˜…â˜…â˜…
                raw_ratings_map = {}
                # source_data_package å°±æ˜¯ TMDb çš„ movie details
                results = source_data_package.get('release_dates', {}).get('results', [])
                for r in results:
                    country = r.get('iso_3166_1')
                    if not country: continue
                    cert = None
                    for release in r.get('release_dates', []):
                        if release.get('certification'):
                            cert = release.get('certification')
                            break
                    if cert:
                        raw_ratings_map[country] = cert
                
                # â˜…â˜…â˜… 2. å­˜å…¥ rating_json â˜…â˜…â˜…
                movie_record['rating_json'] = json.dumps(raw_ratings_map, ensure_ascii=False)
                
                # å¯¼æ¼” (ç”µå½±åœ¨ credits.crew ä¸­)
                crew = source_data_package.get("credits", {}).get('crew', [])
                movie_record['directors_json'] = json.dumps([{'id': p.get('id'), 'name': p.get('name')} for p in crew if p.get('job') == 'Director'], ensure_ascii=False)

                records_to_upsert.append(movie_record)

            # ==================================================================
            # å¤„ç†å‰§é›† (Series)
            # ==================================================================
            elif item_type == "Series":
                series_details = source_data_package.get("series_details", source_data_package)
                seasons_details = source_data_package.get("seasons_details", series_details.get("seasons", []))
                
                series_asset_details = []
                series_path = item_details_from_emby.get('Path')
                if series_path:
                    series_asset = {
                        "path": series_path,
                        "source_library_id": source_lib_id,
                        "ancestor_ids": calculate_ancestor_ids(item_id, id_to_parent_map, lib_guid)
                    }
                    series_asset_details.append(series_asset)

                # æ„å»º Series è®°å½•
                series_record = {
                    "item_type": "Series", "tmdb_id": str(series_details.get('id')), "title": series_details.get('name'),
                    "original_title": series_details.get('original_name'), "overview": series_details.get('overview'),
                    "release_date": series_details.get('first_air_date'), "poster_path": series_details.get('poster_path'),
                    "rating": series_details.get('vote_average'),
                    "total_episodes": series_details.get('number_of_episodes', 0),
                    "watchlist_tmdb_status": series_details.get('status'),
                    "asset_details_json": json.dumps(series_asset_details, ensure_ascii=False),
                    "overview_embedding": overview_embedding_json
                }
                
                actors_relation = [{"tmdb_id": int(p.get("id")), "character": p.get("character"), "order": p.get("order")} for p in final_processed_cast if p.get("id")]
                series_record['actors_json'] = json.dumps(actors_relation, ensure_ascii=False)
                
                # åˆ†çº§
                raw_ratings_map = {}
                results = series_details.get('content_ratings', {}).get('results', [])
                for r in results:
                    country = r.get('iso_3166_1')
                    rating = r.get('rating')
                    if country and rating:
                        raw_ratings_map[country] = rating
                
                # â˜…â˜…â˜… 4. å­˜å…¥ rating_json â˜…â˜…â˜…
                series_record['rating_json'] = json.dumps(raw_ratings_map, ensure_ascii=False)

                # â˜…â˜…â˜… æå–é€šç”¨å­—æ®µ (ä¼ å…¥ 'Series') â˜…â˜…â˜…
                g_json, s_json, k_json, c_json = _extract_common_json_fields(series_details, 'Series')
                series_record['genres_json'] = g_json
                series_record['studios_json'] = s_json
                series_record['keywords_json'] = k_json
                series_record['countries_json'] = c_json
                
                # åˆ›ä½œè€…/å¯¼æ¼” (å‰§é›†åœ¨ created_by ä¸­)
                series_record['directors_json'] = json.dumps([{'id': c.get('id'), 'name': c.get('name')} for c in series_details.get('created_by', [])], ensure_ascii=False)
                
                languages_list = series_details.get('languages', [])
                series_record['original_language'] = series_details.get('original_language') or (languages_list[0] if languages_list else None)
                series_record['in_library'] = True
                series_record['subscription_status'] = 'NONE'
                series_record['emby_item_ids_json'] = json.dumps([item_details_from_emby.get('Id')])
                series_record['date_added'] = item_details_from_emby.get("DateCreated")
                series_record['ignore_reason'] = None
                records_to_upsert.append(series_record)

                # â˜…â˜…â˜… 3. å¤„ç†å­£ (Season) â˜…â˜…â˜…
                emby_season_versions = emby.get_series_seasons(
                    series_id=item_details_from_emby.get('Id'),
                    base_url=self.emby_url,
                    api_key=self.emby_api_key,
                    user_id=self.emby_user_id,
                    series_name_for_log=series_details.get('name')
                ) or []
                seasons_grouped_by_number = defaultdict(list)
                for s_ver in emby_season_versions:
                    if s_ver.get("IndexNumber") is not None:
                        seasons_grouped_by_number[s_ver.get("IndexNumber")].append(s_ver)

                for season in seasons_details:
                    if not isinstance(season, dict): continue
                    s_num = season.get('season_number')
                    if s_num is None: continue 
                    try: s_num_int = int(s_num)
                    except ValueError: continue

                    season_poster = season.get('poster_path') or series_details.get('poster_path')
                    matched_emby_seasons = seasons_grouped_by_number.get(s_num_int, [])

                    records_to_upsert.append({
                        "tmdb_id": str(season.get('id')), "item_type": "Season", 
                        "parent_series_tmdb_id": str(series_details.get('id')), 
                        "title": season.get('name'), "overview": season.get('overview'), 
                        "release_date": season.get('air_date'), "poster_path": season_poster, 
                        "season_number": s_num,
                        "in_library": bool(matched_emby_seasons),
                        "emby_item_ids_json": json.dumps([s['Id'] for s in matched_emby_seasons]) if matched_emby_seasons else '[]'
                    })
                
                # â˜…â˜…â˜… 4. å¤„ç†åˆ†é›† (Episode) â˜…â˜…â˜…
                raw_episodes = source_data_package.get("episodes_details", {})
                episodes_details = list(raw_episodes.values()) if isinstance(raw_episodes, dict) else (raw_episodes if isinstance(raw_episodes, list) else [])
                
                emby_episode_versions = emby.get_all_library_versions(
                    base_url=self.emby_url, api_key=self.emby_api_key, user_id=self.emby_user_id,
                    media_type_filter="Episode", parent_id=item_details_from_emby.get('Id'),
                    fields="Id,Type,ParentIndexNumber,IndexNumber,MediaStreams,Container,Size,Path,ProviderIds,RunTimeTicks,DateCreated,_SourceLibraryId"
                ) or []
                episodes_grouped_by_number = defaultdict(list)
                for ep_version in emby_episode_versions:
                    s_num = ep_version.get("ParentIndexNumber")
                    e_num = ep_version.get("IndexNumber")
                    if s_num is not None and e_num is not None:
                        episodes_grouped_by_number[(s_num, e_num)].append(ep_version)

                for episode in episodes_details:
                    if episode.get('episode_number') is None: continue
                    s_num = episode.get('season_number')
                    e_num = episode.get('episode_number')
                    versions_of_episode = episodes_grouped_by_number.get((s_num, e_num))
                    final_runtime = get_representative_runtime(versions_of_episode, episode.get('runtime'))

                    episode_record = {
                        "tmdb_id": str(episode.get('id')), "item_type": "Episode", 
                        "parent_series_tmdb_id": str(series_details.get('id')), 
                        "title": episode.get('name'), "overview": episode.get('overview'), 
                        "release_date": episode.get('air_date'), 
                        "season_number": s_num, "episode_number": e_num,
                        "runtime_minutes": final_runtime
                    }
                    if versions_of_episode:
                        all_emby_ids = [v.get('Id') for v in versions_of_episode]
                        all_asset_details = []
                        for v in versions_of_episode:
                            details = parse_full_asset_details(v)
                            details['source_library_id'] = item_details_from_emby.get('_SourceLibraryId')
                            all_asset_details.append(details)
                        episode_record['asset_details_json'] = json.dumps(all_asset_details, ensure_ascii=False)
                        episode_record['emby_item_ids_json'] = json.dumps(all_emby_ids)
                        episode_record['in_library'] = True
                    records_to_upsert.append(episode_record)

            if not records_to_upsert:
                return

            # ==================================================================
            # æ‰¹é‡å†™å…¥æ•°æ®åº“
            # ==================================================================
            all_possible_columns = [
                "tmdb_id", "item_type", "title", "original_title", "overview", "release_date", "release_year",
                "original_language",
                "poster_path", "rating", "actors_json", "parent_series_tmdb_id", "season_number", "episode_number",
                "in_library", "subscription_status", "subscription_sources_json", "emby_item_ids_json", "date_added",
                "rating_json",
                "genres_json", "directors_json", "studios_json", "countries_json", "keywords_json", "ignore_reason",
                "asset_details_json",
                "runtime_minutes",
                "overview_embedding",
                "total_episodes",
                "watchlist_tmdb_status"
            ]
            data_for_batch = []
            for record in records_to_upsert:
                db_row_complete = {col: record.get(col) for col in all_possible_columns}
                
                if db_row_complete['in_library'] is None: db_row_complete['in_library'] = False
                if db_row_complete['subscription_status'] is None: db_row_complete['subscription_status'] = 'NONE'
                if db_row_complete['subscription_sources_json'] is None: db_row_complete['subscription_sources_json'] = '[]'
                if db_row_complete['emby_item_ids_json'] is None: db_row_complete['emby_item_ids_json'] = '[]'

                # æå–å¹´ä»½
                release_date_str = db_row_complete.get('release_date')
                if release_date_str and len(release_date_str) >= 4:
                    try: db_row_complete['release_year'] = int(release_date_str[:4])
                    except (ValueError, TypeError): pass
                
                data_for_batch.append(db_row_complete)

            cols_str = ", ".join(all_possible_columns)
            placeholders_str = ", ".join([f"%({col})s" for col in all_possible_columns])
            cols_to_update = [col for col in all_possible_columns if col not in ['tmdb_id', 'item_type']]
            
            cols_to_protect = ['subscription_sources_json']
            timestamp_field = "last_synced_at"
            
            for col in cols_to_protect:
                if col in cols_to_update: cols_to_update.remove(col)

            update_clauses = [f"{col} = EXCLUDED.{col}" for col in cols_to_update]
            update_clauses.append(f"{timestamp_field} = NOW()")

            sql = f"""
                INSERT INTO media_metadata ({cols_str})
                VALUES ({placeholders_str})
                ON CONFLICT (tmdb_id, item_type) DO UPDATE SET {', '.join(update_clauses)};
            """
            
            execute_batch(cursor, sql, data_for_batch)
            logger.info(f"  âœ æˆåŠŸå°† {len(data_for_batch)} æ¡å±‚çº§å…ƒæ•°æ®è®°å½•æ‰¹é‡å†™å…¥æ•°æ®åº“ã€‚")

        except Exception as e:
            logger.error(f"æ‰¹é‡å†™å…¥å±‚çº§å…ƒæ•°æ®åˆ°æ•°æ®åº“æ—¶å¤±è´¥: {e}", exc_info=True)
            raise

    # --- æ ‡è®°ä¸ºå·²å¤„ç† ---
    def _mark_item_as_processed(self, cursor: psycopg2.extensions.cursor, item_id: str, item_name: str, score: float = 10.0):
        """
        ã€é‡æ„ã€‘å°†ä¸€ä¸ªé¡¹ç›®æ ‡è®°ä¸ºâ€œå·²å¤„ç†â€çš„å”¯ä¸€å®˜æ–¹æ–¹æ³•ã€‚
        å®ƒä¼šåŒæ—¶æ›´æ–°æ•°æ®åº“å’Œå†…å­˜ç¼“å­˜ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§ã€‚
        """
        # 1. æ›´æ–°æ•°æ®åº“
        self.log_db_manager.save_to_processed_log(cursor, item_id, item_name, score=score)
        
        # 2. å®æ—¶æ›´æ–°å†…å­˜ç¼“å­˜
        self.processed_items_cache[item_id] = item_name
        
        logger.debug(f"  âœ å·²å°† '{item_name}' æ ‡è®°ä¸ºå·²å¤„ç† (æ•°æ®åº“ & å†…å­˜)ã€‚")
    # --- æ¸…é™¤å·²å¤„ç†è®°å½• ---
    def clear_processed_log(self):
        """
        ã€å·²æ”¹é€ ã€‘æ¸…é™¤æ•°æ®åº“å’Œå†…å­˜ä¸­çš„å·²å¤„ç†è®°å½•ã€‚
        ä½¿ç”¨ä¸­å¤®æ•°æ®åº“è¿æ¥å‡½æ•°ã€‚
        """
        try:
            # 1. â˜…â˜…â˜… è°ƒç”¨ä¸­å¤®å‡½æ•° â˜…â˜…â˜…
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                
                logger.debug("æ­£åœ¨ä»æ•°æ®åº“åˆ é™¤ processed_log è¡¨ä¸­çš„æ‰€æœ‰è®°å½•...")
                cursor.execute("DELETE FROM processed_log")
                # with è¯­å¥ä¼šè‡ªåŠ¨å¤„ç† conn.commit()
            
            logger.info("  âœ æ•°æ®åº“ä¸­çš„å·²å¤„ç†è®°å½•å·²æ¸…é™¤ã€‚")

            # 2. æ¸…ç©ºå†…å­˜ç¼“å­˜
            self.processed_items_cache.clear()
            logger.info("  âœ å†…å­˜ä¸­çš„å·²å¤„ç†è®°å½•ç¼“å­˜å·²æ¸…é™¤ã€‚")

        except Exception as e:
            logger.error(f"æ¸…é™¤æ•°æ®åº“æˆ–å†…å­˜å·²å¤„ç†è®°å½•æ—¶å¤±è´¥: {e}", exc_info=True)
            # 3. â˜…â˜…â˜… é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œé€šçŸ¥ä¸Šæ¸¸è°ƒç”¨è€…æ“ä½œå¤±è´¥ â˜…â˜…â˜…
            raise
    
    # â˜…â˜…â˜… å…¬å¼€çš„ã€ç‹¬ç«‹çš„è¿½å‰§åˆ¤æ–­æ–¹æ³• â˜…â˜…â˜…
    def check_and_add_to_watchlist(self, item_details: Dict[str, Any]):
        """
        æ£€æŸ¥ä¸€ä¸ªåª’ä½“é¡¹ç›®æ˜¯å¦ä¸ºå‰§é›†ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ‰§è¡Œæ™ºèƒ½è¿½å‰§åˆ¤æ–­å¹¶æ·»åŠ åˆ°å¾…çœ‹åˆ—è¡¨ã€‚
        æ­¤æ–¹æ³•è¢«è®¾è®¡ä¸ºç”±å¤–éƒ¨äº‹ä»¶ï¼ˆå¦‚Webhookï¼‰æ˜¾å¼è°ƒç”¨ã€‚
        """
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_details.get('Id')})")
        
        if item_details.get("Type") != "Series":
            # å¦‚æœä¸æ˜¯å‰§é›†ï¼Œç›´æ¥è¿”å›ï¼Œä¸æ‰“å°éå¿…è¦çš„æ—¥å¿—
            return

        logger.info(f"  âœ å¼€å§‹ä¸ºæ–°å…¥åº“å‰§é›† '{item_name_for_log}' è¿›è¡Œè¿½å‰§çŠ¶æ€åˆ¤æ–­...")
        try:
            # å®ä¾‹åŒ– WatchlistProcessor å¹¶æ‰§è¡Œæ·»åŠ æ“ä½œ
            watchlist_proc = WatchlistProcessor(self.config)
            watchlist_proc.add_series_to_watchlist(item_details)
        except Exception as e_watchlist:
            logger.error(f"  âœ åœ¨è‡ªåŠ¨æ·»åŠ  '{item_name_for_log}' åˆ°è¿½å‰§åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e_watchlist}", exc_info=True)

    def signal_stop(self):
        self._stop_event.set()

    def clear_stop_signal(self):
        self._stop_event.clear()

    def get_stop_event(self) -> threading.Event:
        """è¿”å›å†…éƒ¨çš„åœæ­¢äº‹ä»¶å¯¹è±¡ï¼Œä»¥ä¾¿ä¼ é€’ç»™å…¶ä»–å‡½æ•°ã€‚"""
        return self._stop_event

    def is_stop_requested(self) -> bool:
        return self._stop_event.is_set()

    def _load_processed_log_from_db(self) -> Dict[str, str]:
        log_dict = {}
        try:
            # 1. â˜…â˜…â˜… ä½¿ç”¨ with è¯­å¥å’Œä¸­å¤®å‡½æ•° â˜…â˜…â˜…
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                
                # 2. æ‰§è¡ŒæŸ¥è¯¢
                cursor.execute("SELECT item_id, item_name FROM processed_log")
                rows = cursor.fetchall()
                
                # 3. å¤„ç†ç»“æœ
                for row in rows:
                    if row['item_id'] and row['item_name']:
                        log_dict[row['item_id']] = row['item_name']
            
            # 4. with è¯­å¥ä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰äº‹æƒ…ï¼Œä»£ç å¹²å‡€åˆ©è½ï¼

        except Exception as e:
            # 5. â˜…â˜…â˜… è®°å½•æ›´è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯ â˜…â˜…â˜…
            logger.error(f"ä»æ•°æ®åº“è¯»å–å·²å¤„ç†è®°å½•å¤±è´¥: {e}", exc_info=True)
        return log_dict

    # âœ¨ ä» SyncHandler è¿ç§»å¹¶æ”¹é€ ï¼Œç”¨äºåœ¨æœ¬åœ°ç¼“å­˜ä¸­æŸ¥æ‰¾è±†ç“£JSONæ–‡ä»¶
    def _find_local_douban_json(self, imdb_id: Optional[str], douban_id: Optional[str], douban_cache_dir: str) -> Optional[str]:
        """æ ¹æ® IMDb ID æˆ– è±†ç“£ ID åœ¨æœ¬åœ°ç¼“å­˜ç›®å½•ä¸­æŸ¥æ‰¾å¯¹åº”çš„è±†ç“£JSONæ–‡ä»¶ã€‚"""
        if not os.path.exists(douban_cache_dir):
            return None
        
        # ä¼˜å…ˆä½¿ç”¨ IMDb ID åŒ¹é…ï¼Œæ›´å‡†ç¡®
        if imdb_id:
            for dirname in os.listdir(douban_cache_dir):
                if dirname.startswith('0_'): continue
                if imdb_id in dirname:
                    dir_path = os.path.join(douban_cache_dir, dirname)
                    for filename in os.listdir(dir_path):
                        if filename.endswith('.json'):
                            return os.path.join(dir_path, filename)
                            
        # å…¶æ¬¡ä½¿ç”¨è±†ç“£ ID åŒ¹é…
        if douban_id:
            for dirname in os.listdir(douban_cache_dir):
                if dirname.startswith(f"{douban_id}_"):
                    dir_path = os.path.join(douban_cache_dir, dirname)
                    for filename in os.listdir(dir_path):
                        if filename.endswith('.json'):
                            return os.path.join(dir_path, filename)
        return None

    # âœ¨ å°è£…äº†â€œä¼˜å…ˆæœ¬åœ°ç¼“å­˜ï¼Œå¤±è´¥åˆ™åœ¨çº¿è·å–â€çš„é€»è¾‘
    def _get_douban_data_with_local_cache(self, media_info: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], Optional[float]]:
        """
        ã€V3 - æœ€ç»ˆç‰ˆã€‘è·å–è±†ç“£æ•°æ®ï¼ˆæ¼”å‘˜+è¯„åˆ†ï¼‰ã€‚ä¼˜å…ˆæœ¬åœ°ç¼“å­˜ï¼Œå¤±è´¥åˆ™å›é€€åˆ°åŠŸèƒ½å®Œæ•´çš„åœ¨çº¿APIè·¯å¾„ã€‚
        è¿”å›: (æ¼”å‘˜åˆ—è¡¨, è±†ç“£è¯„åˆ†) çš„å…ƒç»„ã€‚
        """
        # 1. å‡†å¤‡æŸ¥æ‰¾æ‰€éœ€çš„ä¿¡æ¯
        provider_ids = media_info.get("ProviderIds", {})
        item_name = media_info.get("Name", "")
        imdb_id = provider_ids.get("Imdb")
        douban_id_from_provider = provider_ids.get("Douban")
        item_type = media_info.get("Type")
        item_year = str(media_info.get("ProductionYear", ""))

        # 2. å°è¯•ä»æœ¬åœ°ç¼“å­˜æŸ¥æ‰¾
        douban_cache_dir_name = "douban-movies" if item_type == "Movie" else "douban-tv"
        douban_cache_path = os.path.join(self.local_data_path, "cache", douban_cache_dir_name)
        local_json_path = self._find_local_douban_json(imdb_id, douban_id_from_provider, douban_cache_path)

        if local_json_path:
            logger.debug(f"  âœ å‘ç°æœ¬åœ°è±†ç“£ç¼“å­˜æ–‡ä»¶ï¼Œå°†ç›´æ¥ä½¿ç”¨: {local_json_path}")
            douban_data = _read_local_json(local_json_path)
            if douban_data:
                cast = douban_data.get('actors', [])
                rating_str = douban_data.get("rating", {}).get("value")
                rating_float = None
                if rating_str:
                    try: rating_float = float(rating_str)
                    except (ValueError, TypeError): pass
                return cast, rating_float
            else:
                logger.warning(f"æœ¬åœ°è±†ç“£ç¼“å­˜æ–‡ä»¶ '{local_json_path}' æ— æ•ˆï¼Œå°†å›é€€åˆ°åœ¨çº¿APIã€‚")
        
        # 3. å¦‚æœæœ¬åœ°æœªæ‰¾åˆ°ï¼Œå›é€€åˆ°åŠŸèƒ½å®Œæ•´çš„åœ¨çº¿APIè·¯å¾„
        logger.info("  âœ æœªæ‰¾åˆ°æœ¬åœ°è±†ç“£ç¼“å­˜ï¼Œå°†é€šè¿‡åœ¨çº¿APIè·å–æ¼”å‘˜å’Œè¯„åˆ†ä¿¡æ¯ã€‚")

        # 3.1 åŒ¹é…è±†ç“£IDå’Œç±»å‹ã€‚ç°åœ¨ match_info è¿”å›çš„ç»“æœæ˜¯å®Œå…¨å¯ä¿¡çš„ã€‚
        match_info_result = self.douban_api.match_info(
            name=item_name, imdbid=imdb_id, mtype=item_type, year=item_year
        )

        if match_info_result.get("error") or not match_info_result.get("id"):
            logger.warning(f"åœ¨çº¿åŒ¹é…è±†ç“£IDå¤±è´¥ for '{item_name}': {match_info_result.get('message', 'æœªæ‰¾åˆ°ID')}")
            return [], None

        douban_id = match_info_result["id"]
        # âœ¨âœ¨âœ¨ ç›´æ¥ä¿¡ä»»ä» douban.py è¿”å›çš„ç±»å‹ âœ¨âœ¨âœ¨
        douban_type = match_info_result.get("type")

        if not douban_type:
            logger.error(f"ä»è±†ç“£åŒ¹é…ç»“æœä¸­æœªèƒ½è·å–åˆ°åª’ä½“ç±»å‹ for ID {douban_id}ã€‚å¤„ç†ä¸­æ­¢ã€‚")
            return [], None

        # 3.2 è·å–æ¼”èŒå‘˜ (ä½¿ç”¨å®Œå…¨å¯ä¿¡çš„ç±»å‹)
        cast_data = self.douban_api.get_acting(
            name=item_name, 
            douban_id_override=douban_id, 
            mtype=douban_type
        )
        douban_cast_raw = cast_data.get("cast", [])

        return douban_cast_raw, None
    
    # --- é€šè¿‡TmdbIDæŸ¥æ‰¾æ˜ å°„è¡¨ ---
    def _find_person_in_map_by_tmdb_id(self, tmdb_id: str, cursor: psycopg2.extensions.cursor) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ® TMDB ID åœ¨ person_identity_map è¡¨ä¸­æŸ¥æ‰¾å¯¹åº”çš„è®°å½•ã€‚
        """
        if not tmdb_id:
            return None
        try:
            cursor.execute(
                "SELECT * FROM person_identity_map WHERE tmdb_person_id = %s",
                (tmdb_id,)
            )
            return cursor.fetchone()
        except psycopg2.Error as e:
            logger.error(f"é€šè¿‡ TMDB ID '{tmdb_id}' æŸ¥è¯¢ person_identity_map æ—¶å‡ºé”™: {e}")
            return None
    
    # --- é€šè¿‡ API æ›´æ–° Emby ä¸­æ¼”å‘˜åå­— ---
    def _update_emby_person_names_from_final_cast(self, final_cast: List[Dict[str, Any]], item_name_for_log: str):
        """
        æ ¹æ®æœ€ç»ˆå¤„ç†å¥½çš„æ¼”å‘˜åˆ—è¡¨ï¼Œé€šè¿‡ API æ›´æ–° Emby ä¸­â€œæ¼”å‘˜â€é¡¹ç›®çš„åå­—ã€‚
        """
        actors_to_update = [
            actor for actor in final_cast 
            if actor.get("emby_person_id") and utils.contains_chinese(actor.get("name"))
        ]

        if not actors_to_update:
            logger.info(f"  âœ æ— éœ€é€šè¿‡ API æ›´æ–°æ¼”å‘˜åå­— (æ²¡æœ‰æ‰¾åˆ°éœ€è¦ç¿»è¯‘çš„ Emby æ¼”å‘˜)ã€‚")
            return

        logger.info(f"  âœ å¼€å§‹ä¸ºã€Š{item_name_for_log}ã€‹çš„ {len(actors_to_update)} ä½æ¼”å‘˜é€šè¿‡ API æ›´æ–°åå­—...")
        
        # æ‰¹é‡è·å–è¿™äº›æ¼”å‘˜åœ¨ Emby ä¸­çš„å½“å‰ä¿¡æ¯ï¼Œä»¥å‡å°‘ API è¯·æ±‚
        person_ids = [actor["emby_person_id"] for actor in actors_to_update]
        current_person_details = emby.get_emby_items_by_id(
            base_url=self.emby_url,
            api_key=self.emby_api_key,
            user_id=self.emby_user_id,
            item_ids=person_ids,
            fields="Name"
        )
        
        current_names_map = {p["Id"]: p.get("Name") for p in current_person_details} if current_person_details else {}

        updated_count = 0
        for actor in actors_to_update:
            person_id = actor["emby_person_id"]
            new_name = actor["name"]
            current_name = current_names_map.get(person_id)

            # åªæœ‰å½“æ–°åå­—å’Œå½“å‰åå­—ä¸åŒæ—¶ï¼Œæ‰æ‰§è¡Œæ›´æ–°
            if new_name != current_name:
                emby.update_person_details(
                    person_id=person_id,
                    new_data={"Name": new_name},
                    emby_server_url=self.emby_url,
                    emby_api_key=self.emby_api_key,
                    user_id=self.emby_user_id
                )
                updated_count += 1
                # åŠ ä¸ªå°å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.2) 

        logger.info(f"  âœ æˆåŠŸé€šè¿‡ API æ›´æ–°äº† {updated_count} ä½æ¼”å‘˜çš„åå­—ã€‚")
    
    # --- å…¨é‡å¤„ç†çš„å…¥å£ ---
    def process_full_library(self, update_status_callback: Optional[callable] = None, force_full_update: bool = False):
        """
        è¿™æ˜¯æ‰€æœ‰å…¨é‡å¤„ç†çš„å”¯ä¸€å…¥å£ã€‚
        """
        self.clear_stop_signal()
        
        logger.trace(f"è¿›å…¥æ ¸å¿ƒæ‰§è¡Œå±‚: process_full_library, æ¥æ”¶åˆ°çš„ force_full_update = {force_full_update}")

        if force_full_update:
            logger.info("  âœ æ£€æµ‹åˆ°â€œæ·±åº¦æ›´æ–°â€æ¨¡å¼ï¼Œæ­£åœ¨æ¸…ç©ºå·²å¤„ç†æ—¥å¿—...")
            try:
                self.clear_processed_log()
            except Exception as e:
                logger.error(f"åœ¨ process_full_library ä¸­æ¸…ç©ºæ—¥å¿—å¤±è´¥: {e}", exc_info=True)
                if update_status_callback: update_status_callback(-1, "æ¸…ç©ºæ—¥å¿—å¤±è´¥")
                return

        libs_to_process_ids = self.config.get("libraries_to_process", [])
        if not libs_to_process_ids:
            logger.warning("  âœ æœªåœ¨é…ç½®ä¸­æŒ‡å®šè¦å¤„ç†çš„åª’ä½“åº“ã€‚")
            return

        logger.info("  âœ æ­£åœ¨å°è¯•ä»Embyè·å–åª’ä½“é¡¹ç›®...")
        all_emby_libraries = emby.get_emby_libraries(self.emby_url, self.emby_api_key, self.emby_user_id) or []
        library_name_map = {lib.get('Id'): lib.get('Name', 'æœªçŸ¥åº“å') for lib in all_emby_libraries}
        
        movies = emby.get_emby_library_items(self.emby_url, self.emby_api_key, "Movie", self.emby_user_id, libs_to_process_ids, library_name_map=library_name_map) or []
        series = emby.get_emby_library_items(self.emby_url, self.emby_api_key, "Series", self.emby_user_id, libs_to_process_ids, library_name_map=library_name_map) or []
        
        if movies:
            source_movie_lib_names = sorted(list({library_name_map.get(item.get('_SourceLibraryId')) for item in movies if item.get('_SourceLibraryId')}))
            logger.info(f"  âœ ä»åª’ä½“åº“ã€{', '.join(source_movie_lib_names)}ã€‘è·å–åˆ° {len(movies)} ä¸ªç”µå½±é¡¹ç›®ã€‚")

        if series:
            source_series_lib_names = sorted(list({library_name_map.get(item.get('_SourceLibraryId')) for item in series if item.get('_SourceLibraryId')}))
            logger.info(f"  âœ ä»åª’ä½“åº“ã€{', '.join(source_series_lib_names)}ã€‘è·å–åˆ° {len(series)} ä¸ªç”µè§†å‰§é¡¹ç›®ã€‚")

        all_items = movies + series
        total = len(all_items)
        
        if total == 0:
            logger.info("  âœ åœ¨æ‰€æœ‰é€‰å®šçš„åº“ä¸­æœªæ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„é¡¹ç›®ã€‚")
            if update_status_callback: update_status_callback(100, "æœªæ‰¾åˆ°å¯å¤„ç†çš„é¡¹ç›®ã€‚")
            return

        # --- æ–°å¢ï¼šæ¸…ç†å·²åˆ é™¤çš„åª’ä½“é¡¹ ---
        if update_status_callback: update_status_callback(20, "æ­£åœ¨æ£€æŸ¥å¹¶æ¸…ç†å·²åˆ é™¤çš„åª’ä½“é¡¹...")
        
        with get_central_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT item_id, item_name FROM processed_log")
            processed_log_entries = cursor.fetchall()
            
            processed_ids_in_db = {entry['item_id'] for entry in processed_log_entries}
            emby_ids_in_library = {item.get('Id') for item in all_items if item.get('Id')}
            
            # æ‰¾å‡ºåœ¨ processed_log ä¸­ä½†ä¸åœ¨ Emby åª’ä½“åº“ä¸­çš„é¡¹ç›®
            deleted_items_to_clean = processed_ids_in_db - emby_ids_in_library
            
            if deleted_items_to_clean:
                logger.info(f"  âœ å‘ç° {len(deleted_items_to_clean)} ä¸ªå·²ä» Emby åª’ä½“åº“åˆ é™¤çš„é¡¹ç›®ï¼Œæ­£åœ¨ä» 'å·²å¤„ç†' ä¸­ç§»é™¤...")
                for deleted_item_id in deleted_items_to_clean:
                    self.log_db_manager.remove_from_processed_log(cursor, deleted_item_id)
                    # åŒæ—¶ä»å†…å­˜ç¼“å­˜ä¸­ç§»é™¤
                    if deleted_item_id in self.processed_items_cache:
                        del self.processed_items_cache[deleted_item_id]
                    logger.debug(f"  âœ å·²ä» 'å·²å¤„ç†' ä¸­ç§»é™¤ ItemID: {deleted_item_id}")
                conn.commit()
                logger.info("  âœ å·²åˆ é™¤åª’ä½“é¡¹çš„æ¸…ç†å·¥ä½œå®Œæˆã€‚")
            else:
                logger.info("  âœ æœªå‘ç°éœ€è¦ä» 'å·²å¤„ç†' ä¸­æ¸…ç†çš„å·²åˆ é™¤åª’ä½“é¡¹ã€‚")
        
        if update_status_callback: update_status_callback(30, "å·²åˆ é™¤åª’ä½“é¡¹æ¸…ç†å®Œæˆï¼Œå¼€å§‹å¤„ç†ç°æœ‰åª’ä½“...")

        # --- ç°æœ‰åª’ä½“é¡¹å¤„ç†å¾ªç¯ ---
        for i, item in enumerate(all_items):
            if self.is_stop_requested():
                logger.warning("  ğŸš« å…¨åº“æ‰«æä»»åŠ¡å·²è¢«ç”¨æˆ·ä¸­æ­¢ã€‚")
                break # ä½¿ç”¨ break ä¼˜é›…åœ°é€€å‡ºå¾ªç¯
            
            item_id = item.get('Id')
            item_name = item.get('Name', f"ID:{item_id}")

            if not force_full_update and item_id in self.processed_items_cache:
                logger.info(f"  âœ æ­£åœ¨è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®: {item_name}")
                if update_status_callback:
                    # è°ƒæ•´è¿›åº¦æ¡çš„èµ·å§‹ç‚¹ï¼Œä½¿å…¶åœ¨æ¸…ç†åä» 30% å¼€å§‹
                    progress_after_cleanup = 30
                    current_progress = progress_after_cleanup + int(((i + 1) / total) * (100 - progress_after_cleanup))
                    update_status_callback(current_progress, f"è·³è¿‡: {item_name}")
                continue

            if update_status_callback:
                progress_after_cleanup = 30
                current_progress = progress_after_cleanup + int(((i + 1) / total) * (100 - progress_after_cleanup))
                update_status_callback(current_progress, f"å¤„ç†ä¸­ ({i+1}/{total}): {item_name}")
            
            self.process_single_item(
                item_id, 
                force_full_update=force_full_update
            )
            
            time_module.sleep(float(self.config.get("delay_between_items_sec", 0.5)))
        
        if not self.is_stop_requested() and update_status_callback:
            update_status_callback(100, "å…¨é‡å¤„ç†å®Œæˆ")
    
    # --- æ ¸å¿ƒå¤„ç†æ€»ç®¡ ---
    def process_single_item(self, emby_item_id: str, force_full_update: bool = False, specific_episode_ids: Optional[List[str]] = None):
        """
        ã€V-API-Ready æœ€ç»ˆç‰ˆ - å¸¦è·³è¿‡åŠŸèƒ½ã€‘
        å…¥å£å‡½æ•°ï¼Œå®ƒä¼šå…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®ã€‚
        """
        # 1. é™¤éå¼ºåˆ¶ï¼Œå¦åˆ™è·³è¿‡å·²å¤„ç†çš„
        if not force_full_update and not specific_episode_ids and emby_item_id in self.processed_items_cache:
            item_name_from_cache = self.processed_items_cache.get(emby_item_id, f"ID:{emby_item_id}")
            logger.info(f"åª’ä½“ '{item_name_from_cache}' è·³è¿‡å·²å¤„ç†è®°å½•ã€‚")
            return True

        # 2. æ£€æŸ¥åœæ­¢ä¿¡å·
        if self.is_stop_requested():
            return False

        # 3. è·å–Embyè¯¦æƒ…ï¼Œè¿™æ˜¯åç»­æ‰€æœ‰æ“ä½œçš„åŸºç¡€
        item_details = emby.get_emby_item_details(
            emby_item_id, self.emby_url, self.emby_api_key, self.emby_user_id
        )
        
        if not item_details:
            logger.error(f"process_single_item: æ— æ³•è·å– Emby é¡¹ç›® {emby_item_id} çš„è¯¦æƒ…ã€‚")
            return False
        
        # è¡¥å…¨ _SourceLibraryIdï¼šå› ä¸ºå•é¡¹è·å–æ¥å£ä¸åŒ…å«æ­¤å­—æ®µï¼Œéœ€é€šè¿‡è·¯å¾„åæŸ¥
        if not item_details.get('_SourceLibraryId'):
            lib_info = emby.get_library_root_for_item(
                item_id=emby_item_id,
                base_url=self.emby_url,
                api_key=self.emby_api_key,
                user_id=self.emby_user_id
            )
            if lib_info and lib_info.get('Id'):
                item_details['_SourceLibraryId'] = lib_info['Id']
                logger.debug(f"  âœ å·²ä¸º '{item_details.get('Name')}' è¡¥å…¨åª’ä½“åº“ID: {lib_info['Id']}")
            else:
                logger.warning(f"  âœ æ— æ³•ç¡®å®š '{item_details.get('Name')}' æ‰€å±çš„åª’ä½“åº“IDã€‚")

        # 4. å°†ä»»åŠ¡äº¤ç»™æ ¸å¿ƒå¤„ç†å‡½æ•°
        return self._process_item_core_logic(
            item_details_from_emby=item_details,
            force_full_update=force_full_update,
            specific_episode_ids=specific_episode_ids
        )

    # ---æ ¸å¿ƒå¤„ç†æµç¨‹ ---
    def _process_item_core_logic(self, item_details_from_emby: Dict[str, Any], force_full_update: bool = False, specific_episode_ids: Optional[List[str]] = None):
        """
        ã€V-Final-Architecture-Pro - â€œè®¾è®¡å¸ˆâ€æœ€ç»ˆç‰ˆ + è¯„åˆ†æœºåˆ¶ã€‘
        æœ¬å‡½æ•°ä½œä¸ºâ€œè®¾è®¡å¸ˆâ€ï¼Œåªè´Ÿè´£è®¡ç®—å’Œæ€è€ƒï¼Œäº§å‡ºâ€œè®¾è®¡å›¾â€å’Œâ€œç‰©æ–™æ¸…å•â€ï¼Œç„¶åå…¨æƒå§”æ‰˜ç»™æ–½å·¥é˜Ÿã€‚
        """
        # ======================================================================
        # é˜¶æ®µ 1: å‡†å¤‡å·¥ä½œ
        # ======================================================================
        item_id = item_details_from_emby.get("Id")
        item_name_for_log = item_details_from_emby.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        tmdb_id = item_details_from_emby.get("ProviderIds", {}).get("Tmdb")
        item_type = item_details_from_emby.get("Type")

        logger.trace(f"--- å¼€å§‹å¤„ç† '{item_name_for_log}' (TMDb ID: {tmdb_id}) ---")

        all_emby_people_for_count = item_details_from_emby.get("People", [])
        original_emby_actor_count = len([p for p in all_emby_people_for_count if p.get("Type") == "Actor"])

        if not tmdb_id:
            logger.error(f"  âœ '{item_name_for_log}' ç¼ºå°‘ TMDb IDï¼Œæ— æ³•å¤„ç†ã€‚")
            return False
        if not self.local_data_path:
            logger.error(f"  âœ '{item_name_for_log}' å¤„ç†å¤±è´¥ï¼šæœªåœ¨é…ç½®ä¸­è®¾ç½®â€œæœ¬åœ°æ•°æ®æºè·¯å¾„â€ã€‚")
            return False
        
        try:
            authoritative_cast_source = []
            tmdb_details_for_extra = None # ç”¨äºå†…éƒ¨ç¼“å­˜

            # =========================================================
            # â˜…â˜…â˜… æ­¥éª¤1:æ£€æŸ¥jsonæ˜¯å¦ç¼ºå¤± â˜…â˜…â˜…
            # =========================================================
            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            source_cache_dir = os.path.join(self.local_data_path, "cache", cache_folder_name, tmdb_id)
            main_json_filename = "all.json" if item_type == "Movie" else "series.json"
            source_json_path = os.path.join(source_cache_dir, main_json_filename)

            if not os.path.exists(source_json_path):
                logger.warning(f"  âœ æ ¸å¿ƒå¤„ç†å‰ç½®æ£€æŸ¥ï¼šæœ¬åœ°å…ƒæ•°æ®æ–‡ä»¶ '{source_json_path}' ä¸å­˜åœ¨ã€‚å¯åŠ¨å¤‡ç”¨æ–¹æ¡ˆ...")
                logger.info(f"  âœ æ­£åœ¨é€šçŸ¥ Emby ä¸º '{item_name_for_log}' åˆ·æ–°å…ƒæ•°æ®ä»¥ç”Ÿæˆç¼“å­˜æ–‡ä»¶...")
                
                emby.refresh_emby_item_metadata(
                    item_emby_id=item_id,
                    emby_server_url=self.emby_url,
                    emby_api_key=self.emby_api_key,
                    user_id_for_ops=self.emby_user_id,
                    replace_all_metadata_param=True,
                    item_name_for_log=item_name_for_log
                )

                # --- æ ¹æ®åª’ä½“ç±»å‹é€‰æ‹©ä¸åŒçš„ç­‰å¾…ç­–ç•¥ ---
                if item_type == "Series":
                    # ç”µè§†å‰§ï¼šæ™ºèƒ½ç­‰å¾…æ¨¡å¼
                    logger.info("  âœ æ£€æµ‹åˆ°ä¸ºç”µè§†å‰§ï¼Œå¯åŠ¨æ™ºèƒ½ç­‰å¾…æ¨¡å¼...")
                    total_wait_time = 0
                    idle_time = 0
                    last_file_count = 0
                    CHECK_INTERVAL = 10  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                    MAX_IDLE_TIME = 60   # è¿ç»­60ç§’æ²¡åŠ¨é™åˆ™è¶…æ—¶
                    MAX_TOTAL_WAIT_MINUTES = 15 # æ€»æœ€é•¿ç­‰å¾…æ—¶é—´15åˆ†é’Ÿ

                    while total_wait_time < MAX_TOTAL_WAIT_MINUTES * 60:
                        time_module.sleep(CHECK_INTERVAL)
                        total_wait_time += CHECK_INTERVAL

                        # æ£€æŸ¥ä¸»æ–‡ä»¶æ˜¯å¦å·²ç”Ÿæˆ
                        if os.path.exists(source_json_path):
                            logger.info(f"  âœ ä¸»æ–‡ä»¶ '{main_json_filename}' å·²ç”Ÿæˆï¼ç­‰å¾…ç»“æŸã€‚")
                            break
                        
                        # æ£€æŸ¥ç›®å½•å†…æ–‡ä»¶æ•°é‡å˜åŒ–
                        try:
                            current_file_count = len(os.listdir(source_cache_dir))
                        except FileNotFoundError:
                            current_file_count = 0

                        if current_file_count > last_file_count:
                            logger.info(f"  âœ ç¼“å­˜ç›®å½•æœ‰æ´»åŠ¨ï¼Œæ£€æµ‹åˆ° {current_file_count - last_file_count} ä¸ªæ–°æ–‡ä»¶ã€‚é‡ç½®ç©ºé—²è®¡æ—¶å™¨ã€‚")
                            idle_time = 0 # æœ‰æ–°æ–‡ä»¶ï¼Œé‡ç½®ç©ºé—²è®¡æ—¶
                            last_file_count = current_file_count
                        else:
                            idle_time += CHECK_INTERVAL
                            logger.info(f"  âœ ç¼“å­˜ç›®å½•æ— æ–°æ–‡ä»¶ï¼Œç©ºé—²æ—¶é—´ç´¯è®¡: {idle_time}/{MAX_IDLE_TIME}ç§’ã€‚")

                        if idle_time >= MAX_IDLE_TIME:
                            logger.warning(f"  âœ ç¼“å­˜ç›®å½•è¿ç»­ {MAX_IDLE_TIME} ç§’æ— æ´»åŠ¨ï¼Œåˆ¤å®šä»»åŠ¡å®Œæˆæˆ–è¶…æ—¶ã€‚")
                            break
                    else: # whileå¾ªç¯æ­£å¸¸ç»“æŸï¼ˆè¾¾åˆ°æ€»æ—¶é•¿ï¼‰
                        logger.warning(f"  âœ å·²è¾¾åˆ°æ€»æœ€é•¿ç­‰å¾…æ—¶é—´ {MAX_TOTAL_WAIT_MINUTES} åˆ†é’Ÿï¼Œåœæ­¢ç­‰å¾…ã€‚")

                else:
                    # ç”µå½±ï¼šç®€å•å®šæ—¶ç­‰å¾…
                    logger.info("  âœ æ£€æµ‹åˆ°ä¸ºç”µå½±ï¼Œå¯åŠ¨ç®€å•ç­‰å¾…æ¨¡å¼...")
                    for attempt in range(10):
                        logger.info(f"  âœ ç­‰å¾…3ç§’åæ£€æŸ¥æ–‡ä»¶... (ç¬¬ {attempt + 1}/10 æ¬¡å°è¯•)")
                        time_module.sleep(3)
                        if os.path.exists(source_json_path):
                            logger.info(f"  âœ æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆï¼")
                            break
            
            # åœ¨æ‰€æœ‰å°è¯•åï¼Œæœ€ç»ˆç¡®è®¤æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(source_json_path):
                logger.error(f"  âœ ç­‰å¾…è¶…æ—¶ï¼Œå…ƒæ•°æ®æ–‡ä»¶ä»æœªç”Ÿæˆã€‚æ— æ³•ç»§ç»­å¤„ç† '{item_name_for_log}'ï¼Œå·²è·³è¿‡ã€‚")
                return False

            # =========================================================
            # â˜…â˜…â˜… æ­¥éª¤ 2: ç¡®å®šå…ƒæ•°æ®éª¨æ¶ â˜…â˜…â˜…
            # =========================================================
            logger.info(f"  âœ æ­£åœ¨é¢„è¯»æœ¬åœ° Cache æ–‡ä»¶ä»¥æ„å»ºå…ƒæ•°æ®éª¨æ¶...")
            source_json_data = _read_local_json(source_json_path)
            
            if source_json_data:
                tmdb_details_for_extra = source_json_data
                # é»˜è®¤æ¼”å‘˜è¡¨ä¹Ÿæ¥è‡ªæœ¬åœ°ï¼ˆä¼šè¢«å¼ºåˆ¶æ›´æ–°è¦†ç›–ï¼‰
                authoritative_cast_source = (source_json_data.get("casts", {}) or source_json_data.get("credits", {})).get("cast", [])

                # =========================================================
                # â˜…â˜…â˜… å‰§é›†ä¸“å±è¡¥ä¸ï¼šå¦‚æœæœ¬åœ°ç¼“å­˜ç¼ºå¤±å…³é”®è¯ï¼Œå¼ºåˆ¶åœ¨çº¿è¡¥å…… â˜…â˜…â˜…
                # =========================================================
                if item_type == "Series":
                    # æ£€æŸ¥ keywords æ˜¯å¦ä¸ºç©ºï¼Œæˆ–è€…æ˜¯å¦ç¼ºå°‘ results (å‰§é›†å…³é”®è¯åœ¨ results é‡Œ)
                    current_kw = tmdb_details_for_extra.get('keywords')
                    is_kw_missing = False
                    
                    if not current_kw:
                        is_kw_missing = True
                    elif isinstance(current_kw, dict) and not current_kw.get('results'):
                        is_kw_missing = True
                    
                    if is_kw_missing and self.tmdb_api_key:
                        logger.info(f"  âœ æ£€æµ‹åˆ°å‰§é›†æœ¬åœ°ç¼“å­˜ç¼ºå¤±å…³é”®è¯ï¼Œæ­£åœ¨ä» TMDb API è¡¥å……...")
                        try:
                            # é‡æ–°è·å–å‰§é›†è¯¦æƒ… (å‡è®¾ get_tv_details åŒ…å« keywords æˆ– append_to_response)
                            fresh_data = tmdb.get_tv_details(tmdb_id, self.tmdb_api_key)
                            
                            kw_data = fresh_data.get('keywords') if fresh_data else None
                            
                            # åªæœ‰å½“å…³é”®è¯æ•°æ®æœ‰æ•ˆï¼ˆåŒ…å« results åˆ—è¡¨ä¸”ä¸ä¸ºç©ºï¼‰æ—¶æ‰æ‰§è¡Œæ›´æ–°
                            if kw_data and isinstance(kw_data, dict) and kw_data.get('results'):
                                # 1. å†…å­˜çƒ­ä¿®è¡¥ (ç¡®ä¿æ•°æ®åº“èƒ½å†™å…¥)
                                tmdb_details_for_extra['keywords'] = kw_data
                                logger.info(f"  âœ [API] è·å–åˆ° {len(kw_data['results'])} ä¸ªå…³é”®è¯ï¼Œå‡†å¤‡åŒæ­¥åˆ°æœ¬åœ°æ–‡ä»¶...")

                                # =================================================
                                # â˜…â˜…â˜… åŠ¨ä½œ A: å¼ºåˆ¶ä¿®å¤æºç¼“å­˜æ–‡ä»¶ (Source Cache) â˜…â˜…â˜…
                                # =================================================
                                try:
                                    # source_json_path åœ¨ä¸Šæ–‡å·²å®šä¹‰ï¼Œç›´æ¥ä½¿ç”¨
                                    if os.path.exists(source_json_path):
                                        # å…ˆè¯»
                                        with open(source_json_path, 'r', encoding='utf-8') as f:
                                            src_data = json.load(f)
                                        
                                        # ä¿®æ”¹
                                        src_data['keywords'] = kw_data
                                        
                                        # åå†™
                                        with open(source_json_path, 'w', encoding='utf-8') as f:
                                            json.dump(src_data, f, ensure_ascii=False, indent=2)
                                        logger.info(f"  âœ [Source] å·²ä¿®å¤æºç¼“å­˜æ–‡ä»¶: {source_json_path}")
                                    else:
                                        logger.warning(f"  âœ [Source] æºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•ä¿®å¤: {source_json_path}")
                                except Exception as e_src:
                                    logger.warning(f"  âœ [Source] ä¿®å¤æºç¼“å­˜æ–‡ä»¶å¤±è´¥: {e_src}")

                                # =================================================
                                # â˜…â˜…â˜… åŠ¨ä½œ B: åŒæ­¥è¦†ç›–ç¼“å­˜æ–‡ä»¶ (Override) â˜…â˜…â˜…
                                # =================================================
                                try:
                                    target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
                                    override_json_path = os.path.join(target_override_dir, main_json_filename)
                                    
                                    # ä»…å½“è¦†ç›–æ–‡ä»¶å·²å­˜åœ¨æ—¶æ‰æ›´æ–° (ä¸åˆ›å»ºæ–°æ–‡ä»¶)
                                    if os.path.exists(override_json_path):
                                        with open(override_json_path, 'r', encoding='utf-8') as f:
                                            existing_data = json.load(f)
                                        
                                        existing_data['keywords'] = kw_data
                                        
                                        with open(override_json_path, 'w', encoding='utf-8') as f:
                                            json.dump(existing_data, f, ensure_ascii=False, indent=2)
                                            
                                        logger.info(f"  âœ [Override] å·²åŒæ­¥å…³é”®è¯åˆ°è¦†ç›–ç¼“å­˜: {override_json_path}")
                                    else:
                                        # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡
                                        pass
                                        
                                except Exception as e_ovr:
                                    logger.warning(f"  âœ [Override] åŒæ­¥è¦†ç›–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e_ovr}")

                            else:
                                logger.warning(f"  âœ TMDb API è¿”å›äº†æ•°æ®ï¼Œä½†å…³é”®è¯åˆ—è¡¨ä¸ºç©º (å¯èƒ½è¯¥å‰§é›†ç¡®å®æ²¡æœ‰å…³é”®è¯)ã€‚")
                        except Exception as e_kw:
                            logger.warning(f"  âœ å°è¯•è¡¥å……å‰§é›†å…³é”®è¯å¤±è´¥: {e_kw}")

                # â˜…â˜…â˜… å…³é”®ä¿®å¤ï¼šå¦‚æœæ˜¯å‰§é›†ï¼Œå¿…é¡»åœ¨æ­¤å¤„èšåˆåˆ†é›†å’Œå­£æ•°æ® â˜…â˜…â˜…
                # è¿™æ ·ä¿è¯äº† tmdb_details_for_extra é‡Œçš„ seasons_details æ°¸è¿œæ˜¯å­—å…¸åˆ—è¡¨ï¼Œé˜²æ­¢ int æŠ¥é”™
                if item_type == "Series":
                    logger.info("  âœ æ£€æµ‹åˆ°å‰§é›†ï¼Œæ­£åœ¨èšåˆæœ¬åœ°åˆ†é›†å…ƒæ•°æ®...")
                    episodes_details_map = {}
                    seasons_details_list = []
                    try:
                        # æ‰«æç›®å½•èšåˆ season-X.json å’Œ season-X-episode-Y.json
                        for fname in os.listdir(source_cache_dir):
                            full_path = os.path.join(source_cache_dir, fname)
                            if fname.startswith("season-") and fname.endswith(".json"):
                                data = _read_local_json(full_path)
                                if data:
                                    if "-episode-" in fname: # åˆ†é›†
                                        key = f"S{data.get('season_number')}E{data.get('episode_number')}"
                                        episodes_details_map[key] = data
                                    else: # å­£
                                        seasons_details_list.append(data)
                        
                        # å°†èšåˆå¥½çš„æ•°æ®å¡å›éª¨æ¶
                        if episodes_details_map: tmdb_details_for_extra['episodes_details'] = episodes_details_map
                        if seasons_details_list: 
                            seasons_details_list.sort(key=lambda x: x.get('season_number', 0))
                            tmdb_details_for_extra['seasons_details'] = seasons_details_list
                    except Exception as e_agg:
                        logger.warning(f"  âœ èšåˆæœ¬åœ°åˆ†é›†æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e_agg}")
            else:
                # å¦‚æœè¿æœ¬åœ°æ–‡ä»¶éƒ½æ²¡æœ‰ï¼Œé‚£å°±çœŸçš„æ²¡æ³•å¼„äº†
                logger.error(f"  âœ ä¸¥é‡é”™è¯¯ï¼šæ‰¾ä¸åˆ°æœ¬åœ°å…ƒæ•°æ®æ–‡ä»¶ '{source_json_path}'ï¼Œæ— æ³•è¿›è¡Œå¤„ç†ã€‚")
                return False

            # å¦‚æœæ˜¯å¼ºåˆ¶æ›´æ–°ï¼Œä» API è·å–æœ€æ–°æ¼”å‘˜è¡¨æ¥æ›¿æ¢ä¸Šé¢çš„é»˜è®¤æ¼”å‘˜è¡¨
            if force_full_update and self.tmdb_api_key:
                logger.info(f"  âœ [æ·±åº¦æ›´æ–°] æ­£åœ¨ä» TMDb API æ‹‰å–å…¨é‡å…ƒæ•°æ® ...")
                
                if item_type == "Movie":
                    # è·å–ç”µå½±å…¨é‡è¯¦æƒ… (å‡è®¾ get_movie_details å†…éƒ¨å·²åŒ…å« append_to_response=credits,release_dates,keywords)
                    fresh_data = tmdb.get_movie_details(tmdb_id, self.tmdb_api_key)
                    if fresh_data:
                        # 1. â˜…â˜…â˜… æ ¸å¿ƒï¼šå…¨é‡è¦†ç›–éª¨æ¶ â˜…â˜…â˜…
                        # è¿™å°†åˆ·æ–° credits(å¯¼æ¼”), release_dates(åˆ†çº§), production_companies(å·¥ä½œå®¤), genres, keywords ç­‰
                        tmdb_details_for_extra.update(fresh_data)
                        
                        # 2. åˆ·æ–°æ¼”å‘˜è¡¨æº
                        if fresh_data.get("credits", {}).get("cast"):
                            authoritative_cast_source = fresh_data["credits"]["cast"]
                        
                        # æ—¥å¿—è®°å½•å…³é”®ä¿¡æ¯æ•°é‡ï¼Œç¡®ä¿å­˜æ´»
                        crew_count = len(fresh_data.get('credits', {}).get('crew', []))
                        rating_data = fresh_data.get('release_dates', {}).get('results', [])
                        logger.info(f"  âœ æˆåŠŸåˆ·æ–°ç”µå½±å…ƒæ•°æ®: å¯¼æ¼”({crew_count}äºº), åˆ†çº§æ•°æ®({len(rating_data)}å›½), ç®€ä»‹ç­‰ã€‚")
                
                elif item_type == "Series":
                    # è·å–å‰§é›†å…¨é‡èšåˆæ•°æ®
                    aggregated_tmdb_data = tmdb.aggregate_full_series_data_from_tmdb(int(tmdb_id), self.tmdb_api_key)
                    if aggregated_tmdb_data:
                        series_details = aggregated_tmdb_data.get("series_details", {})
                        
                        # 1. â˜…â˜…â˜… æ ¸å¿ƒï¼šå…¨é‡è¦†ç›–éª¨æ¶ (å‰§é›†å±‚) â˜…â˜…â˜…
                        # è¿™å°†åˆ·æ–° created_by(ä¸»åˆ›), content_ratings(åˆ†çº§), networks(å·¥ä½œå®¤), keywords ç­‰
                        tmdb_details_for_extra.update(series_details)
                        
                        # 2. åˆ·æ–°æ¼”å‘˜è¡¨æº (èšåˆæ‰€æœ‰åˆ†é›†)
                        all_episodes = list(aggregated_tmdb_data.get("episodes_details", {}).values())
                        authoritative_cast_source = _aggregate_series_cast_from_tmdb_data(series_details, all_episodes)
                        
                        # æ—¥å¿—
                        creators_count = len(series_details.get('created_by', []))
                        ratings_count = len(series_details.get('content_ratings', {}).get('results', []))
                        logger.info(f"  âœ æˆåŠŸåˆ·æ–°å‰§é›†å…ƒæ•°æ®: ä¸»åˆ›({creators_count}äºº), åˆ†çº§æ•°æ®({ratings_count}å›½), èšåˆæ¼”å‘˜({len(authoritative_cast_source)}äºº)ã€‚")
                
            # =========================================================
            # â˜…â˜…â˜… æ­¥éª¤ 3: ç§»é™¤æ— å¤´åƒæ¼”å‘˜ â˜…â˜…â˜…
            # =========================================================
            if self.config.get(constants.CONFIG_OPTION_REMOVE_ACTORS_WITHOUT_AVATARS, True) and authoritative_cast_source:
                original_count = len(authoritative_cast_source)
                
                # ä½¿ç”¨ 'profile_path' ä½œä¸ºåˆ¤æ–­ä¾æ®
                actors_with_avatars = [
                    actor for actor in authoritative_cast_source if actor.get("profile_path")
                ]
                
                if len(actors_with_avatars) < original_count:
                    removed_count = original_count - len(actors_with_avatars)
                    logger.info(f"  âœ åœ¨æ ¸å¿ƒå¤„ç†å‰ï¼Œå·²ä»æºæ•°æ®ä¸­ç§»é™¤ {removed_count} ä½æ— å¤´åƒçš„æ¼”å‘˜ã€‚")
                    # ç”¨ç­›é€‰åçš„åˆ—è¡¨è¦†ç›–åŸå§‹åˆ—è¡¨
                    authoritative_cast_source = actors_with_avatars
                else:
                    logger.debug("  âœ (é¢„æ£€æŸ¥) æ‰€æœ‰æºæ•°æ®ä¸­çš„æ¼”å‘˜å‡æœ‰å¤´åƒï¼Œæ— éœ€é¢„å…ˆç§»é™¤ã€‚")
                
            # =========================================================
            # â˜…â˜…â˜… æ­¥éª¤ 4:  æ•°æ®æ¥æº â˜…â˜…â˜…
            # =========================================================
            final_processed_cast = None
            cache_row = None 
            # 1.å¿«é€Ÿæ¨¡å¼
            if not force_full_update:
                # --- è·¯å¾„å‡†å¤‡ ---
                cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
                target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
                main_json_filename = "all.json" if item_type == "Movie" else "series.json"
                override_json_path = os.path.join(target_override_dir, main_json_filename)
                
                # --- ç­–ç•¥ A: ä¼˜å…ˆå°è¯•åŠ è½½æœ¬åœ° Override æ–‡ä»¶ (åå“ºæ¨¡å¼) ---
                # é€»è¾‘ï¼šå¦‚æœæœ¬åœ°æ–‡ä»¶å­˜åœ¨ï¼Œå®ƒå°±æ˜¯â€œçœŸç†â€ã€‚æ— è®ºæ•°æ®åº“é‡Œæœ‰æ²¡æœ‰ï¼Œéƒ½ä»¥æ–‡ä»¶ä¸ºå‡†ã€‚
                # ä¼˜åŠ¿ï¼š1. ç¡®ä¿æ‰‹åŠ¨ä¿®æ”¹ç”Ÿæ•ˆ 2. æ ‡è®°ä¸º'override_file'æºï¼Œåç»­å¯è·³è¿‡å†—ä½™å†™å…¥ï¼Œæ€§èƒ½æœ€é«˜ã€‚
                if os.path.exists(override_json_path):
                    logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] å‘ç°æœ¬åœ°è¦†ç›–æ–‡ä»¶ï¼Œä¼˜å…ˆåŠ è½½: {override_json_path}")
                    try:
                        override_data = _read_local_json(override_json_path)
                        if override_data:
                            cast_data = (override_data.get('casts', {}) or override_data.get('credits', {})).get('cast', [])
                            if cast_data:
                                logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] æˆåŠŸä»æ–‡ä»¶åŠ è½½ {len(cast_data)} ä½æ¼”å‘˜ï¼Œå°†æ¿€æ´»åå“ºæ•°æ®åº“...")
                                final_processed_cast = cast_data
                                
                                # å…³é”®è®¾ç½® 1: ä»¥æ­¤ä¸ºæºæ›´æ–°æ•°æ®åº“
                                tmdb_details_for_extra = override_data 
                                
                                # =========================================================
                                # â˜…â˜…â˜… å¡«è¡¥ç›²åŒºï¼šå¦‚æœæ˜¯å‰§é›†ï¼Œå¿…é¡»æŠŠåˆ†é›†æ–‡ä»¶ä¹Ÿè¯»è¿›æ¥ï¼ â˜…â˜…â˜…
                                # =========================================================
                                if item_type == "Series":
                                    logger.info("  âœ [å¿«é€Ÿæ¨¡å¼] æ£€æµ‹åˆ°å‰§é›†ï¼Œæ­£åœ¨èšåˆæœ¬åœ°åˆ†é›†å…ƒæ•°æ®ä»¥æ¢å¤æ•°æ®åº“è®°å½•...")
                                    episodes_details_map = {}
                                    seasons_details_list = [] 
                                    
                                    try:
                                        # 1. å…ˆè¯» Override (æ—§çš„/æ‰‹åŠ¨ä¿®æ”¹è¿‡çš„)
                                        if os.path.exists(target_override_dir):
                                            for fname in os.listdir(target_override_dir):
                                                full_path = os.path.join(target_override_dir, fname)
                                                if fname.startswith("season-") and fname.endswith(".json"):
                                                    data = _read_local_json(full_path)
                                                    if data:
                                                        if "-episode-" in fname:
                                                            key = f"S{data.get('season_number')}E{data.get('episode_number')}"
                                                            episodes_details_map[key] = data
                                                        else:
                                                            seasons_details_list.append(data)
                                        
                                        # 2. å†è¯» Source (æ–°çš„)ï¼Œè¡¥å…¨ Override é‡Œæ²¡æœ‰çš„
                                        if os.path.exists(source_cache_dir):
                                            recovered_count = 0
                                            for fname in os.listdir(source_cache_dir):
                                                if fname.startswith("season-") and fname.endswith(".json") and "-episode-" in fname:
                                                    try:
                                                        parts = fname.replace(".json", "").split("-")
                                                        s_num = int(parts[1])
                                                        e_num = int(parts[3])
                                                        key = f"S{s_num}E{e_num}"
                                                        
                                                        # â˜… å…³é”®ï¼šå¦‚æœ Override é‡Œæ²¡æœ‰ï¼Œå°±ä» Source æ‹¿ â˜…
                                                        if key not in episodes_details_map:
                                                            source_file = os.path.join(source_cache_dir, fname)
                                                            ep_data = _read_local_json(source_file)
                                                            if ep_data:
                                                                episodes_details_map[key] = ep_data
                                                                recovered_count += 1
                                                    except: continue
                                            
                                            if recovered_count > 0:
                                                logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] æˆåŠŸä»æºç¼“å­˜è¡¥å…¨äº† {recovered_count} ä¸ªæ–°åˆ†é›†çš„æ•°æ®ã€‚")

                                        # 3. å¡å›éª¨æ¶
                                        if episodes_details_map:
                                            tmdb_details_for_extra['episodes_details'] = episodes_details_map
                                            logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] æœ€ç»ˆèšåˆäº† {len(episodes_details_map)} ä¸ªåˆ†é›†çš„å…ƒæ•°æ®ã€‚")
                                        if seasons_details_list:
                                            seasons_details_list.sort(key=lambda x: x.get('season_number', 0))
                                            tmdb_details_for_extra['seasons_details'] = seasons_details_list

                                    except Exception as e_ep:
                                        logger.warning(f"  âœ [å¿«é€Ÿæ¨¡å¼] èšåˆåˆ†é›†/å­£æ•°æ®æ—¶å‘ç”Ÿå°é”™è¯¯: {e_ep}")

                                # å…³é”®è®¾ç½® 2: æ ‡è®°æºä¸ºæ–‡ä»¶
                                cache_row = {'source': 'override_file'} 

                                # è¡¥å……ï¼šç®€å•çš„ ID æ˜ å°„
                                tmdb_to_emby_map = {}
                                for person in item_details_from_emby.get("People", []):
                                    pid = (person.get("ProviderIds") or {}).get("Tmdb")
                                    if pid: tmdb_to_emby_map[str(pid)] = person.get("Id")
                                for actor in final_processed_cast:
                                    aid = str(actor.get('id'))
                                    if aid in tmdb_to_emby_map:
                                        actor['emby_person_id'] = tmdb_to_emby_map[aid]
                    except Exception as e:
                        logger.warning(f"  âœ è¯»å–è¦†ç›–æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†å°è¯•æ•°æ®åº“ç¼“å­˜ã€‚")

                # --- ç­–ç•¥ B: å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åŠ è½½æ•°æ®åº“ç¼“å­˜ (è‡ªåŠ¨å¤‡ä»½æ¨¡å¼) ---
                # é€»è¾‘ï¼šæ–‡ä»¶æ²¡äº†ï¼Œä½†æ•°æ®åº“é‡Œæœ‰ã€‚è¯»å–æ•°æ®åº“ï¼Œå¹¶åœ¨åç»­é˜¶æ®µè‡ªåŠ¨é‡æ–°ç”Ÿæˆæ–‡ä»¶ã€‚
                if final_processed_cast is None:
                    logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] æœ¬åœ°æ–‡ä»¶æœªå‘½ä¸­ï¼Œå°è¯•åŠ è½½æ•°æ®åº“ç¼“å­˜...")
                    try:
                        with get_central_db_connection() as conn:
                            cursor = conn.cursor()
                            cursor.execute("""
                                SELECT actors_json 
                                FROM media_metadata 
                                WHERE tmdb_id = %s AND item_type = %s
                                  AND actors_json IS NOT NULL AND actors_json::text != '[]'
                            """, (tmdb_id, item_type))
                            db_row = cursor.fetchone()

                            if db_row:
                                logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] æˆåŠŸå‘½ä¸­æ•°æ®åº“ç¼“å­˜ï¼")
                                slim_actors_from_cache = db_row["actors_json"]
                                final_processed_cast = self.actor_db_manager.rehydrate_slim_actors(cursor, slim_actors_from_cache)
                                cache_row = db_row 
                    except Exception as e_cache:
                        logger.warning(f"  âœ åŠ è½½æ•°æ®åº“ç¼“å­˜å¤±è´¥: {e_cache}ã€‚")

            # 2.å®Œæ•´æ¨¡å¼
            if final_processed_cast is None:
                logger.info(f"  âœ æœªå‘½ä¸­ç¼“å­˜æˆ–å¼ºåˆ¶é‡å¤„ç†ï¼Œå¼€å§‹å¤„ç†æ¼”å‘˜è¡¨...")

                with get_central_db_connection() as conn:
                    cursor = conn.cursor()
                    
                    all_emby_people = item_details_from_emby.get("People", [])
                    current_emby_cast_raw = [p for p in all_emby_people if p.get("Type") == "Actor"]
                    emby_config = {"url": self.emby_url, "api_key": self.emby_api_key, "user_id": self.emby_user_id}
                    enriched_emby_cast = self.actor_db_manager.enrich_actors_with_provider_ids(cursor, current_emby_cast_raw, emby_config)
                    douban_cast_raw, _ = self._get_douban_data_with_local_cache(item_details_from_emby)

                    # è°ƒç”¨æ ¸å¿ƒå¤„ç†å™¨å¤„ç†æ¼”å‘˜è¡¨
                    final_processed_cast = self._process_cast_list(
                        tmdb_cast_people=authoritative_cast_source,
                        emby_cast_people=enriched_emby_cast,
                        douban_cast_list=douban_cast_raw,
                        item_details_from_emby=item_details_from_emby,
                        cursor=cursor,
                        tmdb_api_key=self.tmdb_api_key,
                        stop_event=self.get_stop_event()
                    )

            # =========================================================
            # â˜…â˜…â˜… æ­¥éª¤ 5: ç»Ÿä¸€çš„æ”¶å°¾æµç¨‹ â˜…â˜…â˜…
            # =========================================================
            if final_processed_cast is None:
                raise ValueError("æœªèƒ½ç”Ÿæˆæœ‰æ•ˆçš„æœ€ç»ˆæ¼”å‘˜åˆ—è¡¨ã€‚")

            with get_central_db_connection() as conn:
                cursor = conn.cursor()

                is_feedback_mode = (
                    cache_row 
                    and isinstance(cache_row, dict) 
                    and cache_row.get('source') == 'override_file'
                    and not specific_episode_ids  # <--- å…³é”®ï¼šå¦‚æœæœ‰æŒ‡å®šåˆ†é›†(è¿½æ›´)ï¼Œåˆ™å¿…é¡»ä¸º False
                )

                if is_feedback_mode:
                    # --- åˆ†æ”¯ A: çº¯è¯»å–æ¨¡å¼ (æé€Ÿæ¢å¤) ---
                    logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] æ£€æµ‹åˆ°å®Œç¾æœ¬åœ°æ•°æ®ï¼Œè·³è¿‡å›¾ç‰‡ä¸‹è½½ã€æ–‡ä»¶å†™å…¥åŠ Emby åˆ·æ–°ã€‚")
                
                else:
                    # --- åˆ†æ”¯ B: æ­£å¸¸å¤„ç†/è¿½æ›´æ¨¡å¼ ---
                    # å†™å…¥ override æ–‡ä»¶
                    # æ³¨æ„ï¼šsync_single_item_assets å†…éƒ¨å·²ç»æœ‰é’ˆå¯¹ episode_ids_to_sync çš„ä¼˜åŒ–ï¼Œ
                    # å®ƒåªä¼šä¸‹è½½æ–°åˆ†é›†çš„å›¾ç‰‡ï¼Œå¹¶å¤åˆ¶æ–°åˆ†é›†çš„ JSONï¼Œä¸ä¼šé‡æ–°ä¸‹è½½å…¨å¥—å›¾ç‰‡ã€‚
                    self.sync_single_item_assets(
                        item_id=item_id,
                        update_description="ä¸»æµç¨‹å¤„ç†å®Œæˆ" if not specific_episode_ids else f"è¿½æ›´: {len(specific_episode_ids)}ä¸ªåˆ†é›†",
                        final_cast_override=final_processed_cast,
                        episode_ids_to_sync=specific_episode_ids 
                    )

                    # é€šè¿‡ API å®æ—¶æ›´æ–° Emby æ¼”å‘˜åº“ä¸­çš„åå­—
                    self._update_emby_person_names_from_final_cast(final_processed_cast, item_name_for_log)

                    # é€šçŸ¥ Emby åˆ·æ–°
                    logger.info(f"  âœ å¤„ç†å®Œæˆï¼Œæ­£åœ¨é€šçŸ¥ Emby åˆ·æ–°...")
                    emby.refresh_emby_item_metadata(
                        item_emby_id=item_id,
                        emby_server_url=self.emby_url,
                        emby_api_key=self.emby_api_key,
                        user_id_for_ops=self.emby_user_id,
                        replace_all_metadata_param=True, 
                        item_name_for_log=item_name_for_log
                    )

                # æ›´æ–°æˆ‘ä»¬è‡ªå·±çš„æ•°æ®åº“ç¼“å­˜ (è¿™æ˜¯åå“ºæ¨¡å¼çš„æ ¸å¿ƒç›®çš„ï¼Œå¿…é¡»æ‰§è¡Œ)
                self._upsert_media_metadata(
                    cursor=cursor,
                    item_type=item_type,
                    item_details_from_emby=item_details_from_emby,
                    final_processed_cast=final_processed_cast,
                    source_data_package=tmdb_details_for_extra
                )
                
                # ç»¼åˆè´¨æ£€ (è§†é¢‘æµæ£€æŸ¥ + æ¼”å‘˜åŒ¹é…åº¦è¯„åˆ†)
                logger.info(f"  âœ æ­£åœ¨è¯„ä¼°ã€Š{item_name_for_log}ã€‹çš„å¤„ç†è´¨é‡...")
                
                # --- 1. è§†é¢‘æµæ•°æ®å®Œæ•´æ€§æ£€æŸ¥ (ä»…é’ˆå¯¹ Movie å’Œ Episode) ---
                stream_check_passed = True
                stream_fail_reason = ""
                
                if item_type in ['Movie', 'Episode']:
                    has_valid_video = False
                    media_sources = item_details_from_emby.get("MediaSources", [])
                    if media_sources:
                        for source in media_sources:
                            for stream in source.get("MediaStreams", []):
                                # åªè¦å‘ç°ä¸€ä¸ªç±»å‹ä¸º Video çš„æµï¼Œå°±è®¤ä¸ºé€šè¿‡
                                if stream.get("Type") == "Video":
                                    has_valid_video = True
                                    break
                            if has_valid_video: break
                    
                    if not has_valid_video:
                        stream_check_passed = False
                        stream_fail_reason = "ç¼ºå¤±è§†é¢‘æµæ•°æ® (å¯èƒ½æ˜¯strmæ–‡ä»¶æœªæå–æˆ–åˆ†ææœªå®Œæˆ)"
                        logger.warning(f"  âœ [è´¨æ£€å¤±è´¥] ã€Š{item_name_for_log}ã€‹æœªæ£€æµ‹åˆ°è§†é¢‘æµã€‚")

                # æ¼”å‘˜å¤„ç†è´¨é‡è¯„åˆ†
                genres = item_details_from_emby.get("Genres", [])
                is_animation = "Animation" in genres or "åŠ¨ç”»" in genres or "Documentary" in genres or "çºªå½•" in genres
                
                # æ— è®ºæ•°æ®æ¥è‡ª API è¿˜æ˜¯ æœ¬åœ°ç¼“å­˜ï¼Œéƒ½å¿…é¡»æ¥å—è¯„åˆ†ç®—æ³•çš„æ£€éªŒã€‚
                processing_score = actor_utils.evaluate_cast_processing_quality(
                    final_cast=final_processed_cast, 
                    original_cast_count=original_emby_actor_count,
                    expected_final_count=len(final_processed_cast), 
                    is_animation=is_animation
                )

                if cache_row:
                    logger.info(f"  âœ [å¿«é€Ÿæ¨¡å¼] åŸºäºç¼“å­˜æ•°æ®çš„å®æ—¶å¤æ ¸è¯„åˆ†: {processing_score:.2f}")
                
                min_score_for_review = float(self.config.get("min_score_for_review", constants.DEFAULT_MIN_SCORE_FOR_REVIEW))
                
                # æœ€ç»ˆåˆ¤å®šä¸æ—¥å¿—å†™å…¥ ---
                # ä¼˜å…ˆçº§ï¼šè§†é¢‘æµç¼ºå¤± > è¯„åˆ†è¿‡ä½
                if not stream_check_passed:
                    # æƒ…å†µ A: è§†é¢‘æµç¼ºå¤± -> å¼ºåˆ¶å¾…å¤æ ¸
                    logger.warning(f"  âœ [è´¨æ£€]ã€Š{item_name_for_log}ã€‹å› ç¼ºå¤±è§†é¢‘æµæ•°æ®ï¼Œéœ€é‡æ–°å¤„ç†ã€‚")
                    self.log_db_manager.save_to_failed_log(cursor, item_id, item_name_for_log, stream_fail_reason, item_type, score=0.0)
                    # æ ‡è®°ä¸ºå·²å¤„ç†ï¼Œé˜²æ­¢é‡å¤å¾ªç¯ï¼Œä½†åœ¨UIä¸­ä¼šæ˜¾ç¤ºåœ¨â€œå¾…å¤æ ¸â€åˆ—è¡¨
                    self._mark_item_as_processed(cursor, item_id, item_name_for_log, score=0.0)
                    
                elif processing_score < min_score_for_review:
                    # æƒ…å†µ B: è¯„åˆ†è¿‡ä½ -> å¾…å¤æ ¸
                    reason = f"å¤„ç†è¯„åˆ† ({processing_score:.2f}) ä½äºé˜ˆå€¼ ({min_score_for_review})ã€‚"
                    
                    # â˜…â˜…â˜… ä¼˜åŒ–æ—¥å¿—ï¼šå¦‚æœæ˜¯å¿«é€Ÿæ¨¡å¼ä¸‹è¯„åˆ†ä½ï¼Œæç¤ºç”¨æˆ·å¯èƒ½ç¼“å­˜æœ‰é—®é¢˜ â˜…â˜…â˜…
                    if cache_row:
                        logger.warning(f"  âœ [è´¨æ£€]ã€Š{item_name_for_log}ã€‹æœ¬åœ°ç¼“å­˜æ•°æ®è´¨é‡ä¸ä½³ (è¯„åˆ†: {processing_score:.2f})ï¼Œå·²é‡æ–°æ ‡è®°ä¸ºã€å¾…å¤æ ¸ã€‘ã€‚")
                    else:
                        logger.warning(f"  âœ [è´¨æ£€]ã€Š{item_name_for_log}ã€‹å¤„ç†è´¨é‡ä¸ä½³ï¼Œå·²æ ‡è®°ä¸ºã€å¾…å¤æ ¸ã€‘ã€‚åŸå› : {reason}")
                        
                    self.log_db_manager.save_to_failed_log(cursor, item_id, item_name_for_log, reason, item_type, score=processing_score)
                    self._mark_item_as_processed(cursor, item_id, item_name_for_log, score=processing_score)
                    
                else:
                    # æƒ…å†µ C: ä¸€åˆ‡æ­£å¸¸ -> ç§»é™¤å¾…å¤æ ¸æ ‡è®°ï¼ˆå¦‚æœä¹‹å‰æœ‰ï¼‰
                    logger.info(f"  âœ ã€Š{item_name_for_log}ã€‹è´¨æ£€é€šè¿‡ (è¯„åˆ†: {processing_score:.2f})ï¼Œæ ‡è®°ä¸ºå·²å¤„ç†ã€‚")
                    self._mark_item_as_processed(cursor, item_id, item_name_for_log, score=processing_score)
                    self.log_db_manager.remove_from_failed_log(cursor, item_id)
                
                conn.commit()

            logger.trace(f"--- å¤„ç†å®Œæˆ '{item_name_for_log}' ---")

        except (ValueError, InterruptedError) as e:
            logger.warning(f"å¤„ç† '{item_name_for_log}' çš„è¿‡ç¨‹ä¸­æ–­: {e}")
            return False
        except Exception as outer_e:
            logger.error(f"æ ¸å¿ƒå¤„ç†æµç¨‹ä¸­å‘ç”ŸæœªçŸ¥ä¸¥é‡é”™è¯¯ for '{item_name_for_log}': {outer_e}", exc_info=True)
            try:
                with get_central_db_connection() as conn_fail:
                    self.log_db_manager.save_to_failed_log(conn_fail.cursor(), item_id, item_name_for_log, f"æ ¸å¿ƒå¤„ç†å¼‚å¸¸: {str(outer_e)}", item_type)
            except Exception as log_e:
                logger.error(f"å†™å…¥å¾…å¤æ ¸æ—¥å¿—æ—¶å†æ¬¡å‘ç”Ÿé”™è¯¯: {log_e}")
            return False

        logger.trace(f"  âœ… å¤„ç†å®Œæˆ '{item_name_for_log}'")
        return True

    # --- æ ¸å¿ƒå¤„ç†å™¨ ---
    def _process_cast_list(self, tmdb_cast_people: List[Dict[str, Any]],
                                    emby_cast_people: List[Dict[str, Any]],
                                    douban_cast_list: List[Dict[str, Any]],
                                    item_details_from_emby: Dict[str, Any],
                                    cursor: psycopg2.extensions.cursor,
                                    tmdb_api_key: Optional[str],
                                    stop_event: Optional[threading.Event]) -> List[Dict[str, Any]]:
        """
        ã€V-Final with Truncation - Full Codeã€‘
        - åœ¨æ­¥éª¤4çš„å¼€å¤´ï¼Œé‡æ–°åŠ å…¥äº†å¯¹æœ€ç»ˆæ¼”å‘˜åˆ—è¡¨è¿›è¡Œæˆªæ–­çš„é€»è¾‘ã€‚
        - ç¡®ä¿åœ¨è¿›è¡ŒAIç¿»è¯‘ç­‰è€—æ—¶æ“ä½œå‰ï¼Œå°†æ¼”å‘˜æ•°é‡é™åˆ¶åœ¨é…ç½®çš„ä¸Šé™å†…ã€‚
        """
        # --- åœ¨æ‰€æœ‰å¤„ç†å¼€å§‹å‰ï¼Œä»æºå¤´æ¸…æ´—åŒåå¼‚äººæ¼”å‘˜ ---
        logger.debug("  âœ é¢„å¤„ç†ï¼šæ¸…æ´—æºæ•°æ®ä¸­çš„åŒåæ¼”å‘˜ï¼Œåªä¿ç•™orderæœ€å°çš„ä¸€ä¸ªã€‚")
        cleaned_tmdb_cast = []
        seen_names = {} # ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨è§è¿‡çš„åå­—åŠå…¶order
        
        # é¦–å…ˆæŒ‰ order æ’åºï¼Œç¡®ä¿ç¬¬ä¸€ä¸ªé‡åˆ°çš„æ˜¯ order æœ€å°çš„
        tmdb_cast_people.sort(key=lambda x: x.get('order', 999))

        for actor in tmdb_cast_people:
            name = actor.get("name")
            if not name or not isinstance(name, str):
                continue
            
            cleaned_name = name.strip()
            
            if cleaned_name not in seen_names:
                cleaned_tmdb_cast.append(actor)
                seen_names[cleaned_name] = actor.get('order', 999)
            else:
                # è®°å½•è¢«ä¸¢å¼ƒçš„æ¼”å‘˜
                role = actor.get("character", "æœªçŸ¥è§’è‰²")
                logger.info(f"  âœ ä¸ºé¿å…å¼ å† ææˆ´ï¼Œåˆ é™¤åŒåå¼‚äººæ¼”å‘˜: '{cleaned_name}' (è§’è‰²: {role}, order: {actor.get('order', 999)})")

        # ä½¿ç”¨æ¸…æ´—åçš„åˆ—è¡¨è¿›è¡Œåç»­æ‰€æœ‰æ“ä½œ
        tmdb_cast_people = cleaned_tmdb_cast

        # â˜…â˜…â˜… åœ¨æµç¨‹å¼€å§‹æ—¶ï¼Œè®°å½•ä¸‹æ¥è‡ªTMDbçš„åŸå§‹æ¼”å‘˜ID â˜…â˜…â˜…
        original_tmdb_ids = {str(actor.get("id")) for actor in tmdb_cast_people if actor.get("id")}
        # ======================================================================
        # æ­¥éª¤ 1: â˜…â˜…â˜… æ•°æ®é€‚é… â˜…â˜…â˜…
        # ======================================================================
        logger.debug("  âœ å¼€å§‹æ¼”å‘˜æ•°æ®é€‚é… (åæŸ¥ç¼“å­˜æ¨¡å¼)...")
        
        tmdb_actor_map_by_id = {str(actor.get("id")): actor for actor in tmdb_cast_people}
        tmdb_actor_map_by_en_name = {str(actor.get("name") or "").lower().strip(): actor for actor in tmdb_cast_people}

        final_cast_list = []
        used_tmdb_ids = set()

        for emby_actor in emby_cast_people:
            emby_person_id = emby_actor.get("Id")
            emby_tmdb_id = emby_actor.get("ProviderIds", {}).get("Tmdb")
            emby_name_lower = str(emby_actor.get("Name") or "").lower().strip()

            tmdb_match = None

            if emby_tmdb_id and str(emby_tmdb_id) in tmdb_actor_map_by_id:
                tmdb_match = tmdb_actor_map_by_id[str(emby_tmdb_id)]
            else:
                if emby_name_lower in tmdb_actor_map_by_en_name:
                    tmdb_match = tmdb_actor_map_by_en_name[emby_name_lower]
                else:
                    cache_entry = self.actor_db_manager.get_translation_from_db(cursor, emby_actor.get("Name"), by_translated_text=True)
                    if cache_entry and cache_entry.get('original_text'):
                        original_en_name = str(cache_entry['original_text']).lower().strip()
                        if original_en_name in tmdb_actor_map_by_en_name:
                            tmdb_match = tmdb_actor_map_by_en_name[original_en_name]

            if tmdb_match:
                tmdb_id_str = str(tmdb_match.get("id"))
                merged_actor = tmdb_match.copy()
                merged_actor["emby_person_id"] = emby_person_id
                if utils.contains_chinese(emby_actor.get("Name")):
                    merged_actor["name"] = emby_actor.get("Name")
                else:
                    merged_actor["name"] = tmdb_match.get("name")
                merged_actor["character"] = emby_actor.get("Role")
                final_cast_list.append(merged_actor)
                used_tmdb_ids.add(tmdb_id_str)

        for tmdb_id, tmdb_actor_data in tmdb_actor_map_by_id.items():
            if tmdb_id not in used_tmdb_ids:
                new_actor = tmdb_actor_data.copy()
                new_actor["emby_person_id"] = None
                final_cast_list.append(new_actor)

        logger.debug(f"  âœ æ•°æ®é€‚é…å®Œæˆï¼Œç”Ÿæˆäº† {len(final_cast_list)} æ¡åŸºå‡†æ¼”å‘˜æ•°æ®ã€‚")
        
        # ======================================================================
        # æ­¥éª¤ 2: â˜…â˜…â˜… â€œä¸€å¯¹ä¸€åŒ¹é…â€é€»è¾‘ â˜…â˜…â˜…
        # ======================================================================
        douban_candidates = actor_utils.format_douban_cast(douban_cast_list)
        unmatched_local_actors = list(final_cast_list)
        merged_actors = []
        unmatched_douban_actors = []
        logger.info(f"  âœ åŒ¹é…é˜¶æ®µ 1: å¯¹å·å…¥åº§")
        for d_actor in douban_candidates:
            douban_name_zh = d_actor.get("Name", "").lower().strip()
            douban_name_en = d_actor.get("OriginalName", "").lower().strip()
            match_found_for_this_douban_actor = False
            for i, l_actor in enumerate(unmatched_local_actors):
                local_name = str(l_actor.get("name") or "").lower().strip()
                local_original_name = str(l_actor.get("original_name") or "").lower().strip()
                is_match = False
                if douban_name_zh and (douban_name_zh == local_name or douban_name_zh == local_original_name):
                    is_match = True
                elif douban_name_en and (douban_name_en == local_name or douban_name_en == local_original_name):
                    is_match = True
                if is_match:
                    l_actor["name"] = d_actor.get("Name")
                    cleaned_douban_character = utils.clean_character_name_static(d_actor.get("Role"))
                    l_actor["character"] = actor_utils.select_best_role(l_actor.get("character"), cleaned_douban_character)
                    
                    douban_id_to_add = d_actor.get("DoubanCelebrityId")
                    if douban_id_to_add:
                        l_actor["douban_id"] = douban_id_to_add
                    
                    merged_actors.append(unmatched_local_actors.pop(i))
                    match_found_for_this_douban_actor = True
                    break
            if not match_found_for_this_douban_actor:
                unmatched_douban_actors.append(d_actor)

        current_cast_list = merged_actors + unmatched_local_actors
        final_cast_map = {str(actor['id']): actor for actor in current_cast_list if actor.get('id') and str(actor.get('id')) != 'None'}

        # ======================================================================
        # æ­¥éª¤ 3: â˜…â˜…â˜… å¤„ç†è±†ç“£è¡¥å……æ¼”å‘˜ï¼ˆå¸¦ä¸¢å¼ƒé€»è¾‘ å’Œ æ•°é‡ä¸Šé™é€»è¾‘ï¼‰ â˜…â˜…â˜…
        # ======================================================================
        if not unmatched_douban_actors:
            logger.info("  âœ è±†ç“£APIæœªè¿”å›æ¼”å‘˜æˆ–æ‰€æœ‰æ¼”å‘˜å·²åŒ¹é…ï¼Œè·³è¿‡è¡¥å……æ¼”å‘˜æµç¨‹ã€‚")
        else:
            logger.info(f"  âœ å‘ç° {len(unmatched_douban_actors)} ä½æ½œåœ¨çš„è±†ç“£è¡¥å……æ¼”å‘˜ï¼Œå¼€å§‹æ‰§è¡ŒåŒ¹é…ä¸ç­›é€‰...")
            
            limit = self.config.get(constants.CONFIG_OPTION_MAX_ACTORS_TO_PROCESS, 30)
            try:
                limit = int(limit)
                if limit <= 0: limit = 30
            except (ValueError, TypeError):
                limit = 30

            current_actor_count = len(final_cast_map)
            if current_actor_count >= limit:
                logger.info(f"  âœ å½“å‰æ¼”å‘˜æ•° ({current_actor_count}) å·²è¾¾ä¸Šé™ ({limit})ï¼Œå°†è·³è¿‡æ‰€æœ‰è±†ç“£è¡¥å……æ¼”å‘˜çš„æµç¨‹ã€‚")
                still_unmatched_final = unmatched_douban_actors
            else:
                logger.info(f"  âœ å½“å‰æ¼”å‘˜æ•° ({current_actor_count}) ä½äºä¸Šé™ ({limit})ï¼Œè¿›å…¥è¡¥å……æ¨¡å¼ã€‚")
                
                logger.info(f"  âœ åŒ¹é…é˜¶æ®µ 2: ç”¨è±†ç“£IDæŸ¥'æ¼”å‘˜æ˜ å°„è¡¨' ({len(unmatched_douban_actors)} ä½æ¼”å‘˜)")
                still_unmatched = []
                for d_actor in unmatched_douban_actors:
                    if self.is_stop_requested(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                    d_douban_id = d_actor.get("DoubanCelebrityId")
                    match_found = False
                    if d_douban_id:
                        entry = self.actor_db_manager.find_person_by_any_id(cursor, douban_id=d_douban_id)
                        if entry and entry.get("tmdb_person_id") and entry.get("emby_person_id"):
                            tmdb_id_from_map = str(entry.get("tmdb_person_id"))
                            if tmdb_id_from_map not in final_cast_map:
                                logger.info(f"    â”œâ”€ åŒ¹é…æˆåŠŸ (é€šè¿‡ è±†ç“£IDæ˜ å°„): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> åŠ å…¥æœ€ç»ˆæ¼”å‘˜è¡¨")
                                cached_metadata_map = self.actor_db_manager.get_full_actor_details_by_tmdb_ids(cursor, [int(tmdb_id_from_map)])
                                cached_metadata = cached_metadata_map.get(int(tmdb_id_from_map), {})
                                new_actor_entry = {
                                    "id": tmdb_id_from_map, "name": d_actor.get("Name"),
                                    "original_name": cached_metadata.get("original_name") or d_actor.get("OriginalName"),
                                    "character": d_actor.get("Role"), "order": 999,
                                    "imdb_id": entry.get("imdb_id"), "douban_id": d_douban_id,
                                    "emby_person_id": entry.get("emby_person_id")
                                }
                                final_cast_map[tmdb_id_from_map] = new_actor_entry
                            match_found = True
                    if not match_found:
                        still_unmatched.append(d_actor)
                unmatched_douban_actors = still_unmatched

                logger.info(f"  âœ åŒ¹é…é˜¶æ®µ 3: ç”¨IMDb IDè¿›è¡Œæœ€ç»ˆåŒ¹é…å’Œæ–°å¢ ({len(unmatched_douban_actors)} ä½æ¼”å‘˜)")
                still_unmatched_final = []
                for i, d_actor in enumerate(unmatched_douban_actors):
                    if self.is_stop_requested(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                    
                    if len(final_cast_map) >= limit:
                        logger.info(f"  âœ æ¼”å‘˜æ•°å·²è¾¾ä¸Šé™ ({limit})ï¼Œè·³è¿‡å‰©ä½™ {len(unmatched_douban_actors) - i} ä½æ¼”å‘˜çš„APIæŸ¥è¯¢ã€‚")
                        still_unmatched_final.extend(unmatched_douban_actors[i:])
                        break

                    d_douban_id = d_actor.get("DoubanCelebrityId")
                    match_found = False
                    if d_douban_id and self.douban_api and self.tmdb_api_key:
                        if self.is_stop_requested(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                        details = self.douban_api.celebrity_details(d_douban_id)
                        time_module.sleep(0.3)
                        d_imdb_id = None
                        if details and not details.get("error"):
                            try:
                                info_list = details.get("extra", {}).get("info", [])
                                if isinstance(info_list, list):
                                    for item in info_list:
                                        if isinstance(item, list) and len(item) == 2 and item[0] == 'IMDbç¼–å·':
                                            d_imdb_id = item[1]
                                            break
                            except Exception as e_parse:
                                logger.warning(f"  âœ è§£æ IMDb ID æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e_parse}")
                        
                        if d_imdb_id:
                            logger.debug(f"  âœ ä¸º '{d_actor.get('Name')}' è·å–åˆ° IMDb ID: {d_imdb_id}ï¼Œå¼€å§‹åŒ¹é…...")
                            
                            entry_from_map = self.actor_db_manager.find_person_by_any_id(cursor, imdb_id=d_imdb_id)
                            if entry_from_map and entry_from_map.get("tmdb_person_id") and entry_from_map.get("emby_person_id"):
                                tmdb_id_from_map = str(entry_from_map.get("tmdb_person_id"))
                                if tmdb_id_from_map not in final_cast_map:
                                    logger.debug(f"    â”œâ”€ åŒ¹é…æˆåŠŸ (é€šè¿‡ IMDbæ˜ å°„): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> åŠ å…¥æœ€ç»ˆæ¼”å‘˜è¡¨")
                                    new_actor_entry = {
                                        "id": tmdb_id_from_map, "name": d_actor.get("Name"),
                                        "character": d_actor.get("Role"), "order": 999, "imdb_id": d_imdb_id,
                                        "douban_id": d_douban_id, "emby_person_id": entry_from_map.get("emby_person_id")
                                    }
                                    final_cast_map[tmdb_id_from_map] = new_actor_entry
                                match_found = True
                            
                            if not match_found:
                                logger.debug(f"  âœ æ•°æ®åº“æœªæ‰¾åˆ° {d_imdb_id} çš„æ˜ å°„ï¼Œå¼€å§‹é€šè¿‡ TMDb API åæŸ¥...")
                                if self.is_stop_requested(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                                person_from_tmdb = tmdb.find_person_by_external_id(
                                    external_id=d_imdb_id, api_key=self.tmdb_api_key, source="imdb_id"
                                )
                                if person_from_tmdb and person_from_tmdb.get("id"):
                                    tmdb_id_from_find = str(person_from_tmdb.get("id"))
                                    
                                    d_actor['tmdb_id_from_api'] = tmdb_id_from_find
                                    d_actor['imdb_id_from_api'] = d_imdb_id

                                    final_check_row = self.actor_db_manager.find_person_by_any_id(cursor, tmdb_id=tmdb_id_from_find)
                                    if final_check_row and dict(final_check_row).get("emby_person_id"):
                                        emby_pid_from_final_check = dict(final_check_row).get("emby_person_id")
                                        if tmdb_id_from_find not in final_cast_map:
                                            logger.info(f"    â”œâ”€ åŒ¹é…æˆåŠŸ (é€šè¿‡ TMDbåæŸ¥): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> åŠ å…¥æœ€ç»ˆæ¼”å‘˜è¡¨")
                                            new_actor_entry = {
                                                "id": tmdb_id_from_find, "name": d_actor.get("Name"),
                                                "character": d_actor.get("Role"), "order": 999,
                                                "imdb_id": d_imdb_id, "douban_id": d_douban_id,
                                                "emby_person_id": emby_pid_from_final_check
                                            }
                                            final_cast_map[tmdb_id_from_find] = new_actor_entry
                                        match_found = True
                    
                    if not match_found:
                        still_unmatched_final.append(d_actor)

                # --- å¤„ç†æ–°å¢ ---
                if still_unmatched_final:
                    logger.info(f"  âœ æ£€æŸ¥ {len(still_unmatched_final)} ä½æœªåŒ¹é…æ¼”å‘˜ï¼Œå°è¯•åˆå¹¶æˆ–åŠ å…¥æœ€ç»ˆåˆ—è¡¨...")
                    added_count = 0
                    merged_count = 0
                    
                    for d_actor in still_unmatched_final:
                        tmdb_id_to_process = d_actor.get('tmdb_id_from_api')
                        if tmdb_id_to_process:
                            # æƒ…å†µä¸€ï¼šæ¼”å‘˜å·²å­˜åœ¨ï¼Œæ‰§è¡Œåˆå¹¶/æ›´æ–°
                            if tmdb_id_to_process in final_cast_map:
                                existing_actor = final_cast_map[tmdb_id_to_process]
                                original_name = existing_actor.get("name")
                                new_name = d_actor.get("Name")
                                
                                # ä»…å½“è±†ç“£æä¾›äº†æ›´ä¼˜çš„åå­—ï¼ˆå¦‚ä¸­æ–‡åï¼‰æ—¶æ‰æ›´æ–°
                                if new_name and new_name != original_name and utils.contains_chinese(new_name):
                                    existing_actor["name"] = new_name
                                    logger.debug(f"    âœ [åˆå¹¶] å·²å°†æ¼”å‘˜ (TMDb ID: {tmdb_id_to_process}) çš„åå­—ä» '{original_name}' æ›´æ–°ä¸º '{new_name}'")
                                    merged_count += 1
                            
                            # æƒ…å†µäºŒï¼šæ¼”å‘˜ä¸å­˜åœ¨ï¼Œæ‰§è¡Œæ–°å¢
                            else:
                                new_actor_entry = {
                                    "id": tmdb_id_to_process,
                                    "name": d_actor.get("Name"),
                                    "character": d_actor.get("Role"),
                                    "order": 999,
                                    "imdb_id": d_actor.get("imdb_id_from_api"),
                                    "douban_id": d_actor.get("DoubanCelebrityId"),
                                    "emby_person_id": None
                                }
                                final_cast_map[tmdb_id_to_process] = new_actor_entry
                                added_count += 1
                    
                    if merged_count > 0:
                        logger.info(f"  âœ æˆåŠŸåˆå¹¶äº† {merged_count} ä½ç°æœ‰æ¼”å‘˜çš„è±†ç“£ä¿¡æ¯ã€‚")
                    if added_count > 0:
                        logger.info(f"  âœ æˆåŠŸæ–°å¢äº† {added_count} ä½æ¼”å‘˜åˆ°æœ€ç»ˆåˆ—è¡¨ã€‚")
        
        # ======================================================================
        # æ­¥éª¤ 4: â˜…â˜…â˜… ä»TMDbè¡¥å…¨å¤´åƒ â˜…â˜…â˜…
        # ======================================================================
        current_cast_list = list(final_cast_map.values())
        
        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 2/3: ç­›é€‰éœ€è¦è¡¥å…¨çš„æ¼”å‘˜æ—¶ï¼Œæ’é™¤æ‰åŸå§‹TMDbåˆ—è¡¨ä¸­çš„æ¼”å‘˜ â˜…â˜…â˜…
        actors_to_supplement = [
            actor for actor in current_cast_list 
            if str(actor.get("id")) not in original_tmdb_ids and actor.get("id")
        ]
        
        if actors_to_supplement:
            total_to_supplement = len(actors_to_supplement)
            logger.info(f"  âœ å¼€å§‹ä¸º {total_to_supplement} ä½æ–°å¢æ¼”å‘˜æ£€æŸ¥å¹¶è¡¥å…¨å¤´åƒä¿¡æ¯...")

            ids_to_fetch = [actor.get("id") for actor in actors_to_supplement if actor.get("id")]
            all_cached_metadata = self.actor_db_manager.get_full_actor_details_by_tmdb_ids(cursor, ids_to_fetch)
            
            supplemented_count = 0
            for actor in actors_to_supplement:
                if stop_event and stop_event.is_set(): raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                
                tmdb_id = actor.get("id")
                profile_path = None
                cached_meta = all_cached_metadata.get(tmdb_id)
                if cached_meta and cached_meta.get("profile_path"):
                    profile_path = cached_meta["profile_path"]
                
                elif tmdb_api_key:
                    person_details = tmdb.get_person_details_tmdb(tmdb_id, tmdb_api_key)
                    if person_details:
                        if person_details.get("profile_path"):
                            profile_path = person_details["profile_path"]
                
                if profile_path:
                    actor["profile_path"] = profile_path
                    supplemented_count += 1

            logger.info(f"  âœ æ–°å¢æ¼”å‘˜å¤´åƒä¿¡æ¯è¡¥å…¨å®Œæˆï¼ŒæˆåŠŸä¸º {supplemented_count}/{total_to_supplement} ä½æ¼”å‘˜è¡¥å……äº†å¤´åƒã€‚")
        else:
            logger.info("  âœ æ²¡æœ‰éœ€è¦è¡¥å……å¤´åƒçš„æ–°å¢æ¼”å‘˜ã€‚")

        # ======================================================================
        # æ­¥éª¤ 5: â˜…â˜…â˜… ä»æ¼”å‘˜è¡¨ç§»é™¤æ— å¤´åƒæ¼”å‘˜ â˜…â˜…â˜…
        # ======================================================================
        if self.config.get(constants.CONFIG_OPTION_REMOVE_ACTORS_WITHOUT_AVATARS, True):
            actors_with_avatars = [actor for actor in current_cast_list if actor.get("profile_path")]
            actors_without_avatars = [actor for actor in current_cast_list if not actor.get("profile_path")]

            if actors_without_avatars:
                removed_names = [a.get('name', f"TMDbID:{a.get('id')}") for a in actors_without_avatars]
                logger.info(f"  âœ å°†ç§»é™¤ {len(actors_without_avatars)} ä½æ— å¤´åƒçš„æ¼”å‘˜: {removed_names}")
                current_cast_list = actors_with_avatars
        else:
            logger.info("  âœ æœªå¯ç”¨ç§»é™¤æ— å¤´åƒæ¼”å‘˜ã€‚")

        # ======================================================================
        # æ­¥éª¤ 6ï¼šæ™ºèƒ½æˆªæ–­é€»è¾‘ (Smart Truncation) â˜…â˜…â˜…
        # ======================================================================
        max_actors = self.config.get(constants.CONFIG_OPTION_MAX_ACTORS_TO_PROCESS, 30)
        try:
            limit = int(max_actors)
            if limit <= 0: limit = 30
        except (ValueError, TypeError):
            limit = 30

        original_count = len(current_cast_list)
        
        if original_count > limit:
            logger.info(f"  âœ æ¼”å‘˜åˆ—è¡¨æ€»æ•° ({original_count}) è¶…è¿‡ä¸Šé™ ({limit})ï¼Œå°†ä¼˜å…ˆä¿ç•™æœ‰å¤´åƒçš„æ¼”å‘˜åè¿›è¡Œæˆªæ–­ã€‚")
            sort_key = lambda x: x.get('order') if x.get('order') is not None and x.get('order') >= 0 else 999
            with_profile = [actor for actor in current_cast_list if actor.get("profile_path")]
            without_profile = [actor for actor in current_cast_list if not actor.get("profile_path")]
            with_profile.sort(key=sort_key)
            without_profile.sort(key=sort_key)
            prioritized_list = with_profile + without_profile
            current_cast_list = prioritized_list[:limit]
            logger.debug(f"  âœ æˆªæ–­åï¼Œä¿ç•™äº† {len(with_profile)} ä½æœ‰å¤´åƒæ¼”å‘˜ä¸­çš„ {len([a for a in current_cast_list if a.get('profile_path')])} ä½ã€‚")
        else:
            # â–¼â–¼â–¼ æ ¸å¿ƒä¿®æ”¹ï¼šç›´æ¥åœ¨ current_cast_list ä¸Šæ’åº â–¼â–¼â–¼
            current_cast_list.sort(key=lambda x: x.get('order') if x.get('order') is not None and x.get('order') >= 0 else 999)

        # ======================================================================
        # æ­¥éª¤ 7: â˜…â˜…â˜… ç¿»è¯‘å’Œæ ¼å¼åŒ– â˜…â˜…â˜…
        # ======================================================================
        logger.info(f"  âœ å°†å¯¹ {len(current_cast_list)} ä½æ¼”å‘˜è¿›è¡Œæœ€ç»ˆçš„ç¿»è¯‘å’Œæ ¼å¼åŒ–å¤„ç†...")

        if not (self.ai_translator and self.config.get(constants.CONFIG_OPTION_AI_TRANSLATION_ENABLED, False)):
            logger.info("  âœ AIç¿»è¯‘æœªå¯ç”¨ï¼Œå°†ä¿ç•™æ¼”å‘˜å’Œè§’è‰²ååŸæ–‡ã€‚")
        else:
            final_translation_map = {}
            terms_to_translate = set()
            for actor in current_cast_list:
                character = actor.get('character')
                if character:
                    cleaned_character = utils.clean_character_name_static(character)
                    if cleaned_character and not utils.contains_chinese(cleaned_character):
                        terms_to_translate.add(cleaned_character)
                name = actor.get('name')
                if name and not utils.contains_chinese(name):
                    terms_to_translate.add(name)
            
            total_terms_count = len(terms_to_translate)
            logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 1. ä»»åŠ¡æ¦‚è§ˆ: å…±æ”¶é›†åˆ° {total_terms_count} ä¸ªç‹¬ç«‹è¯æ¡éœ€è¦ç¿»è¯‘ã€‚")
            if total_terms_count > 0:
                logger.debug(f"    âœ å¾…å¤„ç†è¯æ¡åˆ—è¡¨: {list(terms_to_translate)}")

            remaining_terms = list(terms_to_translate)
            if remaining_terms:
                cached_results = {}
                terms_for_api = []
                for term in remaining_terms:
                    cached = self.actor_db_manager.get_translation_from_db(cursor, term)
                    if cached and cached.get('translated_text'):
                        cached_results[term] = cached['translated_text']
                    else:
                        terms_for_api.append(term)
                
                cached_count = len(cached_results)
                logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 2. ç¼“å­˜æ£€æŸ¥: å‘½ä¸­æ•°æ®åº“ç¼“å­˜ {cached_count} æ¡ã€‚")
                if cached_count > 0:
                    logger.debug("    âœ å‘½ä¸­ç¼“å­˜çš„è¯æ¡ä¸è¯‘æ–‡:")
                    for k, v in sorted(cached_results.items()):
                        logger.debug(f"    â”œâ”€ {k} âœ {v}")

                if cached_results:
                    final_translation_map.update(cached_results)
                if terms_for_api:
                    logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 3. AIå¤„ç† (å¿«é€Ÿæ¨¡å¼): æäº¤ {len(terms_for_api)} æ¡ã€‚")
                    if terms_for_api:
                        logger.debug(f"    âœ æäº¤ç»™[å¿«é€Ÿæ¨¡å¼]çš„è¯æ¡: {terms_for_api}")
                    fast_api_results = self.ai_translator.batch_translate(terms_for_api, mode='fast')
                    for term, translation in fast_api_results.items():
                        final_translation_map[term] = translation
                        self.actor_db_manager.save_translation_to_db(cursor, term, translation, self.ai_translator.provider)
                failed_terms = []
                for term in remaining_terms:
                    if not utils.contains_chinese(final_translation_map.get(term, term)):
                        failed_terms.append(term)
                remaining_terms = failed_terms
            if remaining_terms:
                logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 4. AIå¤„ç† (éŸ³è¯‘æ¨¡å¼): æäº¤ {len(remaining_terms)} æ¡ã€‚")
                if remaining_terms:
                    logger.debug(f"    âœ æäº¤ç»™[éŸ³è¯‘æ¨¡å¼]çš„è¯æ¡: {remaining_terms}")
                transliterate_results = self.ai_translator.batch_translate(remaining_terms, mode='transliterate')
                final_translation_map.update(transliterate_results)
                still_failed_terms = []
                for term in remaining_terms:
                    if not utils.contains_chinese(final_translation_map.get(term, term)):
                        still_failed_terms.append(term)
                remaining_terms = still_failed_terms
            if remaining_terms:
                item_title = item_details_from_emby.get("Name")
                item_year = item_details_from_emby.get("ProductionYear")
                logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 5. AIå¤„ç† (é¡¾é—®æ¨¡å¼): æäº¤ {len(remaining_terms)} æ¡ã€‚")
                if remaining_terms:
                    logger.debug(f"  âœ æäº¤ç»™[é¡¾é—®æ¨¡å¼]çš„è¯æ¡: {remaining_terms}")
                quality_results = self.ai_translator.batch_translate(remaining_terms, mode='quality', title=item_title, year=item_year)
                final_translation_map.update(quality_results)
            
            successfully_translated_terms = {term for term in terms_to_translate if utils.contains_chinese(final_translation_map.get(term, ''))}
            failed_to_translate_terms = terms_to_translate - successfully_translated_terms
            
            logger.info(f"  âœ [ç¿»è¯‘ç»Ÿè®¡] 6. ç»“æœæ€»ç»“: æˆåŠŸç¿»è¯‘ {len(successfully_translated_terms)}/{total_terms_count} ä¸ªè¯æ¡ã€‚")
            if successfully_translated_terms:
                logger.debug("  âœ ç¿»è¯‘æˆåŠŸåˆ—è¡¨ (åŸæ–‡ âœ è¯‘æ–‡):")
                for term in sorted(list(successfully_translated_terms)):
                    translation = final_translation_map.get(term)
                    logger.debug(f"    â”œâ”€ {term} âœ {translation}")
            if failed_to_translate_terms:
                logger.warning(f"    âœ ç¿»è¯‘å¤±è´¥åˆ—è¡¨ ({len(failed_to_translate_terms)}æ¡): {list(failed_to_translate_terms)}")

            for actor in current_cast_list:
                original_name = actor.get('name')
                if original_name and original_name in final_translation_map:
                    actor['name'] = final_translation_map[original_name]
                original_character = actor.get('character')
                if original_character:
                    cleaned_character = utils.clean_character_name_static(original_character)
                    actor['character'] = final_translation_map.get(cleaned_character, cleaned_character)
                else:
                    actor['character'] = ''

        tmdb_to_emby_id_map = {
            str(actor.get('id')): actor.get('emby_person_id')
            for actor in current_cast_list if actor.get('id') and actor.get('emby_person_id')
        }
        genres = item_details_from_emby.get("Genres", [])
        is_animation = "Animation" in genres or "åŠ¨ç”»" in genres or "Documentary" in genres or "çºªå½•" in genres
        final_cast_perfect = actor_utils.format_and_complete_cast_list(
            current_cast_list, is_animation, self.config, mode='auto'
        )
        for actor in final_cast_perfect:
            tmdb_id_str = str(actor.get("id"))
            if tmdb_id_str in tmdb_to_emby_id_map:
                actor["emby_person_id"] = tmdb_to_emby_id_map[tmdb_id_str]
        for actor in final_cast_perfect:
            actor["provider_ids"] = {
                "Tmdb": str(actor.get("id")),
                "Imdb": actor.get("imdb_id"),
                "Douban": actor.get("douban_id")
            }

        # ======================================================================
        # æ­¥éª¤ 8: â˜…â˜…â˜… æœ€ç»ˆæ•°æ®å›å†™/åå“º â˜…â˜…â˜… 
        # ======================================================================
        logger.info(f"  âœ å¼€å§‹å°† {len(final_cast_perfect)} ä½æœ€ç»ˆæ¼”å‘˜çš„å®Œæ•´ä¿¡æ¯åŒæ­¥å›æ•°æ®åº“...")
        processed_count = 0
        
        # åœ¨å¾ªç¯å¤–å‡†å¤‡ emby_configï¼Œé¿å…é‡å¤åˆ›å»º
        emby_config_for_upsert = {"url": self.emby_url, "api_key": self.emby_api_key, "user_id": self.emby_user_id}

        for actor in final_cast_perfect:
            # ç›´æ¥å°† actor å­—å…¸å’Œ emby_config ä¼ é€’ç»™ upsert_person å‡½æ•°
            map_id, action = self.actor_db_manager.upsert_person(cursor, actor, emby_config_for_upsert)
            
            if action not in ["ERROR", "SKIPPED", "CONFLICT_ERROR", "UNKNOWN_ERROR"]:
                processed_count += 1
            else:
                # å¦‚æœå‘ç”Ÿé”™è¯¯ï¼Œå›æ»šå½“å‰æ¼”å‘˜çš„æ“ä½œï¼Œå¹¶ä¸ºä¸‹ä¸€ä¸ªæ¼”å‘˜å¼€å¯æ–°äº‹åŠ¡
                # è¿™æ˜¯ä¸ºäº†é˜²æ­¢ä¸€ä¸ªæ¼”å‘˜çš„é”™è¯¯å¯¼è‡´æ•´ä¸ªæ‰¹æ¬¡å¤±è´¥
                cursor.connection.rollback()
                cursor.execute("BEGIN")

        logger.info(f"  âœ æˆåŠŸå¤„ç†äº† {processed_count} ä½æ¼”å‘˜çš„æ•°æ®åº“å›å†™/æ›´æ–°ã€‚")

        return final_cast_perfect
    
    # --- ä¸€é”®ç¿»è¯‘ ---
    def translate_cast_list_for_editing(self, 
                                    cast_list: List[Dict[str, Any]], 
                                    title: Optional[str] = None, 
                                    year: Optional[int] = None,
                                    tmdb_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        ã€V14 - çº¯AIç¿»è¯‘ç‰ˆã€‘ä¸ºæ‰‹åŠ¨ç¼–è¾‘é¡µé¢æä¾›çš„ä¸€é”®ç¿»è¯‘åŠŸèƒ½ã€‚
        - å½»åº•ç§»é™¤ä¼ ç»Ÿç¿»è¯‘å¼•æ“çš„é™çº§é€»è¾‘ã€‚
        - å¦‚æœAIç¿»è¯‘æœªå¯ç”¨æˆ–å¤±è´¥ï¼Œåˆ™ç›´æ¥æ”¾å¼ƒç¿»è¯‘ã€‚
        """
        if not cast_list:
            return []

        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 1: æ£€æŸ¥AIç¿»è¯‘æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœæœªå¯ç”¨åˆ™ç›´æ¥è¿”å› â˜…â˜…â˜…
        if not self.ai_translator or not self.config.get(constants.CONFIG_OPTION_AI_TRANSLATION_ENABLED, False):
            logger.info("æ‰‹åŠ¨ç¼–è¾‘-ä¸€é”®ç¿»è¯‘ï¼šAIç¿»è¯‘æœªå¯ç”¨ï¼Œä»»åŠ¡è·³è¿‡ã€‚")
            # å¯ä»¥åœ¨è¿™é‡Œè¿”å›ä¸€ä¸ªæç¤ºç»™å‰ç«¯ï¼Œæˆ–è€…ç›´æ¥è¿”å›åŸå§‹åˆ—è¡¨
            # ä¸ºäº†å‰ç«¯ä½“éªŒï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç¬¬ä¸€ä¸ªéœ€è¦ç¿»è¯‘çš„æ¼”å‘˜ä¸ŠåŠ ä¸€ä¸ªçŠ¶æ€
            translated_cast_for_status = [dict(actor) for actor in cast_list]
            for actor in translated_cast_for_status:
                name_needs_translation = actor.get('name') and not utils.contains_chinese(actor.get('name'))
                role_needs_translation = actor.get('role') and not utils.contains_chinese(actor.get('role'))
                if name_needs_translation or role_needs_translation:
                    actor['matchStatus'] = 'AIæœªå¯ç”¨'
                    break # åªæ ‡è®°ç¬¬ä¸€ä¸ªå³å¯
            return translated_cast_for_status

        # ä»é…ç½®ä¸­è¯»å–æ¨¡å¼
        translation_mode = self.config.get(constants.CONFIG_OPTION_AI_TRANSLATION_MODE, "fast")
        
        context_log = f" (ä¸Šä¸‹æ–‡: {title} {year})" if title and translation_mode == 'quality' else ""
        logger.info(f"æ‰‹åŠ¨ç¼–è¾‘-ä¸€é”®ç¿»è¯‘ï¼šå¼€å§‹æ‰¹é‡å¤„ç† {len(cast_list)} ä½æ¼”å‘˜ (æ¨¡å¼: {translation_mode}){context_log}ã€‚")
        
        translated_cast = [dict(actor) for actor in cast_list]
        
        # --- çº¯AIæ‰¹é‡ç¿»è¯‘é€»è¾‘ ---
        try:
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                
                translation_cache = {} # æœ¬æ¬¡è¿è¡Œçš„å†…å­˜ç¼“å­˜
                texts_to_translate = set()

                # 1. æ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„è¯æ¡
                texts_to_collect = set()
                for actor in translated_cast:
                    for field_key in ['name', 'role']:
                        text = actor.get(field_key, '').strip()
                        if field_key == 'role':
                            text = utils.clean_character_name_static(text)
                        if text and not utils.contains_chinese(text):
                            texts_to_collect.add(text)

                # 2. æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦ä½¿ç”¨ç¼“å­˜
                if translation_mode == 'fast':
                    logger.debug("[å¿«é€Ÿæ¨¡å¼] æ­£åœ¨æ£€æŸ¥å…¨å±€ç¿»è¯‘ç¼“å­˜...")
                    for text in texts_to_collect:
                        cached_entry = self.actor_db_manager.get_translation_from_db(cursor=cursor, text=text)
                        if cached_entry:
                            translation_cache[text] = cached_entry.get("translated_text")
                        else:
                            texts_to_translate.add(text)
                else: # 'quality' mode
                    logger.debug("[é¡¾é—®æ¨¡å¼] è·³è¿‡ç¼“å­˜æ£€æŸ¥ï¼Œç›´æ¥ç¿»è¯‘æ‰€æœ‰è¯æ¡ã€‚")
                    texts_to_translate = texts_to_collect

                # 3. å¦‚æœæœ‰éœ€è¦ç¿»è¯‘çš„è¯æ¡ï¼Œè°ƒç”¨AI
                if texts_to_translate:
                    logger.info(f"æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šå°† {len(texts_to_translate)} ä¸ªè¯æ¡æäº¤ç»™AI (æ¨¡å¼: {translation_mode})ã€‚")
                    translation_map_from_api = self.ai_translator.batch_translate(
                        texts=list(texts_to_translate),
                        mode=translation_mode,
                        title=title,
                        year=year
                    )
                    if translation_map_from_api:
                        translation_cache.update(translation_map_from_api)
                        
                        if translation_mode == 'fast':
                            for original, translated in translation_map_from_api.items():
                                self.actor_db_manager.save_translation_to_db(
                                    cursor=cursor,
                                    original_text=original, 
                                    translated_text=translated, 
                                    engine_used=self.ai_translator.provider
                                )
                    else:
                        logger.warning("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šAIæ‰¹é‡ç¿»è¯‘æœªè¿”å›ä»»ä½•ç»“æœã€‚")
                else:
                    logger.info("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šæ‰€æœ‰è¯æ¡å‡åœ¨ç¼“å­˜ä¸­æ‰¾åˆ°ï¼Œæ— éœ€è°ƒç”¨APIã€‚")

                # 4. å›å¡«æ‰€æœ‰ç¿»è¯‘ç»“æœ
                if translation_cache:
                    for i, actor in enumerate(translated_cast):
                        original_name = actor.get('name', '').strip()
                        if original_name in translation_cache:
                            translated_cast[i]['name'] = translation_cache[original_name]
                        
                        original_role_raw = actor.get('role', '').strip()
                        cleaned_original_role = utils.clean_character_name_static(original_role_raw)
                        
                        if cleaned_original_role in translation_cache:
                            translated_cast[i]['role'] = translation_cache[cleaned_original_role]
                        
                        if translated_cast[i].get('name') != actor.get('name') or translated_cast[i].get('role') != actor.get('role'):
                            translated_cast[i]['matchStatus'] = 'å·²ç¿»è¯‘'
        
        except Exception as e:
            logger.error(f"ä¸€é”®ç¿»è¯‘æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            # å¯ä»¥åœ¨è¿™é‡Œç»™å‡ºä¸€ä¸ªé”™è¯¯æç¤º
            for actor in translated_cast:
                actor['matchStatus'] = 'ç¿»è¯‘å‡ºé”™'
                break
            return translated_cast

        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 2: å½»åº•åˆ é™¤é™çº§é€»è¾‘ â˜…â˜…â˜…
        # åŸæœ‰çš„ if not ai_translation_succeeded: ... else ... ä»£ç å—å·²å…¨éƒ¨ç§»é™¤ã€‚

        logger.info("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘å®Œæˆã€‚")
        return translated_cast
    
    # âœ¨âœ¨âœ¨æ‰‹åŠ¨å¤„ç†âœ¨âœ¨âœ¨
    def process_item_with_manual_cast(self, item_id: str, manual_cast_list: List[Dict[str, Any]], item_name: str) -> bool:
        """
        ã€V2.5 - ç»ˆæä¿®å¤ç‰ˆã€‘
        1. å¢åŠ äº†å®Œæ•´çš„æ—¥å¿—è®°å½•ï¼Œè®©æ¯ä¸€æ­¥æ“ä½œéƒ½æ¸…æ™°å¯è§ã€‚
        2. ä¿®å¤å¹¶å¼ºåŒ–äº†â€œç¿»è¯‘ç¼“å­˜åå“ºâ€åŠŸèƒ½ã€‚
        3. å¢åŠ äº†åœ¨å†™å…¥æ–‡ä»¶å‰çš„å¼ºåˆ¶â€œæœ€ç»ˆæ ¼å¼åŒ–â€æ­¥éª¤ï¼Œç¡®ä¿å‰ç¼€æ°¸è¿œæ­£ç¡®ã€‚
        """
        logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†æµç¨‹å¯åŠ¨ï¼šItemID: {item_id} ('{item_name}')")
        
        try:
            item_details = emby.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id)
            if not item_details: raise ValueError(f"æ— æ³•è·å–é¡¹ç›® {item_id} çš„è¯¦æƒ…ã€‚")
            
            raw_emby_actors = [p for p in item_details.get("People", []) if p.get("Type") == "Actor"]
            emby_config = {"url": self.emby_url, "api_key": self.emby_api_key, "user_id": self.emby_user_id}

            # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹: åœ¨æ‰€æœ‰æ“ä½œå¼€å§‹å‰ï¼Œä¸€æ¬¡æ€§è·å–æ‰€æœ‰ enriched_actors â˜…â˜…â˜…
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                enriched_actors = self.actor_db_manager.enrich_actors_with_provider_ids(cursor, raw_emby_actors, emby_config)

            # ======================================================================
            # æ­¥éª¤ 1: æ•°æ®å‡†å¤‡ä¸å®šä½ (ç°åœ¨åªè´Ÿè´£æ„å»ºæ˜ å°„)
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 1/6: æ„å»ºTMDbä¸Embyæ¼”å‘˜çš„IDæ˜ å°„...")
            tmdb_to_emby_map = {}
            for person in enriched_actors:
                person_tmdb_id = (person.get("ProviderIds") or {}).get("Tmdb")
                if person_tmdb_id:
                    tmdb_to_emby_map[str(person_tmdb_id)] = person.get("Id")
            logger.info(f"  âœ æˆåŠŸæ„å»ºäº† {len(tmdb_to_emby_map)} æ¡IDæ˜ å°„ã€‚")
            
            item_type = item_details.get("Type")
            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            if not tmdb_id: raise ValueError(f"é¡¹ç›® {item_id} ç¼ºå°‘ TMDb IDã€‚")

            # --- æ–°å¢ï¼šè·å– TMDb è¯¦æƒ…ç”¨äºåˆ†çº§æ•°æ®æå– ---
            tmdb_details_for_manual_extra = None
            if self.tmdb_api_key:
                if item_type == "Movie":
                    tmdb_details_for_manual_extra = tmdb.get_movie_details(tmdb_id, self.tmdb_api_key)
                    if not tmdb_details_for_manual_extra:
                        logger.warning(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ— æ³•ä» TMDb è·å–ç”µå½± '{item_name}' ({tmdb_id}) çš„è¯¦æƒ…ã€‚")
                elif item_type == "Series":
                    aggregated_tmdb_data = tmdb.aggregate_full_series_data_from_tmdb(int(tmdb_id), self.tmdb_api_key)
                    if aggregated_tmdb_data:
                        tmdb_details_for_manual_extra = aggregated_tmdb_data.get("series_details")
                    else:
                        logger.warning(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ— æ³•ä» TMDb è·å–å‰§é›† '{item_name}' ({tmdb_id}) çš„è¯¦æƒ…ã€‚")
            else:
                logger.warning("  âœ æ‰‹åŠ¨å¤„ç†ï¼šæœªé…ç½® TMDb API Keyï¼Œæ— æ³•è·å– TMDb è¯¦æƒ…ç”¨äºåˆ†çº§æ•°æ®ã€‚")

            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
            main_json_filename = "all.json" if item_type == "Movie" else "series.json"
            main_json_path = os.path.join(target_override_dir, main_json_filename)

            if not os.path.exists(main_json_path):
                raise FileNotFoundError(f"æ‰‹åŠ¨å¤„ç†å¤±è´¥ï¼šæ‰¾ä¸åˆ°ä¸»å…ƒæ•°æ®æ–‡ä»¶ '{main_json_path}'ã€‚")

            # ======================================================================
            # æ­¥éª¤ 2: æ›´æ–°AIç¿»è¯‘ç¼“å­˜
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 2/5: æ£€æŸ¥å¹¶æ›´æ–°AIç¿»è¯‘ç¼“å­˜...")
            try:
                # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ â‘ : ä»ç¼“å­˜è·å–çš„æ˜¯ tmdbId -> åŸå§‹è§’è‰²å çš„å­—å…¸ â˜…â˜…â˜…
                original_roles_map = self.manual_edit_cache.get(item_id)
                if original_roles_map:
                    with get_central_db_connection() as conn:
                        cursor = conn.cursor()
                        updated_count = 0
                        
                        # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ â‘¡: éå†å‰ç«¯æäº¤çš„åˆ—è¡¨ â˜…â˜…â˜…
                        for actor_from_frontend in manual_cast_list:
                            tmdb_id_str = str(actor_from_frontend.get("tmdbId"))
                            if not tmdb_id_str: continue
                            
                            # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ â‘¢: ç”¨ tmdbId ç²¾å‡†æ‰¾åˆ°ä¿®æ”¹å‰çš„è§’è‰²å â˜…â˜…â˜…
                            original_role = original_roles_map.get(tmdb_id_str)
                            if original_role is None: # å¦‚æœåŸå§‹è®°å½•é‡Œå°±æ²¡æœ‰è¿™ä¸ªæ¼”å‘˜ï¼Œå°±è·³è¿‡
                                continue

                            new_role = actor_from_frontend.get('role', '')
                            
                            cleaned_new_role = utils.clean_character_name_static(new_role)
                            cleaned_original_role = utils.clean_character_name_static(original_role)

                            if cleaned_new_role and cleaned_new_role != cleaned_original_role:
                                cache_entry = self.actor_db_manager.get_translation_from_db(text=cleaned_original_role, by_translated_text=True, cursor=cursor)
                                if cache_entry and 'original_text' in cache_entry:
                                    original_text_key = cache_entry['original_text']
                                    self.actor_db_manager.save_translation_to_db(
                                        cursor=cursor, original_text=original_text_key,
                                        translated_text=cleaned_new_role, engine_used="manual"
                                    )
                                    logger.debug(f"  âœ AIç¿»è¯‘ç¼“å­˜å·²æ›´æ–°: '{original_text_key}' ('{cleaned_original_role}' -> '{cleaned_new_role}')")
                                    updated_count += 1
                        if updated_count > 0:
                            logger.info(f"  âœ æˆåŠŸæ›´æ–°äº† {updated_count} æ¡ç¿»è¯‘ç¼“å­˜ã€‚")
                        else:
                            logger.info(f"  âœ æ— éœ€æ›´æ–°ç¿»è¯‘ç¼“å­˜ (è§’è‰²åæœªå‘ç”Ÿæœ‰æ•ˆå˜æ›´)ã€‚")
                        conn.commit()
                else:
                    logger.warning(f"  âœ æ— æ³•æ›´æ–°ç¿»è¯‘ç¼“å­˜ï¼šå†…å­˜ä¸­æ‰¾ä¸åˆ° ItemID {item_id} çš„åŸå§‹æ¼”å‘˜æ•°æ®ä¼šè¯ã€‚")
            except Exception as e:
                logger.error(f"  âœ æ‰‹åŠ¨å¤„ç†æœŸé—´æ›´æ–°ç¿»è¯‘ç¼“å­˜æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", exc_info=True)
            
            # ======================================================================
            # æ­¥éª¤ 3: APIå‰ç½®æ“ä½œ (æ›´æ–°æ¼”å‘˜å)
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 3/6: é€šè¿‡APIæ›´æ–°ç°æœ‰æ¼”å‘˜çš„åå­—...")
            # æ„å»º TMDb ID -> Emby Person ID å’Œ Emby Person ID -> å½“å‰åå­—çš„æ˜ å°„
            emby_id_to_name_map = {}
            for person in enriched_actors: # â˜…â˜…â˜… ç›´æ¥ä½¿ç”¨ enriched_actors
                person_emby_id = person.get("Id")
                if person_emby_id:
                    emby_id_to_name_map[person_emby_id] = person.get("Name")
            
            tmdb_to_emby_map = {}
            emby_id_to_name_map = {}
            for person in enriched_actors:
                person_tmdb_id = (person.get("ProviderIds") or {}).get("Tmdb")
                person_emby_id = person.get("Id")
                if person_tmdb_id and person_emby_id:
                    tmdb_to_emby_map[str(person_tmdb_id)] = person_emby_id
                    emby_id_to_name_map[person_emby_id] = person.get("Name")

            updated_names_count = 0
            for actor_from_frontend in manual_cast_list:
                tmdb_id_str = str(actor_from_frontend.get("tmdbId"))
                
                # åªå¤„ç†åœ¨æ˜ å°„ä¸­èƒ½æ‰¾åˆ°çš„ã€å·²å­˜åœ¨çš„æ¼”å‘˜
                actor_emby_id = tmdb_to_emby_map.get(tmdb_id_str)
                if not actor_emby_id: continue

                new_name = actor_from_frontend.get("name")
                original_name = emby_id_to_name_map.get(actor_emby_id)
                
                if new_name and original_name and new_name != original_name:
                    emby.update_person_details(
                        person_id=actor_emby_id, new_data={"Name": new_name},
                        emby_server_url=self.emby_url, emby_api_key=self.emby_api_key, user_id=self.emby_user_id
                    )
                    updated_names_count += 1
            
            if updated_names_count > 0:
                logger.info(f"  âœ æˆåŠŸé€šè¿‡ API æ›´æ–°äº† {updated_names_count} ä½æ¼”å‘˜çš„åå­—ã€‚")

            # ======================================================================
            # æ­¥éª¤ 4: æ–‡ä»¶è¯»ã€æ”¹ã€å†™ (åŒ…å«æœ€ç»ˆæ ¼å¼åŒ–)
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 4/6: è¯»å–åŸå§‹æ•°æ®ï¼Œè¯†åˆ«å¹¶è¡¥å…¨æ–°å¢æ¼”å‘˜çš„å…ƒæ•°æ®...")
            with open(main_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            original_cast_data = (data.get('casts', {}) or data.get('credits', {})).get('cast', [])
            original_cast_map = {str(actor.get('id')): actor for actor in original_cast_data if actor.get('id')}

            new_actor_tmdb_ids = [
                int(actor.get("tmdbId")) for actor in manual_cast_list 
                if str(actor.get("tmdbId")) not in original_cast_map
            ]

            all_new_actors_metadata = {}
            if new_actor_tmdb_ids:
                with get_central_db_connection() as conn_new:
                    cursor_new = conn_new.cursor()
                    all_new_actors_metadata = self.actor_db_manager.get_full_actor_details_by_tmdb_ids(cursor_new, new_actor_tmdb_ids)

            new_cast_built = []
            
            with get_central_db_connection() as conn:
                cursor = conn.cursor()

                for actor_from_frontend in manual_cast_list:
                    tmdb_id_str = str(actor_from_frontend.get("tmdbId"))
                    if not tmdb_id_str: continue
                    
                    # --- A. å¤„ç†ç°æœ‰æ¼”å‘˜ ---
                    if tmdb_id_str in original_cast_map:
                        updated_actor_entry = original_cast_map[tmdb_id_str].copy()
                        updated_actor_entry['name'] = actor_from_frontend.get('name')
                        updated_actor_entry['character'] = actor_from_frontend.get('role')
                        new_cast_built.append(updated_actor_entry)
                    
                    # --- B. å¤„ç†æ–°å¢æ¼”å‘˜ ---
                    else:
                        logger.info(f"    â”œâ”€ å‘ç°æ–°æ¼”å‘˜: '{actor_from_frontend.get('name')}' (TMDb ID: {tmdb_id_str})ï¼Œå¼€å§‹è¡¥å…¨å…ƒæ•°æ®...")
                        
                        # B1: ä¼˜å…ˆä» å†…å­˜ ç¼“å­˜è·å–
                        person_details = all_new_actors_metadata.get(int(tmdb_id_str))
                        
                        # B2: å¦‚æœç¼“å­˜æ²¡æœ‰ï¼Œåˆ™ä» TMDb API è·å–å¹¶åå“º
                        if not person_details:
                            logger.debug(f"  âœ ç¼“å­˜æœªå‘½ä¸­ï¼Œä» TMDb API è·å–è¯¦æƒ…...")
                            person_details_from_api = tmdb.get_person_details_tmdb(tmdb_id_str, self.tmdb_api_key)
                            if person_details_from_api:
                                self.actor_db_manager.update_actor_metadata_from_tmdb(cursor, tmdb_id_str, person_details_from_api)
                                person_details = person_details_from_api # ä½¿ç”¨APIè¿”å›çš„æ•°æ®
                            else:
                                logger.warning(f"  âœ æ— æ³•è·å–TMDb ID {tmdb_id_str} çš„è¯¦æƒ…ï¼Œå°†ä½¿ç”¨åŸºç¡€ä¿¡æ¯è·³è¿‡ã€‚")
                                # å³ä½¿å¤±è´¥ï¼Œä¹Ÿåˆ›å»ºä¸€ä¸ªåŸºç¡€å¯¹è±¡ï¼Œé¿å…ä¸¢å¤±
                                person_details = {} 
                        else:
                            logger.debug(f"  âœ æˆåŠŸä»æ•°æ®åº“ç¼“å­˜å‘½ä¸­å…ƒæ•°æ®ã€‚")

                        # B3: æ„å»ºä¸€ä¸ªä¸ override æ–‡ä»¶æ ¼å¼ä¸€è‡´çš„æ–°æ¼”å‘˜å¯¹è±¡
                        new_actor_entry = {
                            "id": int(tmdb_id_str),
                            "name": actor_from_frontend.get('name'),
                            "character": actor_from_frontend.get('role'),
                            "original_name": person_details.get("original_name"),
                            "profile_path": person_details.get("profile_path"),
                            "adult": person_details.get("adult", False),
                            "gender": person_details.get("gender", 0),
                            "known_for_department": person_details.get("known_for_department", "Acting"),
                            "popularity": person_details.get("popularity", 0.0),
                            # æ–°å¢æ¼”å‘˜æ²¡æœ‰è¿™äº›ç”µå½±ç‰¹å®šçš„IDï¼Œè®¾ä¸ºNone
                            "cast_id": None, 
                            "credit_id": None,
                            "order": 999 # æ”¾åˆ°æœ€åï¼Œåç»­æ ¼å¼åŒ–æ­¥éª¤ä¼šé‡æ–°æ’åº
                        }
                        new_cast_built.append(new_actor_entry)

            # ======================================================================
            # æ­¥éª¤ 5: æœ€ç»ˆæ ¼å¼åŒ–å¹¶å†™å…¥æ–‡ä»¶ (é€»è¾‘ä¸å˜)
            # ======================================================================
            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 5/6: é‡å»ºæ¼”å‘˜åˆ—è¡¨å¹¶æ‰§è¡Œæœ€ç»ˆæ ¼å¼åŒ–...")
            genres = item_details.get("Genres", [])
            is_animation = "Animation" in genres or "åŠ¨ç”»" in genres or "Documentary" in genres or "çºªå½•" in genres
            final_formatted_cast = actor_utils.format_and_complete_cast_list(
                new_cast_built, is_animation, self.config, mode='manual'
            )
            # _build_cast_from_final_data ç¡®ä¿äº†æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ï¼Œå³ä½¿æ˜¯None
            final_cast_for_json = self._build_cast_from_final_data(final_formatted_cast)

            if 'casts' in data:
                data['casts']['cast'] = final_cast_for_json
            elif 'credits' in data:
                data['credits']['cast'] = final_cast_for_json
            else:
                data.setdefault('credits', {})['cast'] = final_cast_for_json
            
            with open(main_json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            if item_type == "Series":
                self._inject_cast_to_series_files(
                    target_dir=target_override_dir, cast_list=final_cast_for_json,
                    series_details=item_details, source_dir=os.path.join(self.local_data_path, "cache", cache_folder_name, tmdb_id)
                )

            # ======================================================================
            # æ­¥éª¤ 6: è§¦å‘åˆ·æ–°å¹¶æ›´æ–°æ—¥å¿—
            # ======================================================================
            logger.info("  âœ æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 6/6: è§¦å‘ Emby åˆ·æ–°å¹¶æ›´æ–°å†…éƒ¨æ—¥å¿—...")
            
            emby.refresh_emby_item_metadata(
                item_emby_id=item_id,
                emby_server_url=self.emby_url,
                emby_api_key=self.emby_api_key,
                user_id_for_ops=self.emby_user_id,
                replace_all_metadata_param=True,
                item_name_for_log=item_name
            )

            # æ›´æ–°æˆ‘ä»¬è‡ªå·±çš„æ•°æ®åº“æ—¥å¿—å’Œç¼“å­˜
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                
                # ======================================================================
                # â˜…â˜…â˜… è°ƒç”¨ç»Ÿä¸€çš„ã€å·²è§„èŒƒåŒ–çš„ç¼“å­˜å†™å…¥å‡½æ•° â˜…â˜…â˜…
                # ======================================================================
                self._upsert_media_metadata(
                    cursor=cursor,
                    item_type=item_type,
                    item_details_from_emby=item_details,
                    final_processed_cast=final_formatted_cast, 
                    source_data_package=tmdb_details_for_manual_extra, 
                )
                
                logger.info(f"  âœ æ­£åœ¨å°†æ‰‹åŠ¨å¤„ç†å®Œæˆçš„ã€Š{item_name}ã€‹å†™å…¥å·²å¤„ç†æ—¥å¿—...")
                self.log_db_manager.save_to_processed_log(cursor, item_id, item_name, score=10.0)
                self.log_db_manager.remove_from_failed_log(cursor, item_id)
                conn.commit()

            logger.info(f"  âœ æ‰‹åŠ¨å¤„ç† '{item_name}' æµç¨‹å®Œæˆã€‚")
            return True

        except Exception as e:
            logger.error(f"  âœ æ‰‹åŠ¨å¤„ç† '{item_name}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
            return False
        finally:
            if item_id in self.manual_edit_cache:
                del self.manual_edit_cache[item_id]
                logger.trace(f"å·²æ¸…ç† ItemID {item_id} çš„æ‰‹åŠ¨ç¼–è¾‘ä¼šè¯ç¼“å­˜ã€‚")
    
    # --- ä¸ºå‰ç«¯å‡†å¤‡æ¼”å‘˜åˆ—è¡¨ç”¨äºç¼–è¾‘ ---
    def get_cast_for_editing(self, item_id: str) -> Optional[Dict[str, Any]]:
        """
        ã€V2 - Overrideæ–‡ä»¶ä¸­å¿ƒåŒ–ç‰ˆã€‘
        é‡æ„æ•°æ®æºï¼Œç¡®ä¿å‰ç«¯è·å–å’Œç¼–è¾‘çš„æ¼”å‘˜åˆ—è¡¨ï¼Œä¸ override æ–‡ä»¶ä¸­çš„â€œçœŸç†ä¹‹æºâ€å®Œå…¨ä¸€è‡´ã€‚
        - æ¼”å‘˜è¡¨ä¸»ä½“(åå­—, è§’è‰², é¡ºåº) æ¥è‡ª override ä¸»JSONæ–‡ä»¶ã€‚
        - é€šè¿‡ä¸€æ¬¡ Emby API è°ƒç”¨æ¥è·å– emby_person_id å¹¶è¿›è¡Œæ˜ å°„ã€‚
        """
        logger.info(f"  âœ ä¸ºç¼–è¾‘é¡µé¢å‡†å¤‡æ•°æ®ï¼šItemID {item_id}")
        
        try:
            # æ­¥éª¤ 1: è·å– Emby åŸºç¡€è¯¦æƒ… å’Œ ç”¨äºIDæ˜ å°„çš„Peopleåˆ—è¡¨
            emby_details = emby.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id)
            if not emby_details:
                raise ValueError(f"åœ¨Embyä¸­æœªæ‰¾åˆ°é¡¹ç›® {item_id}")

            item_name_for_log = emby_details.get("Name", f"æœªçŸ¥(ID:{item_id})")
            tmdb_id = emby_details.get("ProviderIds", {}).get("Tmdb")
            item_type = emby_details.get("Type")
            if not tmdb_id:
                raise ValueError(f"é¡¹ç›® '{item_name_for_log}' ç¼ºå°‘ TMDb IDï¼Œæ— æ³•å®šä½å…ƒæ•°æ®æ–‡ä»¶ã€‚")

            # æ­¥éª¤ 2: è¯»å– override æ–‡ä»¶ï¼Œè·å–æƒå¨æ¼”å‘˜è¡¨
            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
            main_json_filename = "all.json" if item_type == "Movie" else "series.json"
            main_json_path = os.path.join(target_override_dir, main_json_filename)

            if not os.path.exists(main_json_path):
                raise FileNotFoundError(f"æ— æ³•ä¸º '{item_name_for_log}' å‡†å¤‡ç¼–è¾‘æ•°æ®ï¼šæ‰¾ä¸åˆ°ä¸»å…ƒæ•°æ®æ–‡ä»¶ '{main_json_path}'ã€‚è¯·ç¡®ä¿è¯¥é¡¹ç›®å·²è¢«è‡³å°‘å¤„ç†è¿‡ä¸€æ¬¡ã€‚")

            with open(main_json_path, 'r', encoding='utf-8') as f:
                override_data = json.load(f)
            
            cast_from_override = (override_data.get('casts', {}) or override_data.get('credits', {})).get('cast', [])
            logger.debug(f"  âœ æˆåŠŸä» override æ–‡ä»¶ä¸º '{item_name_for_log}' åŠ è½½äº† {len(cast_from_override)} ä½æ¼”å‘˜ã€‚")

            # æ­¥éª¤ 3: æ„å»º TMDb ID -> emby_person_id çš„æ˜ å°„
            tmdb_to_emby_map = {}
            for person in emby_details.get("People", []):
                person_tmdb_id = (person.get("ProviderIds") or {}).get("Tmdb")
                if person_tmdb_id:
                    tmdb_to_emby_map[str(person_tmdb_id)] = person.get("Id")
            
            # æ­¥éª¤ 4: ç»„è£…æœ€ç»ˆæ•°æ® (åˆå¹¶ override å†…å®¹ å’Œ emby_person_id)
            cast_for_frontend = []
            session_cache_map = {}
            
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                for actor_data in cast_from_override:
                    actor_tmdb_id = actor_data.get('id')
                    if not actor_tmdb_id: continue
                    
                    emby_person_id = tmdb_to_emby_map.get(str(actor_tmdb_id))
                    
                    # ä»æœ¬åœ°æ•°æ®åº“è·å–å¤´åƒ
                    image_url = None
                    # actor_data å°±æ˜¯ä» override æ–‡ä»¶é‡Œè¯»å‡ºçš„é‚£æ¡è®°å½•ï¼Œå®ƒåŒ…å«äº†æœ€å‡†ç¡®çš„ profile_path
                    profile_path = actor_data.get("profile_path")
                    if profile_path:
                        # å¦‚æœæ˜¯å®Œæ•´çš„ URL (æ¥è‡ªè±†ç“£)ï¼Œåˆ™ç›´æ¥ä½¿ç”¨
                        if profile_path.startswith('http'):
                            image_url = profile_path
                        # å¦åˆ™ï¼Œè®¤ä¸ºæ˜¯ TMDb çš„ç›¸å¯¹è·¯å¾„ï¼Œè¿›è¡Œæ‹¼æ¥
                        else:
                            image_url = f"https://image.tmdb.org/t/p/w185{profile_path}"
                    
                    # æ¸…ç†è§’è‰²å
                    original_role = actor_data.get('character', '')
                    session_cache_map[str(actor_tmdb_id)] = original_role
                    cleaned_role_for_display = utils.clean_character_name_static(original_role)

                    # ä¸ºå‰ç«¯å‡†å¤‡çš„æ•°æ®
                    cast_for_frontend.append({
                        "tmdbId": actor_tmdb_id,
                        "name": actor_data.get('name'),
                        "role": cleaned_role_for_display,
                        "imageUrl": image_url,
                        "emby_person_id": emby_person_id
                    })
                    
            # æ­¥éª¤ 5: ç¼“å­˜ä¼šè¯æ•°æ®å¹¶å‡†å¤‡æœ€ç»ˆå“åº”
            self.manual_edit_cache[item_id] = session_cache_map
            logger.debug(f"å·²ä¸º ItemID {item_id} ç¼“å­˜äº† {len(session_cache_map)} æ¡ç”¨äºæ‰‹åŠ¨ç¼–è¾‘ä¼šè¯çš„æ¼”å‘˜æ•°æ®ã€‚")

            failed_log_info = {}
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT error_message, score FROM failed_log WHERE item_id = %s", (item_id,))
                row = cursor.fetchone()
                if row: failed_log_info = dict(row)

            response_data = {
                "item_id": item_id,
                "item_name": emby_details.get("Name"),
                "item_type": emby_details.get("Type"),
                "image_tag": emby_details.get('ImageTags', {}).get('Primary'),
                "original_score": failed_log_info.get("score"),
                "review_reason": failed_log_info.get("error_message"),
                "current_emby_cast": cast_for_frontend,
                "search_links": {
                    "baidu": utils.generate_search_url('baike', emby_details.get("Name"), emby_details.get("ProductionYear")),
                    "wikipedia": utils.generate_search_url('wikipedia', emby_details.get("Name"), emby_details.get("ProductionYear")),
                    "google": utils.generate_search_url('google', emby_details.get("Name"), emby_details.get("ProductionYear"))
                }
            }
            return response_data

        except Exception as e:
            logger.error(f"  âœ è·å–ç¼–è¾‘æ•°æ®å¤±è´¥ for ItemID {item_id}: {e}", exc_info=True)
            return None
    
    # --- å®æ—¶è¦†ç›–ç¼“å­˜åŒæ­¥ ---
    def sync_single_item_assets(self, item_id: str, 
                                update_description: Optional[str] = None, 
                                sync_timestamp_iso: Optional[str] = None,
                                final_cast_override: Optional[List[Dict[str, Any]]] = None,
                                episode_ids_to_sync: Optional[List[str]] = None):
        """
        çº¯ç²¹çš„é¡¹ç›®ç»ç†ï¼Œè´Ÿè´£æ¥æ”¶è®¾è®¡å¸ˆçš„æ‰€æœ‰ææ–™ï¼Œå¹¶åˆ†å‘ç»™æ–½å·¥é˜Ÿã€‚
        """
        log_prefix = f"å®æ—¶è¦†ç›–ç¼“å­˜åŒæ­¥"
        logger.trace(f"--- {log_prefix} å¼€å§‹æ‰§è¡Œ (ItemID: {item_id}) ---")

        if not self.local_data_path:
            logger.warning(f"  âœ {log_prefix} ä»»åŠ¡è·³è¿‡ï¼Œå› ä¸ºæœªé…ç½®æœ¬åœ°æ•°æ®æºè·¯å¾„ã€‚")
            return

        try:
            item_details = emby.get_emby_item_details(
                item_id, self.emby_url, self.emby_api_key, self.emby_user_id,
                fields="ProviderIds,Type,Name,IndexNumber,ParentIndexNumber"
            )
            if not item_details:
                raise ValueError("åœ¨Embyä¸­æ‰¾ä¸åˆ°è¯¥é¡¹ç›®ã€‚")

            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            if not tmdb_id:
                logger.warning(f"{log_prefix} é¡¹ç›® '{item_details.get('Name')}' ç¼ºå°‘TMDb IDï¼Œæ— æ³•åŒæ­¥ã€‚")
                return

            # 1. è°ƒåº¦å¤–å¢™æ–½å·¥é˜Ÿ
            self.sync_item_images(item_details, update_description, episode_ids_to_sync=episode_ids_to_sync)
            
            # 2. è°ƒåº¦ç²¾è£…ä¿®æ–½å·¥é˜Ÿï¼Œå¹¶æŠŠæ‰€æœ‰å›¾çº¸å’Œææ–™éƒ½ç»™ä»–
            self.sync_item_metadata(
                item_details, 
                tmdb_id, 
                final_cast_override=final_cast_override, 
                episode_ids_to_sync=episode_ids_to_sync
            )

            # 3. è®°å½•å·¥æ—¶
            timestamp_to_log = sync_timestamp_iso or datetime.now(timezone.utc).isoformat()
            with get_central_db_connection() as conn:
                cursor = conn.cursor()
                self.log_db_manager.mark_assets_as_synced(
                    cursor, 
                    item_id, 
                    timestamp_to_log
                )
                conn.commit()
            
            logger.trace(f"--- {log_prefix} æˆåŠŸå®Œæˆ (ItemID: {item_id}) ---")

        except Exception as e:
            logger.error(f"{log_prefix} æ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯ (ItemID: {item_id}): {e}", exc_info=True)

    # --- å¤‡ä»½å›¾ç‰‡ ---
    def sync_item_images(self, item_details: Dict[str, Any], update_description: Optional[str] = None, episode_ids_to_sync: Optional[List[str]] = None) -> bool:
        """
        ã€æ–°å¢-é‡æ„ã€‘è¿™ä¸ªæ–¹æ³•è´Ÿè´£åŒæ­¥ä¸€ä¸ªåª’ä½“é¡¹ç›®çš„æ‰€æœ‰ç›¸å…³å›¾ç‰‡ã€‚
        å®ƒä» _process_item_core_logic ä¸­æå–å‡ºæ¥ï¼Œä»¥ä¾¿å¤ç”¨ã€‚
        """
        item_id = item_details.get("Id")
        item_type = item_details.get("Type")
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        
        if not all([item_id, item_type, self.local_data_path]):
            logger.error(f"  âœ è·³è¿‡ '{item_name_for_log}'ï¼Œå› ä¸ºç¼ºå°‘IDã€ç±»å‹æˆ–æœªé…ç½®æœ¬åœ°æ•°æ®è·¯å¾„ã€‚")
            return False

        try:
            # --- å‡†å¤‡å·¥ä½œ (ç›®å½•ã€TMDb IDç­‰) ---
            log_prefix = "è¦†ç›–ç¼“å­˜-å›¾ç‰‡å¤‡ä»½ï¼š"
            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            if not tmdb_id:
                logger.warning(f"  âœ {log_prefix} é¡¹ç›® '{item_name_for_log}' ç¼ºå°‘TMDb IDï¼Œæ— æ³•ç¡®å®šè¦†ç›–ç›®å½•ï¼Œè·³è¿‡ã€‚")
                return False
            
            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            base_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
            image_override_dir = os.path.join(base_override_dir, "images")
            os.makedirs(image_override_dir, exist_ok=True)

            # --- å®šä¹‰æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡æ˜ å°„ ---
            full_image_map = {"Primary": "poster.jpg", "Backdrop": "fanart.jpg", "Logo": "clearlogo.png"}
            if item_type == "Movie":
                full_image_map["Thumb"] = "landscape.jpg"

            # â˜…â˜…â˜… å…¨æ–°é€»è¾‘åˆ†å‘ â˜…â˜…â˜…
            images_to_sync = {}
            
            # æ¨¡å¼ä¸€ï¼šç²¾å‡†åŒæ­¥ (å½“æè¿°å­˜åœ¨æ—¶)
            if update_description:
                log_prefix = "[è¦†ç›–ç¼“å­˜-å›¾ç‰‡å¤‡ä»½]"
                logger.trace(f"{log_prefix} æ­£åœ¨è§£ææè¿°: '{update_description}'")
                
                # å®šä¹‰å…³é”®è¯åˆ°Embyå›¾ç‰‡ç±»å‹çš„æ˜ å°„ (ä½¿ç”¨å°å†™ä»¥æ–¹ä¾¿åŒ¹é…)
                keyword_map = {
                    "primary": "Primary",
                    "backdrop": "Backdrop",
                    "logo": "Logo",
                    "thumb": "Thumb", # ç”µå½±ç¼©ç•¥å›¾
                    "banner": "Banner" # å‰§é›†æ¨ªå¹… (å¦‚æœéœ€è¦å¯ä»¥æ·»åŠ )
                }
                
                desc_lower = update_description.lower()
                found_specific_image = False
                for keyword, image_type_api in keyword_map.items():
                    if keyword in desc_lower and image_type_api in full_image_map:
                        images_to_sync[image_type_api] = full_image_map[image_type_api]
                        logger.trace(f"{log_prefix} åŒ¹é…åˆ°å…³é”®è¯ '{keyword}'ï¼Œå°†åªåŒæ­¥ {image_type_api} å›¾ç‰‡ã€‚")
                        found_specific_image = True
                        break # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…å°±åœæ­¢ï¼Œé¿å…é‡å¤
                
                if not found_specific_image:
                    logger.trace(f"{log_prefix} æœªèƒ½åœ¨æè¿°ä¸­æ‰¾åˆ°å¯è¯†åˆ«çš„å›¾ç‰‡å…³é”®è¯ï¼Œå°†å›é€€åˆ°å®Œå…¨åŒæ­¥ã€‚")
                    images_to_sync = full_image_map # å›é€€
            
            # æ¨¡å¼äºŒï¼šå®Œå…¨åŒæ­¥ (é»˜è®¤æˆ–å›é€€)
            else:
                log_prefix = "[è¦†ç›–ç¼“å­˜-å›¾ç‰‡å¤‡ä»½]"
                logger.trace(f"  âœ {log_prefix} æœªæä¾›æ›´æ–°æè¿°ï¼Œå°†åŒæ­¥æ‰€æœ‰ç±»å‹çš„å›¾ç‰‡ã€‚")
                images_to_sync = full_image_map

            # --- æ‰§è¡Œä¸‹è½½ ---
            if not episode_ids_to_sync:
                logger.info(f"  âœ {log_prefix} å¼€å§‹ä¸º '{item_name_for_log}' ä¸‹è½½ {len(images_to_sync)} å¼ ä¸»å›¾ç‰‡è‡³è¦†ç›–ç¼“å­˜")
                for image_type, filename in images_to_sync.items():
                    if self.is_stop_requested():
                        logger.warning(f"  ğŸš« {log_prefix} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ­¢å›¾ç‰‡ä¸‹è½½ã€‚")
                        return False
                    emby.download_emby_image(item_id, image_type, os.path.join(image_override_dir, filename), self.emby_url, self.emby_api_key)
            
            # --- åˆ†é›†å›¾ç‰‡é€»è¾‘ ---
            if item_type == "Series":
                children_to_process = []
                # è·å–æ‰€æœ‰å­é¡¹ä¿¡æ¯ï¼Œç”¨äºæŸ¥æ‰¾
                all_children = emby.get_series_children(item_id, self.emby_url, self.emby_api_key, self.emby_user_id, series_name_for_log=item_name_for_log) or []
                
                if episode_ids_to_sync:
                    # æ¨¡å¼ä¸€ï¼šåªå¤„ç†æŒ‡å®šçš„åˆ†é›†
                    logger.info(f"  âœ {log_prefix} å°†åªåŒæ­¥ {len(episode_ids_to_sync)} ä¸ªæŒ‡å®šåˆ†é›†çš„å›¾ç‰‡ã€‚")
                    id_set = set(episode_ids_to_sync)
                    children_to_process = [child for child in all_children if child.get("Id") in id_set]
                elif images_to_sync == full_image_map:
                    # æ¨¡å¼äºŒï¼šå¤„ç†æ‰€æœ‰å­é¡¹ï¼ˆåŸé€»è¾‘ï¼‰
                    children_to_process = all_children

                for child in children_to_process:
                    if self.is_stop_requested():
                        logger.warning(f"  ğŸš« {log_prefix} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ­¢å­é¡¹ç›®å›¾ç‰‡ä¸‹è½½ã€‚")
                        return False
                    child_type, child_id = child.get("Type"), child.get("Id")
                    if child_type == "Season":
                        season_number = child.get("IndexNumber")
                        if season_number is not None:
                            emby.download_emby_image(child_id, "Primary", os.path.join(image_override_dir, f"season-{season_number}.jpg"), self.emby_url, self.emby_api_key)
                    elif child_type == "Episode":
                        season_number, episode_number = child.get("ParentIndexNumber"), child.get("IndexNumber")
                        if season_number is not None and episode_number is not None:
                            emby.download_emby_image(child_id, "Primary", os.path.join(image_override_dir, f"season-{season_number}-episode-{episode_number}.jpg"), self.emby_url, self.emby_api_key)
            
            logger.trace(f"  âœ {log_prefix} æˆåŠŸå®Œæˆ '{item_name_for_log}' çš„è¦†ç›–ç¼“å­˜-å›¾ç‰‡å¤‡ä»½ã€‚")
            return True
        except Exception as e:
            logger.error(f"{log_prefix} ä¸º '{item_name_for_log}' å¤‡ä»½å›¾ç‰‡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return False
    
    # --- å¤‡ä»½å…ƒæ•°æ® ---
    def sync_item_metadata(self, item_details: Dict[str, Any], tmdb_id: str,
                       final_cast_override: Optional[List[Dict[str, Any]]] = None,
                       episode_ids_to_sync: Optional[List[str]] = None):
        """
        ã€V4 - ç²¾è£…ä¿®æ–½å·¥é˜Ÿæœ€ç»ˆç‰ˆã€‘
        æœ¬å‡½æ•°æ˜¯å”¯ä¸€çš„æ–½å·¥é˜Ÿï¼Œè´Ÿè´£æ‰€æœ‰ override æ–‡ä»¶çš„è¯»å†™æ“ä½œã€‚
        """
        item_id = item_details.get("Id")
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        item_type = item_details.get("Type")
        log_prefix = "[è¦†ç›–ç¼“å­˜-å…ƒæ•°æ®å†™å…¥]"

        # å®šä¹‰æ ¸å¿ƒè·¯å¾„
        cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
        source_cache_dir = os.path.join(self.local_data_path, "cache", cache_folder_name, tmdb_id)
        target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
        main_json_filename = "all.json" if item_type == "Movie" else "series.json"
        main_json_path = os.path.join(target_override_dir, main_json_filename)

        # æ­¥éª¤ 1: è¿›åœºæ–½å·¥ï¼Œæ‰“å¥½åŸºç¡€ (å¤åˆ¶æ¯›å¯æˆ¿)
        # åªæœ‰åœ¨éœ€è¦è¿›è¡Œä¸»ä½“è£…ä¿®æ—¶ï¼ˆä¸»æµç¨‹è°ƒç”¨ï¼‰ï¼Œæ‰éœ€è¦å¤åˆ¶ã€‚è¿½æ›´ç­‰é›¶æ´»ä¸éœ€è¦ã€‚
        if final_cast_override is not None:
            logger.info(f"  âœ {log_prefix} å¼€å§‹ä¸º '{item_name_for_log}' å†™å…¥è¦†ç›–ç¼“å­˜...")
            if not os.path.exists(source_cache_dir):
                logger.error(f"  âœ {log_prefix} æ‰¾ä¸åˆ°æºç¼“å­˜ç›®å½•ï¼è·¯å¾„: {source_cache_dir}")
                return
            try:
                shutil.copytree(source_cache_dir, target_override_dir, dirs_exist_ok=True)
            except Exception as e:
                logger.error(f"  âœ {log_prefix} å¤åˆ¶å…ƒæ•°æ®æ—¶å¤±è´¥: {e}", exc_info=True)
                return

        perfect_cast_for_injection = []
        if final_cast_override is not None:
            # --- è§’è‰²ä¸€ï¼šä¸»ä½“ç²¾è£…ä¿® ---
            new_cast_for_json = self._build_cast_from_final_data(final_cast_override)
            
            perfect_cast_for_injection = new_cast_for_json

            # æ­¥éª¤ 2: ä¿®æ”¹ä¸»æ–‡ä»¶
            with open(main_json_path, 'r+', encoding='utf-8') as f:
                data = json.load(f)
                if 'casts' in data: data['casts']['cast'] = perfect_cast_for_injection
                else: data.setdefault('credits', {})['cast'] = perfect_cast_for_injection
                
                f.seek(0)
                json.dump(data, f, ensure_ascii=False, indent=2)
                f.truncate()
        else:
            # --- è§’è‰²äºŒï¼šé›¶æ´»å¤„ç† (è¿½æ›´) ---
            logger.info(f"  âœ {log_prefix} [è¿½æ›´] å¼€å§‹ä¸º '{item_name_for_log}' çš„æ–°åˆ†é›†å†™å…¥è¦†ç›–ç¼“å­˜...")
            if not os.path.exists(main_json_path):
                logger.error(f"  âœ {log_prefix} è¿½æ›´ä»»åŠ¡å¤±è´¥ï¼šæ‰¾ä¸åˆ°ä¸»å…ƒæ•°æ®æ–‡ä»¶ '{main_json_path}'ã€‚")
                return
            try:
                with open(main_json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    perfect_cast_for_injection = (data.get('casts', {}) or data.get('credits', {})).get('cast', [])
            except Exception as e:
                logger.error(f"  âœ {log_prefix} è¯»å–ä¸»å…ƒæ•°æ®æ–‡ä»¶ '{main_json_path}' æ—¶å¤±è´¥: {e}", exc_info=True)
                return

        # æ­¥éª¤ 3: å…¬å…±æ–½å·¥ - æ³¨å…¥åˆ†é›†æ–‡ä»¶
        if item_type == "Series" and perfect_cast_for_injection:
            self._inject_cast_to_series_files(
                target_dir=target_override_dir, 
                cast_list=perfect_cast_for_injection, 
                series_details=item_details, 
                source_dir=source_cache_dir,  
                episode_ids_to_sync=episode_ids_to_sync
            )

    # --- è¾…åŠ©å‡½æ•°ï¼šä»ä¸åŒæ•°æ®æºæ„å»ºæ¼”å‘˜åˆ—è¡¨ ---
    def _build_cast_from_final_data(self, final_cast_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¾…åŠ©å‡½æ•°ï¼šä»ä¸»æµç¨‹çš„æœ€ç»ˆç»“æœæ„å»ºæ¼”å‘˜åˆ—è¡¨"""
        cast_list = []
        for i, actor_info in enumerate(final_cast_data):
            if not actor_info.get("id"): continue
            cast_list.append({
                "id": actor_info.get("id"), "name": actor_info.get("name"), "character": actor_info.get("character"),
                "original_name": actor_info.get("original_name"), "profile_path": actor_info.get("profile_path"),
                "adult": actor_info.get("adult", False), "gender": actor_info.get("gender", 0),
                "known_for_department": actor_info.get("known_for_department", "Acting"),
                "popularity": actor_info.get("popularity", 0.0), "cast_id": actor_info.get("cast_id"),
                "credit_id": actor_info.get("credit_id"), "order": actor_info.get("order", i)
            })
        return cast_list

    # --- è¾…åŠ©å‡½æ•°ï¼šå°†æ¼”å‘˜è¡¨æ³¨å…¥å‰§é›†çš„å­£/é›†JSONæ–‡ä»¶ ---
    def _inject_cast_to_series_files(self, target_dir: str, cast_list: List[Dict[str, Any]], series_details: Dict[str, Any], source_dir: str, episode_ids_to_sync: Optional[List[str]] = None):
        """
        è¾…åŠ©å‡½æ•°ï¼šå°†æ¼”å‘˜è¡¨æ³¨å…¥å‰§é›†çš„å­£/é›†JSONæ–‡ä»¶ã€‚
        """
        log_prefix = "[è¦†ç›–ç¼“å­˜-å…ƒæ•°æ®å†™å…¥]"
        if cast_list is not None:
            logger.info(f"  âœ {log_prefix} å¼€å§‹å°†æ¼”å‘˜è¡¨æ³¨å…¥æ‰€æœ‰å­£/é›†å¤‡ä»½æ–‡ä»¶...")
        else:
            logger.info(f"  âœ {log_prefix} å¼€å§‹å°†å®æ—¶å…ƒæ•°æ®ï¼ˆæ ‡é¢˜/ç®€ä»‹ï¼‰åŒæ­¥åˆ°æ‰€æœ‰å­£/é›†å¤‡ä»½æ–‡ä»¶...")
        
        children_from_emby = emby.get_series_children(
            series_id=series_details.get("Id"), base_url=self.emby_url,
            api_key=self.emby_api_key, user_id=self.emby_user_id,
            series_name_for_log=series_details.get("Name")
        ) or []

        child_data_map = {}
        for child in children_from_emby:
            key = None
            if child.get("Type") == "Season": key = f"season-{child.get('IndexNumber')}"
            elif child.get("Type") == "Episode": key = f"season-{child.get('ParentIndexNumber')}-episode-{child.get('IndexNumber')}"
            if key: child_data_map[key] = child

        updated_children_count = 0
        try:
            files_to_process = set() # ä½¿ç”¨é›†åˆå»é‡
                
            if episode_ids_to_sync:
                id_set = set(episode_ids_to_sync)
                for child in children_from_emby:
                    # å¦‚æœæ˜¯ç›®æ ‡åˆ†é›†
                    if child.get("Id") in id_set and child.get("Type") == "Episode":
                        s_num = child.get('ParentIndexNumber')
                        e_num = child.get('IndexNumber')
                        
                        if s_num is not None:
                            # 1. æ·»åŠ åˆ†é›†æ–‡ä»¶
                            if e_num is not None:
                                files_to_process.add(f"season-{s_num}-episode-{e_num}.json")
                            
                            # 2. â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ï¼šé¡ºä¾¿æŠŠè¯¥åˆ†é›†æ‰€å±çš„â€œå­£â€æ–‡ä»¶ä¹ŸåŠ è¿›å» â˜…â˜…â˜…
                            files_to_process.add(f"season-{s_num}.json")
            else:
                # å…¨é‡æ¨¡å¼/å…ƒæ•°æ®åŒæ­¥æ¨¡å¼ï¼š
                # æ”¹ä¸ºéå† Emby ä¸­çš„æ‰€æœ‰å­é¡¹ï¼Œè€Œä¸æ˜¯åªçœ‹æœ¬åœ°æœ‰ä»€ä¹ˆæ–‡ä»¶ã€‚
                # è¿™æ ·å¯ä»¥ç¡®ä¿ï¼šå¦‚æœ Emby æœ‰æ–°å­£/é›†ï¼Œè€Œæœ¬åœ° override ç¼ºæ–‡ä»¶ï¼Œä¼šè‡ªåŠ¨ä» source è¡¥é½ã€‚
                for key in child_data_map.keys():
                    files_to_process.add(f"{key}.json")

            # è½¬å›åˆ—è¡¨å¹¶æ’åºï¼Œä¿è¯å¤„ç†é¡ºåºä¸€è‡´
            sorted_files_to_process = sorted(list(files_to_process))

            for filename in sorted_files_to_process:
                child_json_path = os.path.join(target_dir, filename)
                
                # â–¼â–¼â–¼ æ ¸å¿ƒä¿®æ”¹ 2/3: æ£€æŸ¥-å¤åˆ¶-ä¿®æ”¹ é€»è¾‘ â–¼â–¼â–¼
                if not os.path.exists(child_json_path):
                    source_json_path = os.path.join(source_dir, filename)
                    if os.path.exists(source_json_path):
                        logger.debug(f"  âœ æ­£åœ¨å¤åˆ¶å…ƒæ•°æ®æ–‡ä»¶ '{filename}'")
                        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                        os.makedirs(os.path.dirname(child_json_path), exist_ok=True)
                        shutil.copy2(source_json_path, child_json_path)
                    else:
                        logger.warning(f"  âœ è·³è¿‡æ³¨å…¥ '{filename}'ï¼Œå› ä¸ºå®ƒåœ¨æºç¼“å­˜å’Œè¦†ç›–ç¼“å­˜ä¸­éƒ½ä¸å­˜åœ¨ã€‚")
                        continue
                
                try:
                    with open(child_json_path, 'r+', encoding='utf-8') as f_child:
                        child_data = json.load(f_child)
                        
                        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šæ¡ä»¶æ€§åœ°æ›´æ–°æ¼”å‘˜è¡¨ â˜…â˜…â˜…
                        if cast_list is not None and 'credits' in child_data and 'cast' in child_data['credits']:
                            child_data['credits']['cast'] = cast_list
                        
                        # æ— è®ºå¦‚ä½•éƒ½æ›´æ–°å…ƒæ•°æ®
                        file_key = os.path.splitext(filename)[0]
                        fresh_data = child_data_map.get(file_key)
                        if fresh_data:
                            child_data['name'] = fresh_data.get('Name', child_data.get('name'))
                            child_data['overview'] = fresh_data.get('Overview', child_data.get('overview'))
                        
                        f_child.seek(0)
                        json.dump(child_data, f_child, ensure_ascii=False, indent=2)
                        f_child.truncate()
                        updated_children_count += 1
                except Exception as e_child:
                    logger.warning(f"  âœ æ›´æ–°å­æ–‡ä»¶ '{filename}' æ—¶å¤±è´¥: {e_child}")
            logger.info(f"  âœ {log_prefix} æˆåŠŸå°†å…ƒæ•°æ®æ³¨å…¥äº† {updated_children_count} ä¸ªå­£/é›†æ–‡ä»¶ã€‚")
        except Exception as e_list:
            logger.error(f"  âœ {log_prefix} éå†å¹¶æ›´æ–°å­£/é›†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e_list}", exc_info=True)

    # æå–æ ‡ç­¾
    def extract_tag_names(item_data):
        """
        å…¼å®¹æ–°æ—§ç‰ˆ Emby API æå–æ ‡ç­¾åã€‚
        """
        tags_set = set()

        # 1. å°è¯•æå– TagItems (æ–°ç‰ˆ/è¯¦ç»†ç‰ˆ)
        tag_items = item_data.get('TagItems')
        if isinstance(tag_items, list):
            for t in tag_items:
                if isinstance(t, dict):
                    name = t.get('Name')
                    if name:
                        tags_set.add(name)
                elif isinstance(t, str) and t:
                    tags_set.add(t)
        
        # 2. å°è¯•æå– Tags (æ—§ç‰ˆ/ç®€ç•¥ç‰ˆ)
        tags = item_data.get('Tags')
        if isinstance(tags, list):
            for t in tags:
                if t:
                    tags_set.add(str(t))
        
        return list(tags_set)

    # --- ä¸ºä¸€ä¸ªåª’ä½“é¡¹åŒæ­¥å…ƒæ•°æ®ç¼“å­˜ ---
    def sync_single_item_to_metadata_cache(self, item_id: str, item_name: Optional[str] = None):
        """
        ã€V12 - æç®€ç‰ˆã€‘
        ä»…ç”¨äºå“åº” 'metadata.update' äº‹ä»¶ã€‚
        å°† Emby ä¸­çš„æœ€æ–°å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€ç®€ä»‹ã€æ ‡ç­¾ç­‰ï¼‰å¿«é€Ÿé•œåƒåˆ°æœ¬åœ°æ•°æ®åº“ã€‚
        
        æ³¨æ„ï¼š'è¿½æ›´/æ–°åˆ†é›†å…¥åº“' ä¸å†ä½¿ç”¨æ­¤å‡½æ•°ï¼Œè€Œæ˜¯èµ° process_single_item -> _upsert_media_metadata æµç¨‹ã€‚
        """
        log_prefix = f"å®æ—¶åŒæ­¥åª’ä½“å…ƒæ•°æ® '{item_name}'"
        # logger.trace(f"  âœ {log_prefix} å¼€å§‹æ‰§è¡Œ...")
        
        try:
            # 1. è·å– Emby æœ€æ–°è¯¦æƒ…
            # ä¸éœ€è¦è¯·æ±‚ MediaSources ç­‰é‡å‹å­—æ®µï¼Œåªéœ€è¦å…ƒæ•°æ®
            fields_to_get = "ProviderIds,Type,Name,OriginalTitle,Overview,Tags,TagItems,OfficialRating,Path,_SourceLibraryId,PremiereDate,ProductionYear"
            item_details = emby.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id, fields=fields_to_get)
            
            if not item_details:
                logger.warning(f"  âœ {log_prefix} æ— æ³•è·å–è¯¦æƒ…ï¼Œè·³è¿‡ã€‚")
                return
            
            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            item_type = item_details.get("Type")
            
            if not tmdb_id or item_type not in ['Movie', 'Series', 'Season', 'Episode']:
                # ä»…åŒæ­¥æˆ‘ä»¬å…³å¿ƒçš„ç±»å‹
                return
            
            # è¡¥å…¨ Library ID
            if not item_details.get('_SourceLibraryId'):
                lib_info = emby.get_library_root_for_item(item_id, self.emby_url, self.emby_api_key, self.emby_user_id)
                if lib_info: item_details['_SourceLibraryId'] = lib_info.get('Id')

            # 2. ç›´æ¥æ›´æ–°æ•°æ®åº“
            with get_central_db_connection() as conn:
                with conn.cursor() as cursor:
                    final_tags = extract_tag_names(item_details)
                    
                    # åŸºç¡€å­—æ®µæ›´æ–°
                    updates = {
                        "title": item_details.get('Name'),
                        "original_title": item_details.get('OriginalTitle'),
                        "overview": item_details.get('Overview'),
                        "tags_json": json.dumps(final_tags, ensure_ascii=False),
                        "last_synced_at": datetime.now(timezone.utc)
                    }
                    
                    # æ—¥æœŸå­—æ®µå¤„ç†
                    if item_details.get('PremiereDate'):
                        updates["release_date"] = item_details['PremiereDate']
                    if item_details.get('ProductionYear'):
                        updates["release_year"] = item_details['ProductionYear']

                    # é’ˆå¯¹ç”µå½±ï¼Œæ›´æ–°èµ„äº§è¯¦æƒ… (è·¯å¾„ç­‰)
                    if item_type == 'Movie':
                        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦é‡æ–°è·å–ä¸€æ¬¡å¸¦ MediaSources çš„è¯¦æƒ…ï¼Œæˆ–è€…ä¸Šé¢çš„ fields_to_get åŠ ä¸Š MediaSources
                        # ä¸ºäº†è½»é‡åŒ–ï¼Œå¦‚æœåªæ˜¯æ”¹æ ‡é¢˜ï¼Œå…¶å®ä¸éœ€è¦æ›´æ–° asset_detailsã€‚
                        # ä½†ä¸ºäº†ä¸¥è°¨ï¼Œå¦‚æœç”¨æˆ·æ”¹äº†è·¯å¾„ï¼Œè¿™é‡Œæœ€å¥½ä¹Ÿæ›´æ–°ä¸€ä¸‹ã€‚
                        # é‰´äº metadata.update å¾ˆå°‘æ¶‰åŠè·¯å¾„å˜æ›´ï¼Œè¿™é‡Œå¯ä»¥é€‰æ‹©æ€§å¿½ç•¥ asset_details çš„æ›´æ–°ï¼Œ
                        # æˆ–è€…ä¸ºäº†ä¿é™©èµ·è§ï¼Œä¿æŒåŸæœ‰çš„ asset_details æ›´æ–°é€»è¾‘ã€‚
                        pass 
                    
                    # æ„å»º SQL
                    set_clauses = [f"{key} = %s" for key in updates.keys()]
                    sql = f"UPDATE media_metadata SET {', '.join(set_clauses)} WHERE tmdb_id = %s AND item_type = %s"
                    
                    cursor.execute(sql, tuple(updates.values()) + (tmdb_id, item_type))
                    
                    # å¦‚æœæ˜¯å‰§é›†ï¼Œä¸” Emby æ”¹äº†åå­—ï¼Œå¯èƒ½éœ€è¦çº§è”æ›´æ–°åˆ†é›†å—ï¼Ÿ
                    # é€šå¸¸ä¸éœ€è¦ï¼Œåˆ†é›†æœ‰è‡ªå·±çš„è®°å½•ã€‚å¦‚æœéœ€è¦ï¼Œé‚£æ˜¯å…¨é‡åˆ·æ–°çš„äº‹äº†ã€‚
                    
                    conn.commit()
            
            logger.info(f"  âœ {log_prefix} æ•°æ®åº“åŒæ­¥å®Œæˆã€‚")

        except Exception as e:
            logger.error(f"{log_prefix} æ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

    # --- å°†æ¥è‡ª Emby çš„å®æ—¶å…ƒæ•°æ®æ›´æ–°åŒæ­¥åˆ° override ç¼“å­˜æ–‡ä»¶ ---
    def sync_emby_updates_to_override_files(self, item_details: Dict[str, Any]):
        """
        å°†æ¥è‡ª Emby çš„å®æ—¶å…ƒæ•°æ®æ›´æ–°åŒæ­¥åˆ° override ç¼“å­˜æ–‡ä»¶ã€‚
        è¿™æ˜¯ä¸€ä¸ª "è¯»-æ”¹-å†™" æ“ä½œï¼Œç”¨äºæŒä¹…åŒ–ç”¨æˆ·åœ¨ Emby UI ä¸Šçš„ä¿®æ”¹ã€‚
        """
        item_id = item_details.get("Id")
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        item_type = item_details.get("Type")
        tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
        log_prefix = "[è¦†ç›–ç¼“å­˜-å…ƒæ•°æ®æŒä¹…åŒ–]"

        if not all([item_id, item_type, tmdb_id, self.local_data_path]):
            logger.warning(f"  âœ {log_prefix} è·³è¿‡ '{item_name_for_log}'ï¼Œç¼ºå°‘å…³é”®IDæˆ–è·¯å¾„é…ç½®ã€‚")
            return

        logger.info(f"  âœ {log_prefix} å¼€å§‹ä¸º '{item_name_for_log}' æ›´æ–°è¦†ç›–ç¼“å­˜æ–‡ä»¶...")

        # --- å®šä½ä¸»æ–‡ä»¶ ---
        cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
        target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
        main_json_filename = "all.json" if item_type == "Movie" else "series.json"
        main_json_path = os.path.join(target_override_dir, main_json_filename)

        # --- å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœ override æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯´æ˜ä»æœªè¢«å®Œæ•´å¤„ç†è¿‡ï¼Œä¸åº”ç»§ç»­ ---
        if not os.path.exists(main_json_path):
            logger.warning(f"  âœ {log_prefix} æ— æ³•æŒä¹…åŒ–ä¿®æ”¹ï¼šä¸»è¦†ç›–æ–‡ä»¶ '{main_json_path}' ä¸å­˜åœ¨ã€‚è¯·å…ˆå¯¹è¯¥é¡¹ç›®è¿›è¡Œä¸€æ¬¡å®Œæ•´å¤„ç†ã€‚")
            return

        try:
            # --- æ ¸å¿ƒçš„ "è¯»-æ”¹-å†™" é€»è¾‘ ---
            with open(main_json_path, 'r+', encoding='utf-8') as f:
                data = json.load(f)

                # å®šä¹‰è¦ä» Emby åŒæ­¥çš„å­—æ®µ
                fields_to_update = {
                    "Name": "title",
                    "OriginalTitle": "original_title",
                    "Overview": "overview",
                    "Tagline": "tagline",
                    "CommunityRating": "vote_average", # ç”¨æˆ·è¯„åˆ†
                    "OfficialRating": "official_rating",
                    "Genres": "genres",
                    "Studios": "production_companies",
                    "Tags": "keywords"
                }
                
                updated_count = 0
                for emby_key, json_key in fields_to_update.items():
                    if emby_key in item_details:
                        new_value = item_details[emby_key]
                        # ç‰¹æ®Šå¤„ç† Studios å’Œ Genres
                        if emby_key in ["Studios", "Genres"]:
                            # å‡è®¾æºæ•°æ®æ˜¯ [{ "Name": "Studio A" }] æˆ– ["Action"]
                            if isinstance(new_value, list):
                                if emby_key == "Studios":
                                     data[json_key] = [{"name": s.get("Name")} for s in new_value if s.get("Name")]
                                else: # Genres
                                     data[json_key] = new_value
                                updated_count += 1
                        else:
                            data[json_key] = new_value
                            updated_count += 1
                
                # å¤„ç†æ—¥æœŸ
                if 'PremiereDate' in item_details:
                    data['release_date'] = (item_details['PremiereDate'] or '').split('T')[0]
                    updated_count += 1

                logger.info(f"  âœ {log_prefix} å‡†å¤‡å°† {updated_count} é¡¹æ›´æ–°å†™å…¥ '{main_json_filename}'ã€‚")

                f.seek(0)
                json.dump(data, f, ensure_ascii=False, indent=2)
                f.truncate()

            # å¦‚æœæ˜¯å‰§é›†ï¼Œè¿˜éœ€è¦æ›´æ–°æ‰€æœ‰å­æ–‡ä»¶çš„ name å’Œ overview
            if item_type == "Series":
                logger.info(f"  âœ {log_prefix} æ£€æµ‹åˆ°ä¸ºå‰§é›†ï¼Œå¼€å§‹åŒæ­¥æ›´æ–°å­é¡¹ï¼ˆå­£/é›†ï¼‰çš„å…ƒæ•°æ®...")
                self._inject_cast_to_series_files(
                    target_dir=target_override_dir,
                    cast_list=None, # â˜…â˜…â˜… å…³é”®ï¼šä¼ å…¥ None è¡¨ç¤ºæˆ‘ä»¬åªæ›´æ–°å…ƒæ•°æ®ï¼Œä¸ç¢°æ¼”å‘˜è¡¨ â˜…â˜…â˜…
                    series_details=item_details,
                    source_dir=os.path.join(self.local_data_path, "cache", cache_folder_name, tmdb_id)
                )

            logger.info(f"  âœ {log_prefix} æˆåŠŸä¸º '{item_name_for_log}' æŒä¹…åŒ–äº†å…ƒæ•°æ®ä¿®æ”¹ã€‚")

        except Exception as e:
            logger.error(f"  âœ {log_prefix} ä¸º '{item_name_for_log}' æ›´æ–°è¦†ç›–ç¼“å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


    def close(self):
        if self.douban_api: self.douban_api.close()
