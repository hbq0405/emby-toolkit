# tasks/media.py
# æ ¸å¿ƒåª’ä½“å¤„ç†ã€å…ƒæ•°æ®ã€èµ„äº§åŒæ­¥ç­‰

import time
import json
import gc
import logging
from typing import Optional, List
from datetime import datetime, timezone
import concurrent.futures
from collections import defaultdict

# å¯¼å…¥éœ€è¦çš„åº•å±‚æ¨¡å—å’Œå…±äº«å®ä¾‹
import task_manager
import handler.tmdb as tmdb
import handler.emby as emby
import handler.telegram as telegram
from database import connection
from utils import translate_country_list, get_unified_rating
from .helpers import parse_full_asset_details

logger = logging.getLogger(__name__)

# â˜…â˜…â˜… ä¸­æ–‡åŒ–è§’è‰²å â˜…â˜…â˜…
def task_role_translation(processor, force_full_update: bool = False):
    """
    æ ¹æ®ä¼ å…¥çš„ force_full_update å‚æ•°ï¼Œå†³å®šæ˜¯æ‰§è¡Œæ ‡å‡†æ‰«æè¿˜æ˜¯æ·±åº¦æ›´æ–°ã€‚
    """
    # 1. æ ¹æ®å‚æ•°å†³å®šæ—¥å¿—ä¿¡æ¯
    if force_full_update:
        logger.info("  âœ å³å°†æ‰§è¡Œæ·±åº¦æ¨¡å¼ï¼Œå°†å¤„ç†æ‰€æœ‰åª’ä½“é¡¹å¹¶ä»TMDbè·å–æœ€æ–°æ•°æ®...")
    else:
        logger.info("  âœ å³å°†æ‰§è¡Œå¿«é€Ÿæ¨¡å¼ï¼Œå°†è·³è¿‡å·²å¤„ç†é¡¹...")


    # 3. è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°ï¼Œå¹¶å°† force_full_update å‚æ•°é€ä¼ ä¸‹å»
    processor.process_full_library(
        update_status_callback=task_manager.update_status_from_thread,
        force_full_update=force_full_update 
    )

# --- ä½¿ç”¨æ‰‹åŠ¨ç¼–è¾‘çš„ç»“æœå¤„ç†åª’ä½“é¡¹ ---
def task_manual_update(processor, item_id: str, manual_cast_list: list, item_name: str):
    """ä»»åŠ¡ï¼šä½¿ç”¨æ‰‹åŠ¨ç¼–è¾‘çš„ç»“æœå¤„ç†åª’ä½“é¡¹"""
    processor.process_item_with_manual_cast(
        item_id=item_id,
        manual_cast_list=manual_cast_list,
        item_name=item_name
    )

def task_sync_metadata_cache(processor, item_id: str, item_name: str, episode_ids_to_add: Optional[List[str]] = None):
    """
    ä»»åŠ¡ï¼šä¸ºå•ä¸ªåª’ä½“é¡¹åŒæ­¥å…ƒæ•°æ®åˆ° media_metadata æ•°æ®åº“è¡¨ã€‚
    å¯æ ¹æ®æ˜¯å¦ä¼ å…¥ episode_ids_to_add æ¥å†³å®šæ‰§è¡Œæ¨¡å¼ã€‚
    """
    sync_mode = "ç²¾å‡†åˆ†é›†è¿½åŠ " if episode_ids_to_add else "å¸¸è§„å…ƒæ•°æ®åˆ·æ–°"
    logger.trace(f"  âœ ä»»åŠ¡å¼€å§‹ï¼šåŒæ­¥åª’ä½“å…ƒæ•°æ®ç¼“å­˜ ({sync_mode}) for '{item_name}' (ID: {item_id})")
    try:
        processor.sync_single_item_to_metadata_cache(item_id, item_name=item_name, episode_ids_to_add=episode_ids_to_add)
        logger.trace(f"  âœ ä»»åŠ¡æˆåŠŸï¼šåŒæ­¥åª’ä½“å…ƒæ•°æ®ç¼“å­˜ for '{item_name}'")
    except Exception as e:
        logger.error(f"  âœ ä»»åŠ¡å¤±è´¥ï¼šåŒæ­¥åª’ä½“å…ƒæ•°æ®ç¼“å­˜ for '{item_name}' æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

def task_sync_images(processor, item_id: str, update_description: str, sync_timestamp_iso: str):
    """
    ä»»åŠ¡ï¼šä¸ºå•ä¸ªåª’ä½“é¡¹åŒæ­¥å›¾ç‰‡å’Œå…ƒæ•°æ®æ–‡ä»¶åˆ°æœ¬åœ° override ç›®å½•ã€‚
    """
    logger.trace(f"ä»»åŠ¡å¼€å§‹ï¼šå›¾ç‰‡å¤‡ä»½ for ID: {item_id} (åŸå› : {update_description})")
    try:
        # --- â–¼â–¼â–¼ æ ¸å¿ƒä¿®å¤ â–¼â–¼â–¼ ---
        # 1. æ ¹æ® item_id è·å–å®Œæ•´çš„åª’ä½“è¯¦æƒ…
        item_details = emby.get_emby_item_details(
            item_id, 
            processor.emby_url, 
            processor.emby_api_key, 
            processor.emby_user_id
        )
        if not item_details:
            logger.error(f"ä»»åŠ¡å¤±è´¥ï¼šæ— æ³•è·å– ID: {item_id} çš„åª’ä½“è¯¦æƒ…ï¼Œè·³è¿‡å›¾ç‰‡å¤‡ä»½ã€‚")
            return

        # 2. ä½¿ç”¨è·å–åˆ°çš„ item_details å­—å…¸æ¥è°ƒç”¨
        processor.sync_item_images(
            item_details=item_details, 
            update_description=update_description
            # episode_ids_to_sync å‚æ•°è¿™é‡Œä¸éœ€è¦ï¼Œsync_item_images ä¼šè‡ªå·±å¤„ç†
        )
        # --- â–²â–²â–² ä¿®å¤ç»“æŸ â–²â–²â–² ---

        logger.trace(f"ä»»åŠ¡æˆåŠŸï¼šå›¾ç‰‡å¤‡ä»½ for ID: {item_id}")
    except Exception as e:
        logger.error(f"ä»»åŠ¡å¤±è´¥ï¼šå›¾ç‰‡å¤‡ä»½ for ID: {item_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

def task_sync_all_metadata(processor, item_id: str, item_name: str):
    """
    ã€ä»»åŠ¡ï¼šå…¨èƒ½å…ƒæ•°æ®åŒæ­¥å™¨ã€‚
    å½“æ”¶åˆ° metadata.update Webhook æ—¶ï¼Œæ­¤ä»»åŠ¡ä¼šï¼š
    1. ä» Emby è·å–æœ€æ–°æ•°æ®ã€‚
    2. å°†æ›´æ–°æŒä¹…åŒ–åˆ° override è¦†ç›–ç¼“å­˜æ–‡ä»¶ã€‚
    3. å°†æ›´æ–°åŒæ­¥åˆ° media_metadata æ•°æ®åº“ç¼“å­˜ã€‚
    """
    log_prefix = f"å…¨èƒ½å…ƒæ•°æ®åŒæ­¥ for '{item_name}'"
    logger.trace(f"  âœ ä»»åŠ¡å¼€å§‹ï¼š{log_prefix}")
    try:
        # æ­¥éª¤ 1: è·å–åŒ…å«äº†ç”¨æˆ·ä¿®æ”¹çš„ã€æœ€æ–°çš„å®Œæ•´åª’ä½“è¯¦æƒ…
        item_details = emby.get_emby_item_details(
            item_id, 
            processor.emby_url, 
            processor.emby_api_key, 
            processor.emby_user_id,
            # è¯·æ±‚æ‰€æœ‰å¯èƒ½è¢«ç”¨æˆ·ä¿®æ”¹çš„å­—æ®µ
            fields="ProviderIds,Type,Name,OriginalTitle,Overview,Tagline,CommunityRating,OfficialRating,Genres,Studios,Tags,PremiereDate"
        )
        if not item_details:
            logger.error(f"  âœ {log_prefix} å¤±è´¥ï¼šæ— æ³•è·å–é¡¹ç›® {item_id} çš„æœ€æ–°è¯¦æƒ…ã€‚")
            return

        # æ­¥éª¤ 2: è°ƒç”¨æ–½å·¥é˜Ÿï¼Œæ›´æ–° override æ–‡ä»¶
        processor.sync_emby_updates_to_override_files(item_details)

        # æ­¥éª¤ 3: è°ƒç”¨å¦ä¸€ä¸ªæ–½å·¥é˜Ÿï¼Œæ›´æ–°æ•°æ®åº“ç¼“å­˜
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¤ç”¨ç°æœ‰çš„ task_sync_metadata_cache é€»è¾‘
        processor.sync_single_item_to_metadata_cache(item_id, item_name=item_name)

        logger.trace(f"  âœ ä»»åŠ¡æˆåŠŸï¼š{log_prefix}")
    except Exception as e:
        logger.error(f"  âœ ä»»åŠ¡å¤±è´¥ï¼š{log_prefix} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

# â˜…â˜…â˜… é‡æ–°å¤„ç†å•ä¸ªé¡¹ç›® â˜…â˜…â˜…
def task_reprocess_single_item(processor, item_id: str, item_name_for_ui: str):
    """
    ã€æœ€ç»ˆç‰ˆ - èŒè´£åˆ†ç¦»ã€‘åå°ä»»åŠ¡ã€‚
    æ­¤ç‰ˆæœ¬è´Ÿè´£åœ¨ä»»åŠ¡å¼€å§‹æ—¶è®¾ç½®â€œæ­£åœ¨å¤„ç†â€çš„çŠ¶æ€ï¼Œå¹¶æ‰§è¡Œæ ¸å¿ƒé€»è¾‘ã€‚
    """
    logger.trace(f"  âœ åå°ä»»åŠ¡å¼€å§‹æ‰§è¡Œ ({item_name_for_ui})")
    
    try:
        # âœ¨ å…³é”®ä¿®æ”¹ï¼šä»»åŠ¡ä¸€å¼€å§‹ï¼Œå°±ç”¨â€œæ­£åœ¨å¤„ç†â€çš„çŠ¶æ€è¦†ç›–æ‰æ—§çŠ¶æ€
        task_manager.update_status_from_thread(0, f"æ­£åœ¨å¤„ç†: {item_name_for_ui}")

        # ç°åœ¨æ‰å¼€å§‹çœŸæ­£çš„å·¥ä½œ
        processor.process_single_item(
            item_id, 
            force_full_update=True
        )
        # ä»»åŠ¡æˆåŠŸå®Œæˆåçš„çŠ¶æ€æ›´æ–°ä¼šè‡ªåŠ¨ç”±ä»»åŠ¡é˜Ÿåˆ—å¤„ç†ï¼Œæˆ‘ä»¬æ— éœ€å…³å¿ƒ
        logger.trace(f"  âœ åå°ä»»åŠ¡å®Œæˆ ({item_name_for_ui})")

    except Exception as e:
        logger.error(f"åå°ä»»åŠ¡å¤„ç† '{item_name_for_ui}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, f"å¤„ç†å¤±è´¥: {item_name_for_ui}")

# â˜…â˜…â˜… é‡æ–°å¤„ç†æ‰€æœ‰å¾…å¤æ ¸é¡¹ â˜…â˜…â˜…
def task_reprocess_all_review_items(processor):
    """
    ã€å·²å‡çº§ã€‘åå°ä»»åŠ¡ï¼šéå†æ‰€æœ‰å¾…å¤æ ¸é¡¹å¹¶é€ä¸€ä»¥â€œå¼ºåˆ¶åœ¨çº¿è·å–â€æ¨¡å¼é‡æ–°å¤„ç†ã€‚
    """
    logger.trace("--- å¼€å§‹æ‰§è¡Œâ€œé‡æ–°å¤„ç†æ‰€æœ‰å¾…å¤æ ¸é¡¹â€ä»»åŠ¡ [å¼ºåˆ¶åœ¨çº¿è·å–æ¨¡å¼] ---")
    try:
        # +++ æ ¸å¿ƒä¿®æ”¹ 1ï¼šåŒæ—¶æŸ¥è¯¢ item_id å’Œ item_name +++
        with connection.get_db_connection() as conn:
            cursor = conn.cursor()
            # ä» failed_log ä¸­åŒæ—¶è·å– ID å’Œ Name
            cursor.execute("SELECT item_id, item_name FROM failed_log")
            # å°†ç»“æœä¿å­˜ä¸ºä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨
            all_items = [{'id': row['item_id'], 'name': row['item_name']} for row in cursor.fetchall()]
        
        total = len(all_items)
        if total == 0:
            logger.info("å¾…å¤æ ¸åˆ—è¡¨ä¸­æ²¡æœ‰é¡¹ç›®ï¼Œä»»åŠ¡ç»“æŸã€‚")
            task_manager.update_status_from_thread(100, "å¾…å¤æ ¸åˆ—è¡¨ä¸ºç©ºã€‚")
            return

        logger.info(f"å…±æ‰¾åˆ° {total} ä¸ªå¾…å¤æ ¸é¡¹éœ€è¦ä»¥â€œå¼ºåˆ¶åœ¨çº¿è·å–â€æ¨¡å¼é‡æ–°å¤„ç†ã€‚")

        # +++ æ ¸å¿ƒä¿®æ”¹ 2ï¼šåœ¨å¾ªç¯ä¸­è§£åŒ… item_id å’Œ item_name +++
        for i, item in enumerate(all_items):
            if processor.is_stop_requested():
                logger.info("  ğŸš« ä»»åŠ¡è¢«ä¸­æ­¢ã€‚")
                break
            
            item_id = item['id']
            item_name = item['name'] or f"ItemID: {item_id}" # å¦‚æœåå­—ä¸ºç©ºï¼Œæä¾›ä¸€ä¸ªå¤‡ç”¨å

            task_manager.update_status_from_thread(int((i/total)*100), f"æ­£åœ¨é‡æ–°å¤„ç† {i+1}/{total}: {item_name}")
            
            # +++ æ ¸å¿ƒä¿®æ”¹ 3ï¼šä¼ é€’æ‰€æœ‰å¿…éœ€çš„å‚æ•° +++
            task_reprocess_single_item(processor, item_id, item_name)
            
            # æ¯ä¸ªé¡¹ç›®ä¹‹é—´ç¨ä½œåœé¡¿
            time.sleep(2) 

    except Exception as e:
        logger.error(f"é‡æ–°å¤„ç†æ‰€æœ‰å¾…å¤æ ¸é¡¹æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, "ä»»åŠ¡å¤±è´¥")

# â˜…â˜…â˜… é‡é‡çº§çš„å…ƒæ•°æ®ç¼“å­˜å¡«å……ä»»åŠ¡ (å†…å­˜ä¼˜åŒ–ç‰ˆ) â˜…â˜…â˜…
def task_populate_metadata_cache(processor, batch_size: int = 50, force_full_update: bool = False):
    """
    - é‡é‡çº§çš„å…ƒæ•°æ®ç¼“å­˜å¡«å……ä»»åŠ¡ (ç±»å‹å®‰å…¨ç‰ˆ)ã€‚
    - ä¿®å¤ï¼šå½»åº•è§£å†³ TMDb ID åœ¨ç”µå½±å’Œå‰§é›†é—´å†²çªçš„é—®é¢˜ã€‚
    - é€»è¾‘ï¼šå…¨ç¨‹ä½¿ç”¨ (tmdb_id, item_type) å¤åˆé”®è¿›è¡Œè¿½è¸ªï¼Œä¸å†é çŒœã€‚
    """
    task_name = "åŒæ­¥åª’ä½“å…ƒæ•°æ®"
    sync_mode = "æ·±åº¦åŒæ­¥ (å…¨é‡)" if force_full_update else "å¿«é€ŸåŒæ­¥ (å¢é‡)"
    logger.info(f"--- æ¨¡å¼: {sync_mode} (åˆ†æ‰¹å¤§å°: {batch_size}) ---")
    
    total_updated_count = 0
    total_offline_count = 0

    try:
        task_manager.update_status_from_thread(0, f"é˜¶æ®µ1/3: å»ºç«‹å·®å¼‚åŸºå‡† ({sync_mode})...")
        
        libs_to_process_ids = processor.config.get("libraries_to_process", [])
        if not libs_to_process_ids:
            raise ValueError("æœªåœ¨é…ç½®ä¸­æŒ‡å®šè¦å¤„ç†çš„åª’ä½“åº“ã€‚")

        # --- 1. å‡†å¤‡åŸºç¡€æ•°æ® ---
        known_emby_status = {}      # {emby_id: is_online}
        emby_sid_to_tmdb_id = {}    # {emby_series_id: tmdb_id} (ä»…ç”¨äºåˆ†é›†æ‰¾çˆ¹ï¼Œçˆ¹è‚¯å®šæ˜¯Series)
        
        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨å¤åˆé”® (tmdb_id, item_type) å­˜å‚¨æ˜ å°„ â˜…â˜…â˜…
        tmdb_key_to_emby_ids = defaultdict(set) 
        
        with connection.get_db_connection() as conn:
            cursor = conn.cursor()
            
            # A. é¢„åŠ è½½æ˜ å°„
            cursor.execute("""
                SELECT tmdb_id, item_type, jsonb_array_elements_text(emby_item_ids_json) as eid 
                FROM media_metadata 
                WHERE item_type IN ('Movie', 'Series')
            """)
            for row in cursor.fetchall():
                e_id, t_id, i_type = row['eid'], row['tmdb_id'], row['item_type']
                
                if i_type == 'Series':
                    emby_sid_to_tmdb_id[e_id] = t_id
                
                if t_id:
                    # ä½¿ç”¨å¤åˆé”®ï¼Œå½»åº•éš”ç¦»ç”µå½±å’Œå‰§é›†
                    tmdb_key_to_emby_ids[(t_id, i_type)].add(e_id)

            # B. è·å–åœ¨çº¿çŠ¶æ€
            if not force_full_update:
                cursor.execute("""
                    SELECT jsonb_array_elements_text(emby_item_ids_json) AS emby_id, in_library
                    FROM media_metadata 
                """)
                known_emby_status = {row['emby_id']: row['in_library'] for row in cursor.fetchall()}
                active_count = sum(1 for v in known_emby_status.values() if v)
                logger.info(f"  âœ æœ¬åœ°æ•°æ®åº“å·²çŸ¥ {len(known_emby_status)} ä¸ªID (å…¶ä¸­åœ¨çº¿: {active_count})ã€‚")

        # --- 2. æ‰«æ Emby (æµå¼å¤„ç†) ---
        task_manager.update_status_from_thread(10, f"é˜¶æ®µ2/3: æ‰«æ Emby å¹¶è®¡ç®—å·®å¼‚...")
        
        top_level_items_map = defaultdict(list)       
        series_to_seasons_map = defaultdict(list)     
        series_to_episode_map = defaultdict(list)     
        
        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨å¤åˆé”®é›†åˆè®°å½•è„æ•°æ® â˜…â˜…â˜…
        dirty_keys = set() # Set of (tmdb_id, item_type)
        
        current_scan_emby_ids = set() 
        pending_children = [] 

        scan_count = 0
        req_fields = "ProviderIds,Type,DateCreated,Name,OriginalTitle,PremiereDate,CommunityRating,Genres,Studios,Tags,DateModified,OfficialRating,ProductionYear,Path,PrimaryImageAspectRatio,Overview,MediaStreams,Container,Size,SeriesId,ParentIndexNumber,IndexNumber,ParentId,RunTimeTicks,_SourceLibraryId"

        item_generator = emby.fetch_all_emby_items_generator(
            base_url=processor.emby_url, 
            api_key=processor.emby_api_key, 
            library_ids=libs_to_process_ids, 
            fields=req_fields
        )

        for item in item_generator:
            scan_count += 1
            if scan_count % 5000 == 0:
                task_manager.update_status_from_thread(10, f"æ­£åœ¨ç´¢å¼• Emby åº“ ({scan_count} å·²æ‰«æ)...")
            
            item_id = str(item.get("Id"))
            if not item_id: continue
            
            current_scan_emby_ids.add(item_id)
            
            item_type = item.get("Type")
            tmdb_id = item.get("ProviderIds", {}).get("Tmdb")
            
            # å®æ—¶æ›´æ–°æ˜ å°„
            if item_type == "Series" and tmdb_id:
                emby_sid_to_tmdb_id[item_id] = str(tmdb_id)
            
            if item_type in ["Movie", "Series"] and tmdb_id:
                tmdb_key_to_emby_ids[(str(tmdb_id), item_type)].add(item_id)

            # è·³è¿‡åˆ¤æ–­
            is_clean = False
            if not force_full_update:
                if known_emby_status.get(item_id) is True:
                    is_clean = True
            
            if is_clean:
                continue 

            # â˜…â˜…â˜… è„æ•°æ®å¤„ç† (ç±»å‹å®‰å…¨) â˜…â˜…â˜…
            
            # A. é¡¶å±‚åª’ä½“
            if item_type in ["Movie", "Series"]:
                if tmdb_id:
                    composite_key = (str(tmdb_id), item_type)
                    top_level_items_map[composite_key].append(item)
                    dirty_keys.add(composite_key) # ç›´æ¥å­˜å…¥ (ID, Type)

            # B. å­é›†åª’ä½“
            elif item_type in ['Season', 'Episode']:
                s_id = str(item.get('SeriesId') or item.get('ParentId')) if item_type == 'Season' else str(item.get('SeriesId'))
                
                if item_type == 'Season':
                    if s_id: series_to_seasons_map[s_id].append(item)
                else:
                    if s_id: series_to_episode_map[s_id].append(item)

                if s_id and s_id in emby_sid_to_tmdb_id:
                    # æ—¢ç„¶æ˜¯å­é›†ï¼Œçˆ¶çº§ä¸€å®šæ˜¯ Series
                    dirty_keys.add((emby_sid_to_tmdb_id[s_id], 'Series'))
                elif s_id:
                    pending_children.append((s_id, item_type))

        # å¤„ç†å­¤å„¿åˆ†é›†
        for s_id, _ in pending_children:
            if s_id in emby_sid_to_tmdb_id:
                dirty_keys.add((emby_sid_to_tmdb_id[s_id], 'Series'))

        gc.collect()

        # --- 3. åå‘å·®å¼‚æ£€æµ‹ (åˆ é™¤) ---
        if not force_full_update:
            active_db_ids = {k for k, v in known_emby_status.items() if v is True}
            missing_emby_ids = active_db_ids - current_scan_emby_ids
            
            del known_emby_status
            del active_db_ids
            del current_scan_emby_ids
            gc.collect()

            if missing_emby_ids:
                logger.info(f"  âœ æ£€æµ‹åˆ° {len(missing_emby_ids)} ä¸ª Emby ID å·²æ¶ˆå¤±ï¼Œæ­£åœ¨åæŸ¥æ‰€å±å‰§é›†...")
                missing_ids_list = list(missing_emby_ids)
                
                with connection.get_db_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT DISTINCT parent_series_tmdb_id AS pid
                        FROM media_metadata 
                        WHERE item_type IN ('Season', 'Episode') 
                          AND in_library = TRUE 
                          AND EXISTS (
                              SELECT 1 
                              FROM jsonb_array_elements_text(emby_item_ids_json) as eid 
                              WHERE eid = ANY(%s)
                          )
                    """, (missing_ids_list,))
                    
                    # åæŸ¥å‡ºæ¥çš„è‚¯å®šæ˜¯ Series
                    affected_parents = set(row['pid'] for row in cursor.fetchall() if row['pid'])
                    if affected_parents:
                        for pid in affected_parents:
                            dirty_keys.add((pid, 'Series'))

        logger.info(f"  âœ Emby æ‰«æå®Œæˆï¼Œå…± {scan_count} ä¸ªé¡¹ã€‚æœ‰ {len(dirty_keys)} ä¸ªé¡¹ç›®æ¶‰åŠå˜æ›´ã€‚")

        # --- 4. ç¡®å®šå¤„ç†é˜Ÿåˆ— (æ— éœ€çŒœæµ‹ç±»å‹) ---
        items_to_process = []
        
        # ç›´æ¥éå† dirty_keysï¼Œé‡Œé¢å·²ç»åŒ…å«äº†å‡†ç¡®çš„ (ID, Type)
        for (tmdb_id, item_type) in dirty_keys:
            
            # ä½¿ç”¨å¤åˆé”®æŸ¥æ‰¾å…³è”çš„ Emby IDs
            related_emby_ids = tmdb_key_to_emby_ids.get((tmdb_id, item_type), set())
            
            if not related_emby_ids:
                continue

            items_to_process.append({
                'tmdb_id': tmdb_id,
                'emby_ids': list(related_emby_ids),
                'type': item_type, # ç›´æ¥ä½¿ç”¨ key é‡Œçš„ typeï¼Œç»å¯¹å‡†ç¡®
                'refetch': True 
            })

        total_to_process = len(items_to_process)
        task_manager.update_status_from_thread(20, f"é˜¶æ®µ3/3: æ­£åœ¨åŒæ­¥ {total_to_process} ä¸ªå˜æ›´é¡¹ç›®...")
        logger.info(f"  âœ æœ€ç»ˆå¤„ç†é˜Ÿåˆ—: {total_to_process} ä¸ªé¡¶å±‚é¡¹ç›®")

        # --- 5. æ‰¹é‡å¤„ç† ---
        processed_count = 0
        for i in range(0, total_to_process, batch_size):
            if processor.is_stop_requested(): break
            batch_tasks = items_to_process[i:i + batch_size]
            
            batch_item_groups = []
            
            # é¢„å¤„ç†ï¼šæ‹‰å– refetch çš„æ•°æ®
            for task in batch_tasks:
                try:
                    target_emby_ids = task['emby_ids']
                    item_type = task['type']
                    
                    # 1. æ‰¹é‡è·å–è¿™äº› Emby ID çš„è¯¦æƒ…
                    top_items = emby.get_emby_items_by_id(
                        base_url=processor.emby_url,
                        api_key=processor.emby_api_key,
                        user_id=processor.emby_user_id,
                        item_ids=target_emby_ids,
                        fields=req_fields
                    )
                    
                    if not top_items: continue

                    # 2. å¦‚æœæ˜¯å‰§é›†ï¼Œè¿˜éœ€è¦æ‹‰å–æ¯ä¸ªå‰§é›†çš„å­é›†
                    if item_type == 'Series':
                        full_group = []
                        full_group.extend(top_items)
                        
                        # æ¸…ç©ºæ—§çš„å­é›†ç¼“å­˜ï¼Œé˜²æ­¢é‡å¤
                        for e_id in target_emby_ids:
                            series_to_seasons_map[e_id] = []
                            series_to_episode_map[e_id] = []
                        
                        children_gen = emby.fetch_all_emby_items_generator(
                            base_url=processor.emby_url,
                            api_key=processor.emby_api_key,
                            library_ids=target_emby_ids, 
                            fields=req_fields
                        )
                        
                        children_list = list(children_gen)
                        full_group.extend(children_list)
                        
                        # é‡æ–°å¡«å…… map
                        for child in children_list:
                            ct = child.get('Type')
                            pid = str(child.get('SeriesId') or child.get('ParentId'))
                            if pid:
                                if ct == 'Season': series_to_seasons_map[pid].append(child)
                                elif ct == 'Episode': series_to_episode_map[pid].append(child)
                        
                        batch_item_groups.append(full_group)
                    
                    else:
                        # ç”µå½±ç›´æ¥æ·»åŠ 
                        batch_item_groups.append(top_items)

                except Exception as e:
                    logger.error(f"å¤„ç†é¡¹ç›® {task.get('tmdb_id')} å¤±è´¥: {e}")

            # --- ä»¥ä¸‹é€»è¾‘ä¿æŒä¸å˜ (å¹¶å‘è·å– TMDB å’Œ å†™å…¥ DB) ---
            
            tmdb_details_map = {}
            def fetch_tmdb_details(item_group):
                if not item_group: return None, None
                item = item_group[0]
                t_id = item.get("ProviderIds", {}).get("Tmdb")
                i_type = item.get("Type")
                if not t_id: return None, None
                details = None
                try:
                    if i_type == 'Movie': details = tmdb.get_movie_details(t_id, processor.tmdb_api_key)
                    elif i_type == 'Series': details = tmdb.get_tv_details(t_id, processor.tmdb_api_key)
                except Exception: pass
                return str(t_id), details

            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = {executor.submit(fetch_tmdb_details, grp): grp for grp in batch_item_groups}
                for future in concurrent.futures.as_completed(futures):
                    t_id_str, details = future.result()
                    if t_id_str and details: tmdb_details_map[t_id_str] = details

            metadata_batch = []
            series_ids_processed_in_batch = set()

            for item_group in batch_item_groups:
                if not item_group: continue
                item = item_group[0]
                tmdb_id_str = str(item.get("ProviderIds", {}).get("Tmdb"))
                item_type = item.get("Type")
                tmdb_details = tmdb_details_map.get(tmdb_id_str)
                
                # --- 1. æ„å»ºé¡¶å±‚è®°å½• ---
                asset_details_list = []
                if item_type == "Movie":
                    asset_details_list = []
                    for v in item_group:
                        details = parse_full_asset_details(v)
                        details['source_library_id'] = v.get('_SourceLibraryId') 
                        asset_details_list.append(details)

                emby_runtime = round(item['RunTimeTicks'] / 600000000) if item.get('RunTimeTicks') else None

                top_record = {
                    "tmdb_id": tmdb_id_str, "item_type": item_type, "title": item.get('Name'),
                    "original_title": item.get('OriginalTitle'), "release_year": item.get('ProductionYear'),
                    "in_library": True, 
                    "subscription_status": "NONE",
                    "emby_item_ids_json": json.dumps(list(set(v.get('Id') for v in item_group if v.get('Id'))), ensure_ascii=False),
                    "asset_details_json": json.dumps(asset_details_list, ensure_ascii=False),
                    "rating": item.get('CommunityRating'),
                    "date_added": item.get('DateCreated'),
                    "genres_json": json.dumps(item.get('Genres', []), ensure_ascii=False),
                    "official_rating": item.get('OfficialRating'), 
                    "unified_rating": get_unified_rating(item.get('OfficialRating')),
                    "runtime_minutes": emby_runtime if (item_type == 'Movie' and emby_runtime) else tmdb_details.get('runtime') if (item_type == 'Movie' and tmdb_details) else None
                }
                if tmdb_details:
                    top_record['poster_path'] = tmdb_details.get('poster_path')
                    top_record['overview'] = tmdb_details.get('overview')
                    top_record['studios_json'] = json.dumps([s['name'] for s in tmdb_details.get('production_companies', [])], ensure_ascii=False)
                    if item_type == 'Movie':
                        top_record['runtime_minutes'] = tmdb_details.get('runtime')
                    
                    directors, countries, keywords = [], [], []
                    if item_type == 'Movie':
                        credits_data = tmdb_details.get("credits", {}) or tmdb_details.get("casts", {})
                        directors = [{'id': p.get('id'), 'name': p.get('name')} for p in credits_data.get('crew', []) if p.get('job') == 'Director']
                        country_codes = [c.get('iso_3166_1') for c in tmdb_details.get('production_countries', [])]
                        countries = translate_country_list(country_codes)
                        keywords_data = tmdb_details.get('keywords', {})
                        keyword_list = keywords_data.get('keywords', []) if isinstance(keywords_data, dict) else []
                        keywords = [k['name'] for k in keyword_list if k.get('name')]
                    elif item_type == 'Series':
                        directors = [{'id': c.get('id'), 'name': c.get('name')} for c in tmdb_details.get('created_by', [])]
                        countries = translate_country_list(tmdb_details.get('origin_country', []))
                        keywords_data = tmdb_details.get('keywords', {})
                        keyword_list = keywords_data.get('results', []) if isinstance(keywords_data, dict) else []
                        keywords = [k['name'] for k in keyword_list if k.get('name')]
                    top_record['directors_json'] = json.dumps(directors, ensure_ascii=False)
                    top_record['countries_json'] = json.dumps(countries, ensure_ascii=False)
                    top_record['keywords_json'] = json.dumps(keywords, ensure_ascii=False)
                else:
                    top_record['poster_path'] = None
                    top_record['studios_json'] = '[]'
                    top_record['directors_json'] = '[]'; top_record['countries_json'] = '[]'; top_record['keywords_json'] = '[]'

                metadata_batch.append(top_record)

                # --- 2. å¤„ç† Series çš„å­é›† ---
                if item_type == "Series":
                    series_ids_processed_in_batch.add(tmdb_id_str)
                    
                    series_emby_ids = [str(v.get('Id')) for v in item_group if v.get('Id')]
                    my_seasons = []
                    my_episodes = []
                    for s_id in series_emby_ids:
                        my_seasons.extend(series_to_seasons_map.get(s_id, []))
                        my_episodes.extend(series_to_episode_map.get(s_id, []))
                    
                    tmdb_children_map = {}
                    processed_season_numbers = set()
                    
                    if tmdb_details and 'seasons' in tmdb_details:
                        for s_info in tmdb_details.get('seasons', []):
                            try:
                                s_num = int(s_info.get('season_number'))
                            except (ValueError, TypeError):
                                continue
                            
                            matched_emby_seasons = []
                            for s in my_seasons:
                                try:
                                    if int(s.get('IndexNumber')) == s_num:
                                        matched_emby_seasons.append(s)
                                except (ValueError, TypeError):
                                    continue
                            
                            if matched_emby_seasons:
                                processed_season_numbers.add(s_num)
                                real_season_tmdb_id = str(s_info.get('id'))
                                season_poster = s_info.get('poster_path')
                                if not season_poster and tmdb_details:
                                    season_poster = tmdb_details.get('poster_path')

                                season_record = {
                                    "tmdb_id": real_season_tmdb_id,
                                    "item_type": "Season",
                                    "parent_series_tmdb_id": tmdb_id_str,
                                    "season_number": s_num,
                                    "title": s_info.get('name'),
                                    "overview": s_info.get('overview'),
                                    "poster_path": season_poster,
                                    "in_library": True,
                                    "subscription_status": "NONE",
                                    "emby_item_ids_json": json.dumps([s.get('Id') for s in matched_emby_seasons]),
                                    "ignore_reason": None
                                }
                                metadata_batch.append(season_record)
                                tmdb_children_map[f"S{s_num}"] = s_info

                                has_eps = any(e.get('ParentIndexNumber') == s_num for e in my_episodes)
                                if has_eps:
                                    try:
                                        s_details = tmdb.get_tv_season_details(tmdb_id_str, s_num, processor.tmdb_api_key)
                                        if s_details and 'episodes' in s_details:
                                            for ep in s_details['episodes']:
                                                if ep.get('episode_number') is not None:
                                                    tmdb_children_map[f"S{s_num}E{ep.get('episode_number')}"] = ep
                                    except: pass

                    # B. å…œåº•å¤„ç†
                    for s in my_seasons:
                        try:
                            s_num = int(s.get('IndexNumber'))
                        except (ValueError, TypeError):
                            continue

                        if s_num not in processed_season_numbers:
                            fallback_season_tmdb_id = f"{tmdb_id_str}-S{s_num}"
                            season_record = {
                                "tmdb_id": fallback_season_tmdb_id,
                                "item_type": "Season",
                                "parent_series_tmdb_id": tmdb_id_str,
                                "season_number": s_num,
                                "title": s.get('Name') or f"Season {s_num}",
                                "overview": None,
                                "poster_path": tmdb_details.get('poster_path') if tmdb_details else None,
                                "in_library": True,
                                "subscription_status": "NONE",
                                "emby_item_ids_json": json.dumps([s.get('Id')]),
                                "ignore_reason": "Local Season Only"
                            }
                            metadata_batch.append(season_record)
                            processed_season_numbers.add(s_num)

                    # C. å¤„ç†åˆ†é›†
                    ep_grouped = defaultdict(list)
                    for ep in my_episodes:
                        s_n, e_n = ep.get('ParentIndexNumber'), ep.get('IndexNumber')
                        if s_n is not None and e_n is not None:
                            ep_grouped[(s_n, e_n)].append(ep)
                    
                    for (s_n, e_n), versions in ep_grouped.items():
                        emby_ep = versions[0]
                        emby_ep_runtime = round(emby_ep['RunTimeTicks'] / 600000000) if emby_ep.get('RunTimeTicks') else None
                        lookup_key = f"S{s_n}E{e_n}"
                        tmdb_ep_info = tmdb_children_map.get(lookup_key)
                        
                        ep_asset_details_list = []
                        for v in versions:
                            details = parse_full_asset_details(v)
                            details['source_library_id'] = v.get('_SourceLibraryId')
                            ep_asset_details_list.append(details)

                        child_record = {
                            "item_type": "Episode",
                            "parent_series_tmdb_id": tmdb_id_str,
                            "season_number": s_n,
                            "episode_number": e_n,
                            "in_library": True,
                            "emby_item_ids_json": json.dumps([v.get('Id') for v in versions]),
                            "asset_details_json": json.dumps(ep_asset_details_list, ensure_ascii=False),
                            "ignore_reason": None
                        }

                        if tmdb_ep_info and tmdb_ep_info.get('id'):
                            child_record['tmdb_id'] = str(tmdb_ep_info.get('id'))
                            child_record['title'] = tmdb_ep_info.get('name')
                            child_record['overview'] = tmdb_ep_info.get('overview')
                            child_record['poster_path'] = tmdb_ep_info.get('still_path')
                            child_record['runtime_minutes'] = emby_ep_runtime if emby_ep_runtime else tmdb_ep_info.get('runtime')
                        else:
                            child_record['tmdb_id'] = f"{tmdb_id_str}-S{s_n}E{e_n}"
                            child_record['title'] = versions[0].get('Name')
                            child_record['overview'] = versions[0].get('Overview')
                            child_record['runtime_minutes'] = emby_ep_runtime
                        
                        metadata_batch.append(child_record)

            # 7. å†™å…¥æ•°æ®åº“ & å­é›†ç¦»çº¿å¯¹è´¦
            if metadata_batch:
                total_updated_count += len(metadata_batch)

                with connection.get_db_connection() as conn:
                    cursor = conn.cursor()
                    
                    # --- A. æ‰§è¡Œå†™å…¥ ---
                    for idx, metadata in enumerate(metadata_batch):
                        savepoint_name = f"sp_{idx}"
                        try:
                            cursor.execute(f"SAVEPOINT {savepoint_name};")
                            columns = [k for k, v in metadata.items() if v is not None]
                            values = [v for v in metadata.values() if v is not None]
                            cols_str = ', '.join(columns)
                            vals_str = ', '.join(['%s'] * len(values))
                            
                            update_clauses = []
                            for col in columns:
                                if col in ('tmdb_id', 'item_type', 'subscription_sources_json'): continue
                                update_clauses.append(f"{col} = EXCLUDED.{col}")
                            
                            sql = f"""
                                INSERT INTO media_metadata ({cols_str}, last_synced_at) 
                                VALUES ({vals_str}, NOW()) 
                                ON CONFLICT (tmdb_id, item_type) 
                                DO UPDATE SET {', '.join(update_clauses)}, last_synced_at = NOW()
                            """
                            cursor.execute(sql, tuple(values))
                        except Exception as e:
                            cursor.execute(f"ROLLBACK TO SAVEPOINT {savepoint_name};")
                            logger.error(f"å†™å…¥å¤±è´¥ {metadata.get('tmdb_id')}: {e}")
                    
                    # --- B. æ‰§è¡Œå­é›†ç¦»çº¿å¯¹è´¦ ---
                    if series_ids_processed_in_batch:
                        active_child_ids = {
                            m['tmdb_id'] for m in metadata_batch 
                            if m['item_type'] in ('Season', 'Episode')
                        }
                        active_child_ids_list = list(active_child_ids)
                        
                        if active_child_ids_list:
                            cursor.execute("""
                                UPDATE media_metadata
                                SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = '[]'::jsonb
                                WHERE parent_series_tmdb_id = ANY(%s)
                                  AND item_type IN ('Season', 'Episode')
                                  AND in_library = TRUE
                                  AND tmdb_id != ALL(%s)
                            """, (list(series_ids_processed_in_batch), active_child_ids_list))
                            total_offline_count += cursor.rowcount
                        else:
                            cursor.execute("""
                                UPDATE media_metadata
                                SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = '[]'::jsonb
                                WHERE parent_series_tmdb_id = ANY(%s)
                                  AND item_type IN ('Season', 'Episode')
                                  AND in_library = TRUE
                            """, (list(series_ids_processed_in_batch),))
                            total_offline_count += cursor.rowcount

                    conn.commit()
            
            del batch_item_groups
            del tmdb_details_map
            del metadata_batch
            gc.collect()

            processed_count += len(batch_tasks)
            task_manager.update_status_from_thread(20 + int((processed_count / total_to_process) * 80), f"å¤„ç†è¿›åº¦ {processed_count}/{total_to_process}...")

        final_msg = f"åŒæ­¥å®Œæˆï¼æ–°å¢/æ›´æ–°: {total_updated_count} ä¸ªåª’ä½“é¡¹, æ ‡è®°ç¦»çº¿: {total_offline_count} ä¸ªåª’ä½“é¡¹ã€‚"
        logger.info(f"  âœ… {final_msg}")
        task_manager.update_status_from_thread(100, final_msg)

    except Exception as e:
        logger.error(f"æ‰§è¡Œ '{task_name}' ä»»åŠ¡æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, f"ä»»åŠ¡å¤±è´¥: {e}")

def task_apply_main_cast_to_episodes(processor, series_id: str, episode_ids: list):
    """
    ã€V2 - æ–‡ä»¶ä¸­å¿ƒåŒ–é‡æ„ç‰ˆã€‘
    è½»é‡çº§ä»»åŠ¡ï¼šå½“å‰§é›†è¿½æ›´æ–°å¢åˆ†é›†æ—¶ï¼Œå°†ä¸»é¡¹ç›®çš„å®Œç¾æ¼”å‘˜è¡¨æ³¨å…¥åˆ°æ–°åˆ†é›†çš„ override å…ƒæ•°æ®æ–‡ä»¶ä¸­ã€‚
    æ­¤ä»»åŠ¡ä¸å†è¯»å†™ Emby APIï¼Œè€Œæ˜¯å§”æ‰˜æ ¸å¿ƒå¤„ç†å™¨çš„ sync_single_item_assets æ–¹æ³•æ‰§è¡Œç²¾å‡†çš„æ–‡ä»¶åŒæ­¥æ“ä½œã€‚
    """
    try:
        if not episode_ids:
            logger.info(f"  âœ å‰§é›† {series_id} è¿½æ›´ä»»åŠ¡è·³è¿‡ï¼šæœªæä¾›éœ€è¦æ›´æ–°çš„åˆ†é›†IDã€‚")
            return
        
        series_details_for_log = emby.get_emby_item_details(series_id, processor.emby_url, processor.emby_api_key, processor.emby_user_id, fields="Name,ProviderIds")
        series_name = series_details_for_log.get("Name", f"ID:{series_id}") if series_details_for_log else f"ID:{series_id}"

        logger.info(f"  âœ è¿½æ›´ä»»åŠ¡å¯åŠ¨ï¼šå‡†å¤‡ä¸ºå‰§é›† ã€Š{series_name}ã€‹ çš„ {len(episode_ids)} ä¸ªæ–°åˆ†é›†åŒæ­¥å…ƒæ•°æ®...")

        processor.sync_single_item_assets(
            item_id=series_id,
            update_description=f"è¿½æ›´æ–°å¢ {len(episode_ids)} ä¸ªåˆ†é›†",
            sync_timestamp_iso=datetime.now(timezone.utc).isoformat(),
            episode_ids_to_sync=episode_ids
        )

        logger.info(f"  âœ å¤„ç†å®Œæˆï¼Œæ­£åœ¨é€šçŸ¥ Emby åˆ·æ–°...")
        emby.refresh_emby_item_metadata(
            item_emby_id=series_id,
            emby_server_url=processor.emby_url,
            emby_api_key=processor.emby_api_key,
            user_id_for_ops=processor.emby_user_id,
            replace_all_metadata_param=True,
            item_name_for_log=series_name
        )

        # TGé€šçŸ¥
        if series_details_for_log:
            logger.info(f"  âœ æ­£åœ¨ä¸ºã€Š{series_name}ã€‹è§¦å‘è¿½æ›´é€šçŸ¥...")
            telegram.send_media_notification(
                item_details=series_details_for_log,
                notification_type='update',
                new_episode_ids=episode_ids
            )

        # æ­¥éª¤ 3: æ›´æ–°çˆ¶å‰§é›†åœ¨å…ƒæ•°æ®ç¼“å­˜ä¸­çš„ last_synced_at æ—¶é—´æˆ³ (è¿™ä¸ªé€»è¾‘å¯ä»¥ä¿ç•™)
        if series_details_for_log:
            tmdb_id = series_details_for_log.get("ProviderIds", {}).get("Tmdb")
            if tmdb_id:
                try:
                    with connection.get_db_connection() as conn:
                        with conn.cursor() as cursor:
                            cursor.execute(
                                "UPDATE media_metadata SET last_synced_at = %s WHERE tmdb_id = %s AND item_type = 'Series'",
                                (datetime.now(timezone.utc), tmdb_id)
                            )
                except Exception as db_e:
                    logger.error(f"  âœ æ›´æ–°å‰§é›†ã€Š{series_name}ã€‹çš„æ—¶é—´æˆ³æ—¶å‘ç”Ÿæ•°æ®åº“é”™è¯¯: {db_e}", exc_info=True)

    except Exception as e:
        logger.error(f"  âœ ä¸ºå‰§é›† {series_id} çš„æ–°åˆ†é›†åº”ç”¨ä¸»æ¼”å‘˜è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise