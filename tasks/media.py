# tasks/media.py
# æ ¸å¿ƒåª’ä½“å¤„ç†ã€å…ƒæ•°æ®ã€èµ„äº§åŒæ­¥ç­‰

import time
import json
import gc
import logging
from typing import List
import concurrent.futures
from collections import defaultdict

# å¯¼å…¥éœ€è¦çš„åº•å±‚æ¨¡å—å’Œå…±äº«å®ä¾‹
import task_manager
import utils
import handler.tmdb as tmdb
import handler.emby as emby
import handler.telegram as telegram
from database import connection, settings_db, media_db
from .helpers import parse_full_asset_details

logger = logging.getLogger(__name__)

# â˜…â˜…â˜… ä¸­æ–‡åŒ–è§’è‰²å â˜…â˜…â˜…
def task_role_translation(processor, force_full_update: bool = False):
    """
    æ ¹æ®ä¼ å…¥çš„ force_full_update å‚æ•°ï¼Œå†³å®šæ˜¯æ‰§è¡Œæ ‡å‡†æ‰«æè¿˜æ˜¯æ·±åº¦æ›´æ–°ã€‚
    """
    # 1. æ ¹æ®å‚æ•°å†³å®šæ—¥å¿—ä¿¡æ¯
    if force_full_update:
        logger.info("  âœ å³å°†æ‰§è¡Œæ·±åº¦æ¨¡å¼ï¼Œå°†å¤„ç†æ‰€æœ‰åª’ä½“é¡¹å¹¶ä»TMDbè·å–æœ€æ–°æ•°æ®...")
    else:
        logger.info("  âœ å³å°†æ‰§è¡Œå¿«é€Ÿæ¨¡å¼ï¼Œå°†è·³è¿‡å·²å¤„ç†é¡¹...")


    # 3. è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°ï¼Œå¹¶å°† force_full_update å‚æ•°é€ä¼ ä¸‹å»
    processor.process_full_library(
        update_status_callback=task_manager.update_status_from_thread,
        force_full_update=force_full_update 
    )

# --- ä½¿ç”¨æ‰‹åŠ¨ç¼–è¾‘çš„ç»“æœå¤„ç†åª’ä½“é¡¹ ---
def task_manual_update(processor, item_id: str, manual_cast_list: list, item_name: str):
    """ä»»åŠ¡ï¼šä½¿ç”¨æ‰‹åŠ¨ç¼–è¾‘çš„ç»“æœå¤„ç†åª’ä½“é¡¹"""
    processor.process_item_with_manual_cast(
        item_id=item_id,
        manual_cast_list=manual_cast_list,
        item_name=item_name
    )

def task_sync_images(processor, item_id: str, update_description: str, sync_timestamp_iso: str):
    """
    ä»»åŠ¡ï¼šä¸ºå•ä¸ªåª’ä½“é¡¹åŒæ­¥å›¾ç‰‡å’Œå…ƒæ•°æ®æ–‡ä»¶åˆ°æœ¬åœ° override ç›®å½•ã€‚
    """
    logger.trace(f"ä»»åŠ¡å¼€å§‹ï¼šå›¾ç‰‡å¤‡ä»½ for ID: {item_id} (åŸå› : {update_description})")
    try:
        # --- â–¼â–¼â–¼ æ ¸å¿ƒä¿®å¤ â–¼â–¼â–¼ ---
        # 1. æ ¹æ® item_id è·å–å®Œæ•´çš„åª’ä½“è¯¦æƒ…
        item_details = emby.get_emby_item_details(
            item_id, 
            processor.emby_url, 
            processor.emby_api_key, 
            processor.emby_user_id
        )
        if not item_details:
            logger.error(f"ä»»åŠ¡å¤±è´¥ï¼šæ— æ³•è·å– ID: {item_id} çš„åª’ä½“è¯¦æƒ…ï¼Œè·³è¿‡å›¾ç‰‡å¤‡ä»½ã€‚")
            return

        # 2. ä½¿ç”¨è·å–åˆ°çš„ item_details å­—å…¸æ¥è°ƒç”¨
        processor.sync_item_images(
            item_details=item_details, 
            update_description=update_description
            # episode_ids_to_sync å‚æ•°è¿™é‡Œä¸éœ€è¦ï¼Œsync_item_images ä¼šè‡ªå·±å¤„ç†
        )
        # --- â–²â–²â–² ä¿®å¤ç»“æŸ â–²â–²â–² ---

        logger.trace(f"ä»»åŠ¡æˆåŠŸï¼šå›¾ç‰‡å¤‡ä»½ for ID: {item_id}")
    except Exception as e:
        logger.error(f"ä»»åŠ¡å¤±è´¥ï¼šå›¾ç‰‡å¤‡ä»½ for ID: {item_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

def task_sync_all_metadata(processor, item_id: str, item_name: str):
    """
    ã€ä»»åŠ¡ï¼šå…¨èƒ½å…ƒæ•°æ®åŒæ­¥å™¨ã€‚
    å½“æ”¶åˆ° metadata.update Webhook æ—¶ï¼Œæ­¤ä»»åŠ¡ä¼šï¼š
    1. ä» Emby è·å–æœ€æ–°æ•°æ®ã€‚
    2. å°†æ›´æ–°æŒä¹…åŒ–åˆ° override è¦†ç›–ç¼“å­˜æ–‡ä»¶ã€‚
    3. å°†æ›´æ–°åŒæ­¥åˆ° media_metadata æ•°æ®åº“ç¼“å­˜ã€‚
    """
    log_prefix = f"å…¨èƒ½å…ƒæ•°æ®åŒæ­¥ for '{item_name}'"
    logger.trace(f"  âœ ä»»åŠ¡å¼€å§‹ï¼š{log_prefix}")
    try:
        # æ­¥éª¤ 1: è·å–åŒ…å«äº†ç”¨æˆ·ä¿®æ”¹çš„ã€æœ€æ–°çš„å®Œæ•´åª’ä½“è¯¦æƒ…
        item_details = emby.get_emby_item_details(
            item_id, 
            processor.emby_url, 
            processor.emby_api_key, 
            processor.emby_user_id,
            # è¯·æ±‚æ‰€æœ‰å¯èƒ½è¢«ç”¨æˆ·ä¿®æ”¹çš„å­—æ®µ
            fields="ProviderIds,Type,Name,OriginalTitle,Overview,Tagline,CommunityRating,OfficialRating,Genres,Studios,Tags,PremiereDate"
        )
        if not item_details:
            logger.error(f"  âœ {log_prefix} å¤±è´¥ï¼šæ— æ³•è·å–é¡¹ç›® {item_id} çš„æœ€æ–°è¯¦æƒ…ã€‚")
            return

        # æ­¥éª¤ 2: è°ƒç”¨æ–½å·¥é˜Ÿï¼Œæ›´æ–° override æ–‡ä»¶
        processor.sync_emby_updates_to_override_files(item_details)

        # æ­¥éª¤ 3: è°ƒç”¨å¦ä¸€ä¸ªæ–½å·¥é˜Ÿï¼Œæ›´æ–°æ•°æ®åº“ç¼“å­˜
        processor.sync_single_item_to_metadata_cache(item_id, item_name=item_name)

        logger.trace(f"  âœ ä»»åŠ¡æˆåŠŸï¼š{log_prefix}")
    except Exception as e:
        logger.error(f"  âœ ä»»åŠ¡å¤±è´¥ï¼š{log_prefix} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

# â˜…â˜…â˜… é‡æ–°å¤„ç†å•ä¸ªé¡¹ç›® â˜…â˜…â˜…
def task_reprocess_single_item(processor, item_id: str, item_name_for_ui: str):
    """
    ã€æœ€ç»ˆç‰ˆ - èŒè´£åˆ†ç¦»ã€‘åå°ä»»åŠ¡ã€‚
    æ­¤ç‰ˆæœ¬è´Ÿè´£åœ¨ä»»åŠ¡å¼€å§‹æ—¶è®¾ç½®â€œæ­£åœ¨å¤„ç†â€çš„çŠ¶æ€ï¼Œå¹¶æ‰§è¡Œæ ¸å¿ƒé€»è¾‘ã€‚
    """
    logger.trace(f"  âœ åå°ä»»åŠ¡å¼€å§‹æ‰§è¡Œ ({item_name_for_ui})")
    
    try:
        # âœ¨ å…³é”®ä¿®æ”¹ï¼šä»»åŠ¡ä¸€å¼€å§‹ï¼Œå°±ç”¨â€œæ­£åœ¨å¤„ç†â€çš„çŠ¶æ€è¦†ç›–æ‰æ—§çŠ¶æ€
        task_manager.update_status_from_thread(0, f"æ­£åœ¨å¤„ç†: {item_name_for_ui}")

        # ç°åœ¨æ‰å¼€å§‹çœŸæ­£çš„å·¥ä½œ
        processor.process_single_item(
            item_id, 
            force_full_update=True
        )
        # ä»»åŠ¡æˆåŠŸå®Œæˆåçš„çŠ¶æ€æ›´æ–°ä¼šè‡ªåŠ¨ç”±ä»»åŠ¡é˜Ÿåˆ—å¤„ç†ï¼Œæˆ‘ä»¬æ— éœ€å…³å¿ƒ
        logger.trace(f"  âœ åå°ä»»åŠ¡å®Œæˆ ({item_name_for_ui})")

    except Exception as e:
        logger.error(f"åå°ä»»åŠ¡å¤„ç† '{item_name_for_ui}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, f"å¤„ç†å¤±è´¥: {item_name_for_ui}")

# â˜…â˜…â˜… é‡æ–°å¤„ç†æ‰€æœ‰å¾…å¤æ ¸é¡¹ â˜…â˜…â˜…
def task_reprocess_all_review_items(processor):
    """
    ã€å·²å‡çº§ã€‘åå°ä»»åŠ¡ï¼šéå†æ‰€æœ‰å¾…å¤æ ¸é¡¹å¹¶é€ä¸€ä»¥â€œå¼ºåˆ¶åœ¨çº¿è·å–â€æ¨¡å¼é‡æ–°å¤„ç†ã€‚
    """
    logger.trace("--- å¼€å§‹æ‰§è¡Œâ€œé‡æ–°å¤„ç†æ‰€æœ‰å¾…å¤æ ¸é¡¹â€ä»»åŠ¡ [å¼ºåˆ¶åœ¨çº¿è·å–æ¨¡å¼] ---")
    try:
        # +++ æ ¸å¿ƒä¿®æ”¹ 1ï¼šåŒæ—¶æŸ¥è¯¢ item_id å’Œ item_name +++
        with connection.get_db_connection() as conn:
            cursor = conn.cursor()
            # ä» failed_log ä¸­åŒæ—¶è·å– ID å’Œ Name
            cursor.execute("SELECT item_id, item_name FROM failed_log")
            # å°†ç»“æœä¿å­˜ä¸ºä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨
            all_items = [{'id': row['item_id'], 'name': row['item_name']} for row in cursor.fetchall()]
        
        total = len(all_items)
        if total == 0:
            logger.info("å¾…å¤æ ¸åˆ—è¡¨ä¸­æ²¡æœ‰é¡¹ç›®ï¼Œä»»åŠ¡ç»“æŸã€‚")
            task_manager.update_status_from_thread(100, "å¾…å¤æ ¸åˆ—è¡¨ä¸ºç©ºã€‚")
            return

        logger.info(f"å…±æ‰¾åˆ° {total} ä¸ªå¾…å¤æ ¸é¡¹éœ€è¦ä»¥â€œå¼ºåˆ¶åœ¨çº¿è·å–â€æ¨¡å¼é‡æ–°å¤„ç†ã€‚")

        # +++ æ ¸å¿ƒä¿®æ”¹ 2ï¼šåœ¨å¾ªç¯ä¸­è§£åŒ… item_id å’Œ item_name +++
        for i, item in enumerate(all_items):
            if processor.is_stop_requested():
                logger.info("  ğŸš« ä»»åŠ¡è¢«ä¸­æ­¢ã€‚")
                break
            
            item_id = item['id']
            item_name = item['name'] or f"ItemID: {item_id}" # å¦‚æœåå­—ä¸ºç©ºï¼Œæä¾›ä¸€ä¸ªå¤‡ç”¨å

            task_manager.update_status_from_thread(int((i/total)*100), f"æ­£åœ¨é‡æ–°å¤„ç† {i+1}/{total}: {item_name}")
            
            # +++ æ ¸å¿ƒä¿®æ”¹ 3ï¼šä¼ é€’æ‰€æœ‰å¿…éœ€çš„å‚æ•° +++
            task_reprocess_single_item(processor, item_id, item_name)
            
            # æ¯ä¸ªé¡¹ç›®ä¹‹é—´ç¨ä½œåœé¡¿
            time.sleep(2) 

    except Exception as e:
        logger.error(f"é‡æ–°å¤„ç†æ‰€æœ‰å¾…å¤æ ¸é¡¹æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, "ä»»åŠ¡å¤±è´¥")

# æå–æ ‡ç­¾
def extract_tag_names(item_data):
    """
    å…¼å®¹æ–°æ—§ç‰ˆ Emby API æå–æ ‡ç­¾åã€‚
    """
    tags_set = set()

    # 1. å°è¯•æå– TagItems (æ–°ç‰ˆ/è¯¦ç»†ç‰ˆ)
    tag_items = item_data.get('TagItems')
    if isinstance(tag_items, list):
        for t in tag_items:
            if isinstance(t, dict):
                name = t.get('Name')
                if name:
                    tags_set.add(name)
            elif isinstance(t, str) and t:
                tags_set.add(t)
    
    # 2. å°è¯•æå– Tags (æ—§ç‰ˆ/ç®€ç•¥ç‰ˆ)
    tags = item_data.get('Tags')
    if isinstance(tags, list):
        for t in tags:
            if t:
                tags_set.add(str(t))
    
    return list(tags_set)

# --- æå–åŸå§‹åˆ†çº§æ•°æ®ï¼Œä¸è¿›è¡Œä»»ä½•è®¡ç®— ---
def _extract_and_map_tmdb_ratings(tmdb_details, item_type):
    """
    ä» TMDb è¯¦æƒ…ä¸­æå–æ‰€æœ‰å›½å®¶çš„åˆ†çº§æ•°æ®ï¼Œå¹¶æ‰§è¡Œæ™ºèƒ½æ˜ å°„ï¼ˆè¡¥å…¨ US åˆ†çº§ï¼‰ã€‚
    è¿”å›: å­—å…¸ { 'US': 'R', 'CN': 'PG-13', ... }
    """
    if not tmdb_details:
        return {}

    ratings_map = {}
    origin_country = None

    # 1. æå–åŸå§‹æ•°æ®
    if item_type == 'Movie':
        # ç”µå½±ï¼šåœ¨ release_dates ä¸­æŸ¥æ‰¾
        results = tmdb_details.get('release_dates', {}).get('results', [])
        for r in results:
            country = r.get('iso_3166_1')
            if not country: continue
            cert = None
            for release in r.get('release_dates', []):
                if release.get('certification'):
                    cert = release.get('certification')
                    break 
            if cert:
                ratings_map[country] = cert
        
        # è·å–åŸäº§å›½
        p_countries = tmdb_details.get('production_countries', [])
        if p_countries:
            origin_country = p_countries[0].get('iso_3166_1')

    elif item_type == 'Series':
        # å‰§é›†ï¼šåœ¨ content_ratings ä¸­æŸ¥æ‰¾
        results = tmdb_details.get('content_ratings', {}).get('results', [])
        for r in results:
            country = r.get('iso_3166_1')
            rating = r.get('rating')
            if country and rating:
                ratings_map[country] = rating
        
        # è·å–åŸäº§å›½
        o_countries = tmdb_details.get('origin_country', [])
        if o_countries:
            origin_country = o_countries[0]

    # 2. â˜…â˜…â˜… æ‰§è¡Œæ˜ å°„é€»è¾‘ (æ ¸å¿ƒä¿®å¤) â˜…â˜…â˜…
    # å¦‚æœå·²ç»æœ‰ US åˆ†çº§ï¼Œç›´æ¥è¿”å›ï¼Œä¸åšæ˜ å°„ï¼ˆä»¥ TMDb åŸç”Ÿ US ä¸ºå‡†ï¼Œæˆ–è€…ä½ å¯ä»¥é€‰æ‹©è¦†ç›–ï¼‰
    # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©ï¼šå¦‚æœåŸç”Ÿæ²¡æœ‰ USï¼Œæˆ–è€…æˆ‘ä»¬æƒ³å¼ºåˆ¶æ£€æŸ¥æ˜ å°„ï¼Œå°±æ‰§è¡Œæ˜ å°„ã€‚
    # ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬æ€»æ˜¯å°è¯•è®¡ç®—æ˜ å°„å€¼ï¼Œå¦‚æœè®¡ç®—å‡ºæ¥äº†ï¼Œå°±è¡¥å…¨è¿›å»ã€‚
    
    target_us_code = None
    
    # åŠ è½½é…ç½®
    rating_mapping = settings_db.get_setting('rating_mapping') or utils.DEFAULT_RATING_MAPPING
    priority_list = settings_db.get_setting('rating_priority') or utils.DEFAULT_RATING_PRIORITY

    # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾
    for p_country in priority_list:
        search_country = origin_country if p_country == 'ORIGIN' else p_country
        if not search_country: continue
        
        if search_country in ratings_map:
            source_rating = ratings_map[search_country]
            
            # å°è¯•æ˜ å°„
            if isinstance(rating_mapping, dict) and search_country in rating_mapping and 'US' in rating_mapping:
                current_val = None
                # æ‰¾æºåˆ†çº§å¯¹åº”çš„ Value
                for rule in rating_mapping[search_country]:
                    if str(rule['code']).strip().upper() == str(source_rating).strip().upper():
                        current_val = rule.get('emby_value')
                        break
                
                # æ‰¾ US å¯¹åº”çš„ Code
                if current_val is not None:
                    valid_us_rules = []
                    for rule in rating_mapping['US']:
                        r_code = rule.get('code', '')
                        # ç®€å•çš„ç±»å‹è¿‡æ»¤
                        if item_type == 'Movie' and r_code.startswith('TV-'): continue
                        valid_us_rules.append(rule)
                    
                    for rule in valid_us_rules:
                        # å°è¯•ç²¾ç¡®åŒ¹é…
                        try:
                            if int(rule.get('emby_value')) == int(current_val):
                                target_us_code = rule['code']
                                break
                        except: continue
                    
                    # å¦‚æœæ²¡ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•å‘ä¸Šå…¼å®¹ (+1)
                    if not target_us_code:
                        for rule in valid_us_rules:
                            try:
                                if int(rule.get('emby_value')) == int(current_val) + 1:
                                    target_us_code = rule['code']
                                    break
                            except: continue

            if target_us_code:
                break
            # å¦‚æœæ²¡æ˜ å°„æˆåŠŸï¼Œä½†è¿™æ˜¯é«˜ä¼˜å…ˆçº§å›½å®¶ï¼Œä¸”æ²¡æœ‰ US åˆ†çº§ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘ç›´æ¥ç”¨å®ƒçš„åˆ†çº§åšå…œåº•ï¼ˆè§†éœ€æ±‚è€Œå®šï¼‰
            # è¿™é‡Œæˆ‘ä»¬åªåšæ˜ å°„è¡¥å…¨

    # 3. è¡¥å…¨ US åˆ†çº§
    if target_us_code:
        # å¼ºåˆ¶è¦†ç›–/æ·»åŠ  US åˆ†çº§
        ratings_map['US'] = target_us_code

    return ratings_map

# â˜…â˜…â˜… é‡é‡çº§çš„å…ƒæ•°æ®ç¼“å­˜å¡«å……ä»»åŠ¡ (å†…å­˜ä¼˜åŒ–ç‰ˆ) â˜…â˜…â˜…
def task_populate_metadata_cache(processor, batch_size: int = 50, force_full_update: bool = False):
    """
    - é‡é‡çº§çš„å…ƒæ•°æ®ç¼“å­˜å¡«å……ä»»åŠ¡ (ç±»å‹å®‰å…¨ç‰ˆ)ã€‚
    - ä¿®å¤ï¼šå½»åº•è§£å†³ TMDb ID åœ¨ç”µå½±å’Œå‰§é›†é—´å†²çªçš„é—®é¢˜ã€‚
    - ä¿®å¤ï¼šå®Œå–„ç¦»çº¿æ£€æµ‹é€»è¾‘ï¼Œç¡®ä¿æ¶ˆå¤±çš„ç”µå½±/å‰§é›†èƒ½è¢«æ­£ç¡®æ ‡è®°ä¸ºç¦»çº¿ã€‚
    - ä¼˜åŒ–ï¼šå¢åŠ è¯¦ç»†çš„è·³è¿‡ç»Ÿè®¡ï¼Œè§£é‡Šæ•°é‡å·®å¼‚ã€‚
    """
    task_name = "åŒæ­¥åª’ä½“å…ƒæ•°æ®"
    sync_mode = "æ·±åº¦åŒæ­¥ (å…¨é‡)" if force_full_update else "å¿«é€ŸåŒæ­¥ (å¢é‡)"
    logger.info(f"--- æ¨¡å¼: {sync_mode} (åˆ†æ‰¹å¤§å°: {batch_size}) ---")
    
    total_updated_count = 0
    total_offline_count = 0

    try:
        task_manager.update_status_from_thread(0, f"é˜¶æ®µ1/3: å»ºç«‹å·®å¼‚åŸºå‡† ({sync_mode})...")
        
        libs_to_process_ids = processor.config.get("libraries_to_process", [])
        if not libs_to_process_ids:
            raise ValueError("æœªåœ¨é…ç½®ä¸­æŒ‡å®šè¦å¤„ç†çš„åª’ä½“åº“ã€‚")

        # --- 1. å‡†å¤‡åŸºç¡€æ•°æ® ---
        known_emby_status = {}      # {emby_id: is_online}
        emby_sid_to_tmdb_id = {}    # {emby_series_id: tmdb_id}
        tmdb_key_to_emby_ids = defaultdict(set) 
        
        with connection.get_db_connection() as conn:
            cursor = conn.cursor()
            
            # A. é¢„åŠ è½½æ˜ å°„
            cursor.execute("""
                SELECT tmdb_id, item_type, jsonb_array_elements_text(emby_item_ids_json) as eid 
                FROM media_metadata 
                WHERE item_type IN ('Movie', 'Series')
            """)
            for row in cursor.fetchall():
                e_id, t_id, i_type = row['eid'], row['tmdb_id'], row['item_type']
                if i_type == 'Series':
                    emby_sid_to_tmdb_id[e_id] = t_id
                if t_id:
                    tmdb_key_to_emby_ids[(t_id, i_type)].add(e_id)

            # B. è·å–åœ¨çº¿çŠ¶æ€
            if not force_full_update:
                cursor.execute("""
                    SELECT jsonb_array_elements_text(emby_item_ids_json) AS emby_id, in_library
                    FROM media_metadata 
                """)
                known_emby_status = {row['emby_id']: row['in_library'] for row in cursor.fetchall()}
                
                cursor.execute("""
                    SELECT COUNT(*) as total, SUM(CASE WHEN in_library THEN 1 ELSE 0 END) as online 
                    FROM media_metadata
                """)
                stat_row = cursor.fetchone()
                total_items = stat_row['total'] if stat_row else 0
                online_items = stat_row['online'] if stat_row and stat_row['online'] is not None else 0
                
                logger.info(f"  âœ æœ¬åœ°æ•°æ®åº“å…±å­˜å‚¨ {total_items} ä¸ªåª’ä½“é¡¹ (å…¶ä¸­åœ¨çº¿: {online_items}, ç¦»çº¿: {total_items - online_items})ã€‚")

        logger.info("  âœ æ­£åœ¨é¢„åŠ è½½ Emby æ–‡ä»¶å¤¹è·¯å¾„æ˜ å°„...")
        folder_map = emby.get_all_folder_mappings(processor.emby_url, processor.emby_api_key)
        logger.info(f"  âœ åŠ è½½äº† {len(folder_map)} ä¸ªæ–‡ä»¶å¤¹è·¯å¾„èŠ‚ç‚¹ã€‚")

        # --- 2. æ‰«æ Emby (æµå¼å¤„ç†) ---
        task_manager.update_status_from_thread(10, f"é˜¶æ®µ2/3: æ‰«æ Emby å¹¶è®¡ç®—å·®å¼‚...")
        
        top_level_items_map = defaultdict(list)       
        series_to_seasons_map = defaultdict(list)     
        series_to_episode_map = defaultdict(list)     
        emby_id_to_lib_id = {}
        id_to_parent_map = {}
        lib_id_to_guid_map = {}
        
        try:
            import requests
            lib_resp = requests.get(f"{processor.emby_url}/Library/VirtualFolders", params={"api_key": processor.emby_api_key})
            if lib_resp.status_code == 200:
                for lib in lib_resp.json():
                    l_id = str(lib.get('ItemId'))
                    l_guid = str(lib.get('Guid'))
                    if l_id and l_guid:
                        lib_id_to_guid_map[l_id] = l_guid
        except Exception as e:
            logger.error(f"è·å–åº“ GUID æ˜ å°„å¤±è´¥: {e}")

        dirty_keys = set() 
        current_scan_emby_ids = set() 
        pending_children = [] 

        # â˜…â˜…â˜… æ–°å¢è®¡æ•°å™¨ â˜…â˜…â˜…
        scan_count = 0
        skipped_no_tmdb = 0
        skipped_other_type = 0
        skipped_clean = 0

        req_fields = "ProviderIds,Type,DateCreated,Name,OriginalTitle,PremiereDate,CommunityRating,Genres,Studios,Tags,TagItems,DateModified,OfficialRating,ProductionYear,Path,PrimaryImageAspectRatio,Overview,MediaStreams,Container,Size,SeriesId,ParentIndexNumber,IndexNumber,ParentId,RunTimeTicks,_SourceLibraryId"

        item_generator = emby.fetch_all_emby_items_generator(
            base_url=processor.emby_url, 
            api_key=processor.emby_api_key, 
            library_ids=libs_to_process_ids, 
            fields=req_fields
        )

        for item in item_generator:
            scan_count += 1
            if scan_count % 5000 == 0:
                task_manager.update_status_from_thread(10, f"æ­£åœ¨ç´¢å¼• Emby åº“ ({scan_count} å·²æ‰«æ)...")
            
            item_id = str(item.get("Id"))
            parent_id = str(item.get("ParentId"))
            if item_id and parent_id:
                id_to_parent_map[item_id] = parent_id
            
            if not item_id: 
                continue

            emby_id_to_lib_id[item_id] = item.get('_SourceLibraryId')
            
            item_type = item.get("Type")
            tmdb_id = item.get("ProviderIds", {}).get("Tmdb")

            # 1. è®°å½•æ‰€æœ‰æ‰«æåˆ°çš„ ID (ç”¨äºåå‘æ£€æµ‹ç¦»çº¿)
            # æ³¨æ„ï¼šåªæœ‰æˆ‘ä»¬å…³å¿ƒçš„ç±»å‹æ‰è®°å½•ï¼Œå¦åˆ™ä¼šè¯¯åˆ¤ç¦»çº¿
            if item_type in ["Movie", "Series", "Season", "Episode"]:
                current_scan_emby_ids.add(item_id)
            else:
                skipped_other_type += 1
                continue # è·³è¿‡éåª’ä½“ç±»å‹ (Folder, BoxSetç­‰)

            # å®æ—¶æ›´æ–°æ˜ å°„
            if item_type == "Series" and tmdb_id:
                emby_sid_to_tmdb_id[item_id] = str(tmdb_id)
            
            if item_type in ["Movie", "Series"] and tmdb_id:
                tmdb_key_to_emby_ids[(str(tmdb_id), item_type)].add(item_id)

            # è·³è¿‡åˆ¤æ–­ (å·²å­˜åœ¨ä¸”åœ¨çº¿)
            is_clean = False
            if not force_full_update:
                if known_emby_status.get(item_id) is True:
                    is_clean = True
            
            if is_clean:
                skipped_clean += 1
                continue 

            # â˜…â˜…â˜… è„æ•°æ®å¤„ç† â˜…â˜…â˜…
            
            # A. é¡¶å±‚åª’ä½“
            if item_type in ["Movie", "Series"]:
                if tmdb_id:
                    composite_key = (str(tmdb_id), item_type)
                    top_level_items_map[composite_key].append(item)
                    dirty_keys.add(composite_key)
                else:
                    skipped_no_tmdb += 1 # è®°å½•æ—  TMDb ID çš„é¡¹ç›®

            # B. å­é›†åª’ä½“
            elif item_type in ['Season', 'Episode']:
                s_id = str(item.get('SeriesId') or item.get('ParentId')) if item_type == 'Season' else str(item.get('SeriesId'))
                
                if item_type == 'Season':
                    if s_id: series_to_seasons_map[s_id].append(item)
                else:
                    if s_id: series_to_episode_map[s_id].append(item)

                if s_id and s_id in emby_sid_to_tmdb_id:
                    dirty_keys.add((emby_sid_to_tmdb_id[s_id], 'Series'))
                elif s_id:
                    pending_children.append((s_id, item_type))

        # å¤„ç†å­¤å„¿åˆ†é›†
        for s_id, _ in pending_children:
            if s_id in emby_sid_to_tmdb_id:
                dirty_keys.add((emby_sid_to_tmdb_id[s_id], 'Series'))

        gc.collect()

        # --- 3. åå‘å·®å¼‚æ£€æµ‹ (åˆ é™¤) ---
        if not force_full_update:
            active_db_ids = {k for k, v in known_emby_status.items() if v is True}
            missing_emby_ids = active_db_ids - current_scan_emby_ids
            
            del known_emby_status
            del active_db_ids
            del current_scan_emby_ids
            gc.collect()

            if missing_emby_ids:
                logger.info(f"  âœ æ£€æµ‹åˆ° {len(missing_emby_ids)} ä¸ª Emby ID å·²æ¶ˆå¤±ï¼Œæ­£åœ¨å¤„ç†ç¦»çº¿æ ‡è®°...")
                missing_ids_list = list(missing_emby_ids)
                
                with connection.get_db_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT tmdb_id, item_type, parent_series_tmdb_id
                        FROM media_metadata 
                        WHERE in_library = TRUE 
                          AND EXISTS (
                              SELECT 1 
                              FROM jsonb_array_elements_text(emby_item_ids_json) as eid 
                              WHERE eid = ANY(%s)
                          )
                    """, (missing_ids_list,))
                    
                    rows = cursor.fetchall()
                    direct_offline_tmdb_ids = []
                    affected_parent_ids = set()
                    
                    for row in rows:
                        r_type = row['item_type']
                        r_tmdb = row['tmdb_id']
                        r_parent = row['parent_series_tmdb_id']
                        
                        if r_type in ['Movie', 'Series']:
                            direct_offline_tmdb_ids.append(r_tmdb)
                        elif r_type in ['Season', 'Episode'] and r_parent:
                            affected_parent_ids.add(r_parent)

                    if direct_offline_tmdb_ids:
                        logger.info(f"  âœ æ­£åœ¨æ ‡è®° {len(direct_offline_tmdb_ids)} ä¸ªé¡¶å±‚é¡¹ç›®ä¸ºç¦»çº¿...")
                        cursor.execute("""
                            UPDATE media_metadata
                            SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = '[]'::jsonb
                            WHERE tmdb_id = ANY(%s) AND item_type IN ('Movie', 'Series')
                        """, (direct_offline_tmdb_ids,))
                        total_offline_count += cursor.rowcount
                        
                    if affected_parent_ids:
                        logger.info(f"  âœ å› åˆ†é›†æ¶ˆå¤±ï¼Œå°† {len(affected_parent_ids)} ä¸ªçˆ¶å‰§é›†åŠ å…¥åˆ·æ–°é˜Ÿåˆ—...")
                        for pid in affected_parent_ids:
                            dirty_keys.add((pid, 'Series'))
                    
                    conn.commit()

        # â˜…â˜…â˜… æ‰“å°è¯¦ç»†ç»Ÿè®¡æ—¥å¿— â˜…â˜…â˜…
        logger.info(f"  âœ Emby æ‰«æå®Œæˆï¼Œå…±æ‰«æ {scan_count} ä¸ªé¡¹ã€‚")
        logger.info(f"    - å·²å…¥åº“: {skipped_clean}")
        logger.info(f"    - å·²è·³è¿‡: {skipped_no_tmdb + skipped_other_type} (å« {skipped_no_tmdb} ä¸ªæ— ID, {skipped_other_type} ä¸ªéåª’ä½“)")
        logger.info(f"    - éœ€åŒæ­¥: {len(dirty_keys)}")

        # --- 4. ç¡®å®šå¤„ç†é˜Ÿåˆ— (æ— éœ€çŒœæµ‹ç±»å‹) ---
        items_to_process = []
        
        # ç›´æ¥éå† dirty_keysï¼Œé‡Œé¢å·²ç»åŒ…å«äº†å‡†ç¡®çš„ (ID, Type)
        for (tmdb_id, item_type) in dirty_keys:
            
            # ä½¿ç”¨å¤åˆé”®æŸ¥æ‰¾å…³è”çš„ Emby IDs
            related_emby_ids = tmdb_key_to_emby_ids.get((tmdb_id, item_type), set())
            
            if not related_emby_ids:
                continue

            items_to_process.append({
                'tmdb_id': tmdb_id,
                'emby_ids': list(related_emby_ids),
                'type': item_type, # ç›´æ¥ä½¿ç”¨ key é‡Œçš„ typeï¼Œç»å¯¹å‡†ç¡®
                'refetch': True 
            })

        total_to_process = len(items_to_process)
        task_manager.update_status_from_thread(20, f"é˜¶æ®µ3/3: æ­£åœ¨åŒæ­¥ {total_to_process} ä¸ªå˜æ›´é¡¹ç›®...")
        logger.info(f"  âœ æœ€ç»ˆå¤„ç†é˜Ÿåˆ—: {total_to_process} ä¸ªé¡¶å±‚é¡¹ç›®")

        # --- 5. æ‰¹é‡å¤„ç† ---
        processed_count = 0
        for i in range(0, total_to_process, batch_size):
            if processor.is_stop_requested(): break
            batch_tasks = items_to_process[i:i + batch_size]
            
            batch_item_groups = []
            
            # é¢„å¤„ç†ï¼šæ‹‰å– refetch çš„æ•°æ®
            for task in batch_tasks:
                try:
                    target_emby_ids = task['emby_ids']
                    item_type = task['type']
                    
                    # 1. æ‰¹é‡è·å–è¿™äº› Emby ID çš„è¯¦æƒ…
                    top_items = emby.get_emby_items_by_id(
                        base_url=processor.emby_url,
                        api_key=processor.emby_api_key,
                        user_id=processor.emby_user_id,
                        item_ids=target_emby_ids,
                        fields=req_fields
                    )
                    
                    if not top_items: continue

                    # å› ä¸º get_emby_items_by_id é‡æ–°æ‹‰å–çš„æ•°æ®æ²¡æœ‰è¿™ä¸ªå­—æ®µï¼Œæˆ‘ä»¬éœ€è¦ä»ä¹‹å‰çš„æ˜ å°„ä¸­è¡¥å›å»
                    for item in top_items:
                        eid = str(item.get('Id'))
                        if eid in emby_id_to_lib_id:
                            item['_SourceLibraryId'] = emby_id_to_lib_id[eid]

                    # 2. å¦‚æœæ˜¯å‰§é›†ï¼Œè¿˜éœ€è¦æ‹‰å–æ¯ä¸ªå‰§é›†çš„å­é›†
                    if item_type == 'Series':
                        full_group = []
                        full_group.extend(top_items)
                        
                        # æ¸…ç©ºæ—§çš„å­é›†ç¼“å­˜ï¼Œé˜²æ­¢é‡å¤
                        for e_id in target_emby_ids:
                            series_to_seasons_map[e_id] = []
                            series_to_episode_map[e_id] = []
                        
                        children_gen = emby.fetch_all_emby_items_generator(
                            base_url=processor.emby_url,
                            api_key=processor.emby_api_key,
                            library_ids=target_emby_ids, 
                            fields=req_fields
                        )
                        
                        children_list = list(children_gen)
                        for child in children_list:
                            parent_series_id = str(child.get('SeriesId') or child.get('ParentId'))
                            if parent_series_id and parent_series_id in emby_id_to_lib_id:
                                real_lib_id = emby_id_to_lib_id[parent_series_id]
                                child['_SourceLibraryId'] = real_lib_id 
                        full_group.extend(children_list)
                        
                        # é‡æ–°å¡«å…… map
                        for child in children_list:
                            ct = child.get('Type')
                            pid = str(child.get('SeriesId') or child.get('ParentId'))
                            if pid:
                                if ct == 'Season': series_to_seasons_map[pid].append(child)
                                elif ct == 'Episode': series_to_episode_map[pid].append(child)
                        
                        batch_item_groups.append(full_group)
                    
                    else:
                        # ç”µå½±ç›´æ¥æ·»åŠ 
                        batch_item_groups.append(top_items)

                except Exception as e:
                    logger.error(f"å¤„ç†é¡¹ç›® {task.get('tmdb_id')} å¤±è´¥: {e}")

            # --- ä»¥ä¸‹é€»è¾‘ä¿æŒä¸å˜ (å¹¶å‘è·å– TMDB å’Œ å†™å…¥ DB) ---
            
            tmdb_details_map = {}
            def fetch_tmdb_details(item_group):
                if not item_group: return None, None
                item = item_group[0]
                t_id = item.get("ProviderIds", {}).get("Tmdb")
                i_type = item.get("Type")
                if not t_id: return None, None
                details = None
                try:
                    if i_type == 'Movie': details = tmdb.get_movie_details(t_id, processor.tmdb_api_key)
                    elif i_type == 'Series': details = tmdb.get_tv_details(t_id, processor.tmdb_api_key)
                except Exception: pass
                return str(t_id), details

            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = {executor.submit(fetch_tmdb_details, grp): grp for grp in batch_item_groups}
                for future in concurrent.futures.as_completed(futures):
                    t_id_str, details = future.result()
                    if t_id_str and details: tmdb_details_map[t_id_str] = details

            metadata_batch = []
            series_ids_processed_in_batch = set()

            for item_group in batch_item_groups:
                if not item_group: continue
                item = item_group[0]
                tmdb_id_str = str(item.get("ProviderIds", {}).get("Tmdb"))
                item_type = item.get("Type")
                tmdb_details = tmdb_details_map.get(tmdb_id_str)
                
                # --- 1. æ„å»ºé¡¶å±‚è®°å½• ---
                asset_details_list = []
                if item_type in ["Movie", "Series"]:
                    for v in item_group:
                        # ä»…å¤„ç†å½“å‰ç±»å‹çš„é¡¹ç›® (é˜²æ­¢ Series ç»„é‡Œæ··å…¥ Season/Episode)
                        if v.get('Type') != item_type:
                            continue
                            
                        source_lib_id = str(v.get('_SourceLibraryId'))
                        current_lib_guid = lib_id_to_guid_map.get(source_lib_id)

                        details = parse_full_asset_details(
                            v, 
                            id_to_parent_map=id_to_parent_map, 
                            library_guid=current_lib_guid
                        )
                        details['source_library_id'] = source_lib_id
                        asset_details_list.append(details)

                emby_runtime = round(item['RunTimeTicks'] / 600000000) if item.get('RunTimeTicks') else None

                # æå–å‘è¡Œæ—¥æœŸ 
                emby_date = item.get('PremiereDate')
                tmdb_date = None
                if tmdb_details:
                    if item_type == 'Movie': 
                        tmdb_date = tmdb_details.get('release_date')
                    elif item_type == 'Series': 
                        tmdb_date = tmdb_details.get('first_air_date')
                
                final_release_date = emby_date or tmdb_date
                # æå–å…¨é‡åˆ†çº§æ•°æ®
                raw_ratings_map = _extract_and_map_tmdb_ratings(tmdb_details, item_type)
                # åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²ï¼Œå‡†å¤‡å­˜å…¥æ•°æ®åº“
                rating_json_str = json.dumps(raw_ratings_map, ensure_ascii=False)
                top_record = {
                    "tmdb_id": tmdb_id_str, "item_type": item_type, "title": item.get('Name'),
                    "original_title": item.get('OriginalTitle'), "release_year": item.get('ProductionYear'),
                    "original_language": tmdb_details.get('original_language') if tmdb_details else None,
                    "in_library": True, 
                    "subscription_status": "NONE",
                    "emby_item_ids_json": json.dumps(list(set(v.get('Id') for v in item_group if v.get('Id') and v.get('Type') == item_type)), ensure_ascii=False),
                    "asset_details_json": json.dumps(asset_details_list, ensure_ascii=False),
                    "rating": item.get('CommunityRating'),
                    "date_added": item.get('DateCreated'),
                    "release_date": final_release_date,
                    "genres_json": json.dumps(item.get('Genres', []), ensure_ascii=False),
                    "tags_json": json.dumps(extract_tag_names(item), ensure_ascii=False),
                    "official_rating_json": rating_json_str,
                    "runtime_minutes": emby_runtime if (item_type == 'Movie' and emby_runtime) else tmdb_details.get('runtime') if (item_type == 'Movie' and tmdb_details) else None
                }
                if tmdb_details:
                    top_record['poster_path'] = tmdb_details.get('poster_path')
                    top_record['overview'] = tmdb_details.get('overview')
                    if tmdb_details.get('vote_average') is not None:
                        top_record['rating'] = tmdb_details.get('vote_average')
                    # 1. è·å–åŸºç¡€åˆ¶ä½œå…¬å¸
                    raw_studios = tmdb_details.get('production_companies', []) or []

                    # 2. å¦‚æœæ˜¯ç”µè§†å‰§ï¼Œè¿½åŠ  Networks (ç”µè§†å°/æµåª’ä½“å¹³å°)
                    if item_type == 'Series':
                        networks = tmdb_details.get('networks', []) or []
                        raw_studios.extend(networks)

                    # 3. å»é‡ (ä½¿ç”¨å­—å…¸ä»¥ ID ä¸ºé”®è¿›è¡Œå»é‡) å¹¶æ ¼å¼åŒ–
                    unique_studios_map = {}
                    for s in raw_studios:
                        s_id = s.get('id')
                        s_name = s.get('name')
                        if s_name:
                            # å¦‚æœ ID å†²çªï¼Œåæ¥çš„è¦†ç›–å‰é¢çš„ï¼ˆé€šå¸¸ Networks åœ¨åï¼Œä¿ç•™ Networks æ›´åˆç†ï¼‰
                            unique_studios_map[s_id] = {'id': s_id, 'name': s_name}

                    top_record['studios_json'] = json.dumps(list(unique_studios_map.values()), ensure_ascii=False)
                    if item_type == 'Movie':
                        top_record['runtime_minutes'] = tmdb_details.get('runtime')
                    
                    directors, countries, keywords = [], [], []
                    if item_type == 'Movie':
                        credits_data = tmdb_details.get("credits", {}) or tmdb_details.get("casts", {})
                        directors = [{'id': p.get('id'), 'name': p.get('name')} for p in credits_data.get('crew', []) if p.get('job') == 'Director']
                        countries = [c.get('iso_3166_1') for c in tmdb_details.get('production_countries', []) if c.get('iso_3166_1')]
                        keywords_data = tmdb_details.get('keywords', {})
                        keyword_list = keywords_data.get('keywords', []) if isinstance(keywords_data, dict) else []
                        keywords = [{'id': k.get('id'), 'name': k.get('name')} for k in keyword_list if k.get('name')]
                    elif item_type == 'Series':
                        directors = [{'id': c.get('id'), 'name': c.get('name')} for c in tmdb_details.get('created_by', [])]
                        countries = tmdb_details.get('origin_country', [])
                        keywords_data = tmdb_details.get('keywords', {})
                        keyword_list = keywords_data.get('results', []) if isinstance(keywords_data, dict) else []
                        keywords = [{'id': k.get('id'), 'name': k.get('name')} for k in keyword_list if k.get('name')]
                    top_record['directors_json'] = json.dumps(directors, ensure_ascii=False)
                    top_record['countries_json'] = json.dumps(countries, ensure_ascii=False)
                    top_record['keywords_json'] = json.dumps(keywords, ensure_ascii=False)
                else:
                    top_record['poster_path'] = None
                    top_record['studios_json'] = '[]'
                    top_record['directors_json'] = '[]'; top_record['countries_json'] = '[]'; top_record['keywords_json'] = '[]'

                metadata_batch.append(top_record)

                # --- 2. å¤„ç† Series çš„å­é›† ---
                if item_type == "Series":
                    series_ids_processed_in_batch.add(tmdb_id_str)
                    
                    series_emby_ids = [str(v.get('Id')) for v in item_group if v.get('Id')]
                    my_seasons = []
                    my_episodes = []
                    for s_id in series_emby_ids:
                        my_seasons.extend(series_to_seasons_map.get(s_id, []))
                        my_episodes.extend(series_to_episode_map.get(s_id, []))
                    
                    tmdb_children_map = {}
                    processed_season_numbers = set()
                    
                    if tmdb_details and 'seasons' in tmdb_details:
                        for s_info in tmdb_details.get('seasons', []):
                            try:
                                s_num = int(s_info.get('season_number'))
                            except (ValueError, TypeError):
                                continue
                            
                            matched_emby_seasons = []
                            for s in my_seasons:
                                try:
                                    if int(s.get('IndexNumber')) == s_num:
                                        matched_emby_seasons.append(s)
                                except (ValueError, TypeError):
                                    continue
                            
                            if matched_emby_seasons:
                                processed_season_numbers.add(s_num)
                                real_season_tmdb_id = str(s_info.get('id'))
                                season_poster = s_info.get('poster_path')
                                if not season_poster and tmdb_details:
                                    season_poster = tmdb_details.get('poster_path')

                                # æå–å­£å‘è¡Œæ—¥æœŸ
                                s_release_date = s_info.get('air_date')
                                
                                if not s_release_date and matched_emby_seasons:
                                    s_release_date = matched_emby_seasons[0].get('PremiereDate')
                                
                                # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œéå†è¯¥å­£ä¸‹çš„åˆ†é›†æ‰¾æœ€æ—©çš„
                                if not s_release_date:
                                    # ç­›é€‰å‡ºå±äºå½“å‰å­£(s_num)ä¸”æœ‰æ—¥æœŸçš„åˆ†é›†
                                    ep_dates = [
                                        e.get('PremiereDate') for e in my_episodes 
                                        if e.get('ParentIndexNumber') == s_num and e.get('PremiereDate')
                                    ]
                                    if ep_dates:
                                        # å–æœ€æ—©çš„æ—¥æœŸä½œä¸ºå­£çš„å‘è¡Œæ—¥æœŸ
                                        s_release_date = min(ep_dates)
                                season_record = {
                                    "tmdb_id": real_season_tmdb_id,
                                    "item_type": "Season",
                                    "parent_series_tmdb_id": tmdb_id_str,
                                    "season_number": s_num,
                                    "title": s_info.get('name'),
                                    "overview": s_info.get('overview'),
                                    "poster_path": season_poster,
                                    "rating": s_info.get('vote_average'),
                                    "in_library": True,
                                    "release_date": s_release_date,
                                    "subscription_status": "NONE",
                                    "emby_item_ids_json": json.dumps([s.get('Id') for s in matched_emby_seasons]),
                                    "tags_json": json.dumps(extract_tag_names(matched_emby_seasons[0]) if matched_emby_seasons else [], ensure_ascii=False),
                                    "ignore_reason": None
                                }
                                metadata_batch.append(season_record)
                                tmdb_children_map[f"S{s_num}"] = s_info

                                has_eps = any(e.get('ParentIndexNumber') == s_num for e in my_episodes)
                                if has_eps:
                                    try:
                                        s_details = tmdb.get_tv_season_details(tmdb_id_str, s_num, processor.tmdb_api_key)
                                        if s_details and 'episodes' in s_details:
                                            for ep in s_details['episodes']:
                                                if ep.get('episode_number') is not None:
                                                    tmdb_children_map[f"S{s_num}E{ep.get('episode_number')}"] = ep
                                    except: pass

                    # B. å…œåº•å¤„ç†
                    for s in my_seasons:
                        try:
                            s_num = int(s.get('IndexNumber'))
                        except (ValueError, TypeError):
                            continue

                        if s_num not in processed_season_numbers:
                            # å…œåº•é€»è¾‘ä¹ŸåŠ ä¸Šåˆ†é›†æ—¥æœŸæ¨æ–­ 
                            s_release_date = s.get('PremiereDate')
                            if not s_release_date:
                                ep_dates = [
                                    e.get('PremiereDate') for e in my_episodes 
                                    if e.get('ParentIndexNumber') == s_num and e.get('PremiereDate')
                                ]
                                if ep_dates:
                                    s_release_date = min(ep_dates)
                            fallback_season_tmdb_id = f"{tmdb_id_str}-S{s_num}"
                            season_record = {
                                "tmdb_id": fallback_season_tmdb_id,
                                "item_type": "Season",
                                "parent_series_tmdb_id": tmdb_id_str,
                                "season_number": s_num,
                                "title": s.get('Name') or f"Season {s_num}",
                                "overview": None,
                                "poster_path": tmdb_details.get('poster_path') if tmdb_details else None,
                                "in_library": True,
                                "release_date": s_release_date,
                                "subscription_status": "NONE",
                                "emby_item_ids_json": json.dumps([s.get('Id')]),
                                "tags_json": json.dumps(extract_tag_names(s), ensure_ascii=False),
                                "ignore_reason": "Local Season Only"
                            }
                            metadata_batch.append(season_record)
                            processed_season_numbers.add(s_num)

                    # C. å¤„ç†åˆ†é›†
                    ep_grouped = defaultdict(list)
                    for ep in my_episodes:
                        s_n, e_n = ep.get('ParentIndexNumber'), ep.get('IndexNumber')
                        if s_n is not None and e_n is not None:
                            ep_grouped[(s_n, e_n)].append(ep)
                    
                    for (s_n, e_n), versions in ep_grouped.items():
                        emby_ep = versions[0]
                        emby_ep_runtime = round(emby_ep['RunTimeTicks'] / 600000000) if emby_ep.get('RunTimeTicks') else None
                        lookup_key = f"S{s_n}E{e_n}"
                        tmdb_ep_info = tmdb_children_map.get(lookup_key)
                        
                        ep_asset_details_list = []
                        for v in versions:
                            details = parse_full_asset_details(v) 
                            ep_asset_details_list.append(details)

                        # æå–åˆ†é›†å‘è¡Œæ—¥æœŸ 
                        ep_release_date = emby_ep.get('PremiereDate')
                        if not ep_release_date and tmdb_ep_info:
                            ep_release_date = tmdb_ep_info.get('air_date')
                        child_record = {
                            "item_type": "Episode",
                            "parent_series_tmdb_id": tmdb_id_str,
                            "season_number": s_n,
                            "episode_number": e_n,
                            "in_library": True,
                            "release_date": ep_release_date,
                            "rating": emby_ep.get('CommunityRating'),
                            "emby_item_ids_json": json.dumps([v.get('Id') for v in versions]),
                            "asset_details_json": json.dumps(ep_asset_details_list, ensure_ascii=False),
                            "tags_json": json.dumps(extract_tag_names(versions[0]), ensure_ascii=False),
                            "ignore_reason": None
                        }

                        if tmdb_ep_info and tmdb_ep_info.get('id'):
                            child_record['tmdb_id'] = str(tmdb_ep_info.get('id'))
                            child_record['title'] = tmdb_ep_info.get('name')
                            child_record['overview'] = tmdb_ep_info.get('overview')
                            child_record['poster_path'] = tmdb_ep_info.get('still_path')
                            child_record['runtime_minutes'] = emby_ep_runtime if emby_ep_runtime else tmdb_ep_info.get('runtime')
                            if tmdb_ep_info.get('vote_average') is not None:
                                child_record['rating'] = tmdb_ep_info.get('vote_average')
                        else:
                            child_record['tmdb_id'] = f"{tmdb_id_str}-S{s_n}E{e_n}"
                            child_record['title'] = versions[0].get('Name')
                            child_record['overview'] = versions[0].get('Overview')
                            child_record['runtime_minutes'] = emby_ep_runtime
                        
                        metadata_batch.append(child_record)

            # 7. å†™å…¥æ•°æ®åº“ & å­é›†ç¦»çº¿å¯¹è´¦
            if metadata_batch:
                total_updated_count += len(metadata_batch)

                with connection.get_db_connection() as conn:
                    cursor = conn.cursor()
                    
                    # --- A. æ‰§è¡Œå†™å…¥ ---
                    for idx, metadata in enumerate(metadata_batch):
                        savepoint_name = f"sp_{idx}"
                        try:
                            cursor.execute(f"SAVEPOINT {savepoint_name};")
                            columns = [k for k, v in metadata.items() if v is not None]
                            values = [v for v in metadata.values() if v is not None]
                            cols_str = ', '.join(columns)
                            vals_str = ', '.join(['%s'] * len(values))
                            
                            update_clauses = []
                            for col in columns:
                                # åœ¨ UPDATE æ—¶æ’é™¤ è®¢é˜…çŠ¶æ€å’Œè®¢é˜…æ¥æº
                                if col in ('tmdb_id', 'item_type', 'subscription_sources_json', 'subscription_status'): 
                                    continue
                                
                                update_clauses.append(f"{col} = EXCLUDED.{col}")
                            
                            sql = f"""
                                INSERT INTO media_metadata ({cols_str}, last_synced_at) 
                                VALUES ({vals_str}, NOW()) 
                                ON CONFLICT (tmdb_id, item_type) 
                                DO UPDATE SET {', '.join(update_clauses)}, last_synced_at = NOW()
                            """
                            cursor.execute(sql, tuple(values))
                        except Exception as e:
                            cursor.execute(f"ROLLBACK TO SAVEPOINT {savepoint_name};")
                            logger.error(f"å†™å…¥å¤±è´¥ {metadata.get('tmdb_id')}: {e}")
                    
                    # --- B. æ‰§è¡Œå­é›†ç¦»çº¿å¯¹è´¦ ---
                    if series_ids_processed_in_batch:
                        active_child_ids = {
                            m['tmdb_id'] for m in metadata_batch 
                            if m['item_type'] in ('Season', 'Episode')
                        }
                        active_child_ids_list = list(active_child_ids)
                        
                        if active_child_ids_list:
                            cursor.execute("""
                                UPDATE media_metadata
                                SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = '[]'::jsonb
                                WHERE parent_series_tmdb_id = ANY(%s)
                                  AND item_type IN ('Season', 'Episode')
                                  AND in_library = TRUE
                                  AND tmdb_id != ALL(%s)
                            """, (list(series_ids_processed_in_batch), active_child_ids_list))
                            total_offline_count += cursor.rowcount
                        else:
                            cursor.execute("""
                                UPDATE media_metadata
                                SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = '[]'::jsonb
                                WHERE parent_series_tmdb_id = ANY(%s)
                                  AND item_type IN ('Season', 'Episode')
                                  AND in_library = TRUE
                            """, (list(series_ids_processed_in_batch),))
                            total_offline_count += cursor.rowcount

                    conn.commit()
            
            del batch_item_groups
            del tmdb_details_map
            del metadata_batch
            gc.collect()

            processed_count += len(batch_tasks)
            task_manager.update_status_from_thread(20 + int((processed_count / total_to_process) * 80), f"å¤„ç†è¿›åº¦ {processed_count}/{total_to_process}...")

        final_msg = f"åŒæ­¥å®Œæˆï¼æ–°å¢/æ›´æ–°: {total_updated_count} ä¸ªåª’ä½“é¡¹, æ ‡è®°ç¦»çº¿: {total_offline_count} ä¸ªåª’ä½“é¡¹ã€‚"
        logger.info(f"  âœ… {final_msg}")
        task_manager.update_status_from_thread(100, final_msg)

    except Exception as e:
        logger.error(f"æ‰§è¡Œ '{task_name}' ä»»åŠ¡æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, f"ä»»åŠ¡å¤±è´¥: {e}")

# --- è‡ªåŠ¨æ‰“æ ‡ ---
def task_bulk_auto_tag(processor, library_ids: List[str], tags: List[str]):
    """
    åå°ä»»åŠ¡ï¼šæ”¯æŒä¸ºå¤šä¸ªåª’ä½“åº“æ‰¹é‡æ‰“æ ‡ç­¾ã€‚
    """
    try:
        total_libs = len(library_ids)
        for lib_idx, lib_id in enumerate(library_ids):
            task_manager.update_status_from_thread(int((lib_idx/total_libs)*100), f"æ­£åœ¨æ‰«æç¬¬ {lib_idx+1}/{total_libs} ä¸ªåª’ä½“åº“...")
            
            items = emby.get_emby_library_items(
                base_url=processor.emby_url,
                api_key=processor.emby_api_key,
                library_ids=[lib_id],
                media_type_filter="Movie,Series,Episode",
                user_id=processor.emby_user_id
            )
            
            if not items: continue

            for i, item in enumerate(items):
                if processor.is_stop_requested(): return
                
                # è¿›åº¦æ˜¾ç¤ºä¼˜åŒ–ï¼šæ˜¾ç¤ºå½“å‰åº“çš„è¿›åº¦
                task_manager.update_status_from_thread(
                    int((lib_idx/total_libs)*100 + (i/len(items))*(100/total_libs)), 
                    f"åº“({lib_idx+1}/{total_libs}) æ­£åœ¨æ‰“æ ‡: {item.get('Name')}"
                )
                
                emby.add_tags_to_item(item.get("Id"), tags, processor.emby_url, processor.emby_api_key, processor.emby_user_id)
        
        task_manager.update_status_from_thread(100, "æ‰€æœ‰é€‰å®šåº“æ‰¹é‡æ‰“æ ‡å®Œæˆ")
    except Exception as e:
        logger.error(f"æ‰¹é‡æ‰“æ ‡ä»»åŠ¡å¤±è´¥: {e}")
        task_manager.update_status_from_thread(-1, "ä»»åŠ¡å¼‚å¸¸ä¸­æ­¢")

def task_bulk_remove_tags(processor, library_ids: List[str], tags: List[str]):
    """
    åå°ä»»åŠ¡ï¼šä»æŒ‡å®šåª’ä½“åº“ä¸­æ‰¹é‡ç§»é™¤ç‰¹å®šæ ‡ç­¾ã€‚
    """
    try:
        total_libs = len(library_ids)
        for lib_idx, lib_id in enumerate(library_ids):
            items = emby.get_emby_library_items(
                base_url=processor.emby_url, api_key=processor.emby_api_key,
                library_ids=[lib_id], media_type_filter="Movie,Series,Episode",
                user_id=processor.emby_user_id
            )
            if not items: continue

            for i, item in enumerate(items):
                if processor.is_stop_requested(): return
                task_manager.update_status_from_thread(
                    int((lib_idx/total_libs)*100 + (i/len(items))*(100/total_libs)), 
                    f"æ­£åœ¨ç§»é™¤æ ‡ç­¾({lib_idx+1}/{total_libs}): {item.get('Name')}"
                )
                emby.remove_tags_from_item(item.get("Id"), tags, processor.emby_url, processor.emby_api_key, processor.emby_user_id)
        
        task_manager.update_status_from_thread(100, "æ‰¹é‡æ ‡ç­¾ç§»é™¤å®Œæˆ")
    except Exception as e:
        logger.error(f"æ‰¹é‡æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
        task_manager.update_status_from_thread(-1, "æ¸…ç†ä»»åŠ¡å¼‚å¸¸ä¸­æ­¢")

# â˜…â˜…â˜… åˆ†çº§åŒæ­¥ç‰¹ç§éƒ¨é˜Ÿ â˜…â˜…â˜…
def task_sync_ratings_to_emby(processor, force_full_update: bool = False):
    """
    ã€åˆ†çº§åŒæ­¥ä»»åŠ¡ã€‘
    force_full_update=True: åŒæ­¥ CustomRating + OfficialRating (å•å‘å¼ºåˆ¶: DB US -> Emby)ã€‚
    force_full_update=False: ä»…åŒæ­¥ CustomRating (åŒå‘äº’è¡¥: æœ‰è¦†ç›–æ— )ã€‚
    """
    mode = 'deep' if force_full_update else 'fast'
    logger.info(f"--- å¼€å§‹æ‰§è¡Œåˆ†çº§åŒæ­¥ä»»åŠ¡ (æ¨¡å¼: {mode}) ---")
    
    # 1. ä»æ•°æ®åº“è·å–æ‰€æœ‰åœ¨åº“é¡¹ç›®
    # æˆ‘ä»¬åªéœ€è¦æŸ¥é‚£äº›ç¡®å®åœ¨åº“é‡Œçš„ï¼Œä¸åœ¨åº“çš„åŒæ­¥äº†ä¹Ÿæ²¡æ„ä¹‰
    with connection.get_db_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT tmdb_id, item_type, emby_item_ids_json, custom_rating, official_rating_json 
            FROM media_metadata 
            WHERE in_library = TRUE 
              AND emby_item_ids_json IS NOT NULL 
              AND jsonb_array_length(emby_item_ids_json) > 0
        """)
        all_items = cursor.fetchall()

    total_items = len(all_items)
    logger.info(f"  âœ æ‰«æåˆ° {total_items} ä¸ªåœ¨åº“é¡¹ç›®ï¼Œå‡†å¤‡è¿›è¡Œå·®å¼‚æ¯”å¯¹...")
    
    # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜çˆ†ç‚¸
    BATCH_SIZE = 200
    updated_emby_count = 0
    updated_db_count = 0
    
    for i in range(0, total_items, BATCH_SIZE):
        if processor.is_stop_requested(): break
        
        batch = all_items[i : i + BATCH_SIZE]
        
        # æå–è¿™ä¸€æ‰¹çš„ Emby ID
        emby_id_map = {} # {emby_id: db_row}
        emby_ids_to_fetch = []
        
        for row in batch:
            try:
                e_ids = row['emby_item_ids_json']
                if e_ids:
                    # é€šå¸¸å–ç¬¬ä¸€ä¸ª ID å³å¯
                    eid = e_ids[0]
                    emby_id_map[eid] = row
                    emby_ids_to_fetch.append(eid)
            except: continue

        if not emby_ids_to_fetch: continue

        # æ‰¹é‡è·å– Emby ä¾§çš„ç°çŠ¶
        # æˆ‘ä»¬åªéœ€è¦ OfficialRating, CustomRating, LockedFields
        emby_items = emby.get_emby_items_by_id(
            base_url=processor.emby_url,
            api_key=processor.emby_api_key,
            user_id=processor.emby_user_id,
            item_ids=emby_ids_to_fetch,
            fields="OfficialRating,CustomRating,LockedFields,Name"
        )
        
        for e_item in emby_items:
            eid = e_item['Id']
            db_row = emby_id_map.get(eid)
            if not db_row: continue
            
            tmdb_id = db_row['tmdb_id']
            item_type = db_row['item_type']
            item_name = e_item.get('Name', tmdb_id)
            
            # --- æ•°æ®å‡†å¤‡ ---
            db_custom = db_row['custom_rating']
            emby_custom = e_item.get('CustomRating')
            
            db_official_json = db_row['official_rating_json'] or {}
            # è¿™é‡Œçš„ json å¯èƒ½æ˜¯ dict ä¹Ÿå¯èƒ½æ˜¯ strï¼Œpsycopg2 cursor_factory=RealDictCursor é€šå¸¸ä¼šè‡ªåŠ¨è½¬ dict
            # ä½†ä¸ºäº†ä¿é™©ï¼Œå¦‚æœæ˜¯ str å°± load ä¸€ä¸‹
            if isinstance(db_official_json, str):
                try: db_official_json = json.loads(db_official_json)
                except: db_official_json = {}
            
            # æå– DB é‡Œçš„ US åˆ†çº§ (è¿™æ˜¯æˆ‘ä»¬çš„çœŸç†æ ‡å‡†)
            db_us_rating = db_official_json.get('US')
            emby_official = e_item.get('OfficialRating')

            changes_to_emby = {}
            changes_to_db = {}

            # =========================================================
            # é€»è¾‘ A: CustomRating (åŒå‘äº’è¡¥ - æœ‰è¦†ç›–æ— )
            # =========================================================
            # 1. DB æœ‰ï¼ŒEmby æ—  -> æ¨ç»™ Emby (æ¢å¤ä¸¢å¤±çš„æ•°æ®)
            if db_custom and not emby_custom:
                changes_to_emby['CustomRating'] = db_custom
            
            # 2. Emby æœ‰ï¼ŒDB æ—  -> æ‹‰å› DB (ä¿å­˜ç”¨æˆ·åœ¨å‰ç«¯çš„æ“ä½œ)
            elif emby_custom and not db_custom:
                changes_to_db['custom_rating'] = emby_custom
            
            # 3. éƒ½æœ‰ï¼Œä½†ä¸ä¸€è‡´ -> ä»¥ DB ä¸ºå‡† (é˜²æ­¢ Emby çæ”¹ï¼Œæˆ–è€…ç”¨æˆ·æƒ³å›æ»š)
            # è¿™é‡Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©ä»¥ Emby ä¸ºå‡†ï¼Œçœ‹ä½ è§‰å¾—å“ªè¾¹æ›´æƒå¨ã€‚
            # æ—¢ç„¶ä½ è¯´ "Embyä¸€åˆ·æ–°å°±æ²¡äº†"ï¼Œè¯´æ˜ DB æ˜¯é¿é£æ¸¯ï¼Œæ‰€ä»¥å†²çªæ—¶ä¿¡ DBã€‚
            elif db_custom and emby_custom and db_custom != emby_custom:
                changes_to_emby['CustomRating'] = db_custom

            # =========================================================
            # é€»è¾‘ B: OfficialRating (æ·±åº¦æ¨¡å¼ - å•å‘å¼ºåˆ¶ DB->Emby)
            # =========================================================
            if mode == 'deep':
                # åªæœ‰å½“ DB é‡Œæ˜ç¡®æœ‰ US åˆ†çº§ï¼Œä¸” Emby å½“å‰åˆ†çº§ä¸ä¸€è‡´æ—¶ï¼Œæ‰è¦†ç›–
                # è¿™æ ·èƒ½è§£å†³ "è™šæ‹Ÿåº“çœ‹å¾—åˆ°(å› ä¸ºè¯»DB)ï¼ŒEmbyçœ‹ä¸åˆ°(å› ä¸ºEmbyåˆ†çº§é”™)" çš„ç°å—é—®é¢˜
                if db_us_rating and db_us_rating != emby_official:
                    changes_to_emby['OfficialRating'] = db_us_rating
                    
                    # å¦‚æœ Emby é”å®šäº† OfficialRatingï¼Œæˆ‘ä»¬éœ€è¦è§£é”å—ï¼Ÿ
                    # update_emby_item_details å†…éƒ¨é€»è¾‘é€šå¸¸ä¸å¤„ç†è§£é”ï¼Œ
                    # å¦‚æœéœ€è¦å¼ºè¡Œè¦†ç›–ï¼Œæœ€å¥½æŠŠ LockedFields ä¹Ÿå¤„ç†ä¸€ä¸‹
                    locked = e_item.get('LockedFields', [])
                    if 'OfficialRating' in locked:
                        locked.remove('OfficialRating')
                        changes_to_emby['LockedFields'] = locked

            # =========================================================
            # æ‰§è¡Œæ›´æ–°
            # =========================================================
            
            # 1. æ›´æ–° Emby
            if changes_to_emby:
                success = emby.update_emby_item_details(
                    item_id=eid,
                    new_data=changes_to_emby,
                    emby_server_url=processor.emby_url,
                    emby_api_key=processor.emby_api_key,
                    user_id=processor.emby_user_id
                )
                if success:
                    updated_emby_count += 1
                    logger.trace(f"  âœ [åŒæ­¥->Emby] {item_name}: {changes_to_emby}")

            # 2. æ›´æ–° DB
            if changes_to_db:
                media_db.update_media_metadata_fields(tmdb_id, item_type, changes_to_db)
                updated_db_count += 1
                logger.trace(f"  âœ [åŒæ­¥->DB] {item_name}: {changes_to_db}")

        # è¿›åº¦æ±‡æŠ¥
        progress = int((i / total_items) * 100)
        task_manager.update_status_from_thread(progress, f"åˆ†çº§åŒæ­¥({mode}): å·²å¤„ç† {i}/{total_items}...")

    logger.info(f"--- åˆ†çº§åŒæ­¥å®Œæˆ ({mode}) ---")
    logger.info(f"  âœ æ¨é€ç»™ Emby çš„æ›´æ–°: {updated_emby_count} æ¡")
    logger.info(f"  âœ æ‹‰å–å› DB çš„æ›´æ–°: {updated_db_count} æ¡")
    task_manager.update_status_from_thread(100, f"åˆ†çº§åŒæ­¥å®Œæˆ: Embyæ›´æ–°{updated_emby_count}, DBæ›´æ–°{updated_db_count}")
