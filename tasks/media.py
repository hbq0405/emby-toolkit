# tasks/media.py
# æ ¸å¿ƒåª’ä½“å¤„ç†ã€å…ƒæ•°æ®ã€èµ„äº§åŒæ­¥ç­‰

import time
import json
import gc
import logging
from typing import Optional, List
from datetime import datetime, timezone
import concurrent.futures
from collections import defaultdict

# å¯¼å…¥éœ€è¦çš„åº•å±‚æ¨¡å—å’Œå…±äº«å®ä¾‹
import task_manager
import handler.tmdb as tmdb
import handler.emby as emby
import handler.telegram as telegram
from database import connection
from utils import translate_country_list, get_unified_rating
from .helpers import parse_full_asset_details

logger = logging.getLogger(__name__)

# â˜…â˜…â˜… ä¸­æ–‡åŒ–è§’è‰²å â˜…â˜…â˜…
def task_role_translation(processor, force_full_update: bool = False):
    """
    æ ¹æ®ä¼ å…¥çš„ force_full_update å‚æ•°ï¼Œå†³å®šæ˜¯æ‰§è¡Œæ ‡å‡†æ‰«æè¿˜æ˜¯æ·±åº¦æ›´æ–°ã€‚
    """
    # 1. æ ¹æ®å‚æ•°å†³å®šæ—¥å¿—ä¿¡æ¯
    if force_full_update:
        logger.info("  âœ å³å°†æ‰§è¡Œæ·±åº¦æ¨¡å¼ï¼Œå°†å¤„ç†æ‰€æœ‰åª’ä½“é¡¹å¹¶ä»TMDbè·å–æœ€æ–°æ•°æ®...")
    else:
        logger.info("  âœ å³å°†æ‰§è¡Œå¿«é€Ÿæ¨¡å¼ï¼Œå°†è·³è¿‡å·²å¤„ç†é¡¹...")


    # 3. è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°ï¼Œå¹¶å°† force_full_update å‚æ•°é€ä¼ ä¸‹å»
    processor.process_full_library(
        update_status_callback=task_manager.update_status_from_thread,
        force_full_update=force_full_update 
    )

# --- ä½¿ç”¨æ‰‹åŠ¨ç¼–è¾‘çš„ç»“æœå¤„ç†åª’ä½“é¡¹ ---
def task_manual_update(processor, item_id: str, manual_cast_list: list, item_name: str):
    """ä»»åŠ¡ï¼šä½¿ç”¨æ‰‹åŠ¨ç¼–è¾‘çš„ç»“æœå¤„ç†åª’ä½“é¡¹"""
    processor.process_item_with_manual_cast(
        item_id=item_id,
        manual_cast_list=manual_cast_list,
        item_name=item_name
    )

def task_sync_metadata_cache(processor, item_id: str, item_name: str, episode_ids_to_add: Optional[List[str]] = None):
    """
    ä»»åŠ¡ï¼šä¸ºå•ä¸ªåª’ä½“é¡¹åŒæ­¥å…ƒæ•°æ®åˆ° media_metadata æ•°æ®åº“è¡¨ã€‚
    å¯æ ¹æ®æ˜¯å¦ä¼ å…¥ episode_ids_to_add æ¥å†³å®šæ‰§è¡Œæ¨¡å¼ã€‚
    """
    sync_mode = "ç²¾å‡†åˆ†é›†è¿½åŠ " if episode_ids_to_add else "å¸¸è§„å…ƒæ•°æ®åˆ·æ–°"
    logger.trace(f"  âœ ä»»åŠ¡å¼€å§‹ï¼šåŒæ­¥åª’ä½“å…ƒæ•°æ®ç¼“å­˜ ({sync_mode}) for '{item_name}' (ID: {item_id})")
    try:
        processor.sync_single_item_to_metadata_cache(item_id, item_name=item_name, episode_ids_to_add=episode_ids_to_add)
        logger.trace(f"  âœ ä»»åŠ¡æˆåŠŸï¼šåŒæ­¥åª’ä½“å…ƒæ•°æ®ç¼“å­˜ for '{item_name}'")
    except Exception as e:
        logger.error(f"  âœ ä»»åŠ¡å¤±è´¥ï¼šåŒæ­¥åª’ä½“å…ƒæ•°æ®ç¼“å­˜ for '{item_name}' æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

def task_sync_images(processor, item_id: str, update_description: str, sync_timestamp_iso: str):
    """
    ä»»åŠ¡ï¼šä¸ºå•ä¸ªåª’ä½“é¡¹åŒæ­¥å›¾ç‰‡å’Œå…ƒæ•°æ®æ–‡ä»¶åˆ°æœ¬åœ° override ç›®å½•ã€‚
    """
    logger.trace(f"ä»»åŠ¡å¼€å§‹ï¼šå›¾ç‰‡å¤‡ä»½ for ID: {item_id} (åŸå› : {update_description})")
    try:
        # --- â–¼â–¼â–¼ æ ¸å¿ƒä¿®å¤ â–¼â–¼â–¼ ---
        # 1. æ ¹æ® item_id è·å–å®Œæ•´çš„åª’ä½“è¯¦æƒ…
        item_details = emby.get_emby_item_details(
            item_id, 
            processor.emby_url, 
            processor.emby_api_key, 
            processor.emby_user_id
        )
        if not item_details:
            logger.error(f"ä»»åŠ¡å¤±è´¥ï¼šæ— æ³•è·å– ID: {item_id} çš„åª’ä½“è¯¦æƒ…ï¼Œè·³è¿‡å›¾ç‰‡å¤‡ä»½ã€‚")
            return

        # 2. ä½¿ç”¨è·å–åˆ°çš„ item_details å­—å…¸æ¥è°ƒç”¨
        processor.sync_item_images(
            item_details=item_details, 
            update_description=update_description
            # episode_ids_to_sync å‚æ•°è¿™é‡Œä¸éœ€è¦ï¼Œsync_item_images ä¼šè‡ªå·±å¤„ç†
        )
        # --- â–²â–²â–² ä¿®å¤ç»“æŸ â–²â–²â–² ---

        logger.trace(f"ä»»åŠ¡æˆåŠŸï¼šå›¾ç‰‡å¤‡ä»½ for ID: {item_id}")
    except Exception as e:
        logger.error(f"ä»»åŠ¡å¤±è´¥ï¼šå›¾ç‰‡å¤‡ä»½ for ID: {item_id} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

def task_sync_all_metadata(processor, item_id: str, item_name: str):
    """
    ã€ä»»åŠ¡ï¼šå…¨èƒ½å…ƒæ•°æ®åŒæ­¥å™¨ã€‚
    å½“æ”¶åˆ° metadata.update Webhook æ—¶ï¼Œæ­¤ä»»åŠ¡ä¼šï¼š
    1. ä» Emby è·å–æœ€æ–°æ•°æ®ã€‚
    2. å°†æ›´æ–°æŒä¹…åŒ–åˆ° override è¦†ç›–ç¼“å­˜æ–‡ä»¶ã€‚
    3. å°†æ›´æ–°åŒæ­¥åˆ° media_metadata æ•°æ®åº“ç¼“å­˜ã€‚
    """
    log_prefix = f"å…¨èƒ½å…ƒæ•°æ®åŒæ­¥ for '{item_name}'"
    logger.trace(f"  âœ ä»»åŠ¡å¼€å§‹ï¼š{log_prefix}")
    try:
        # æ­¥éª¤ 1: è·å–åŒ…å«äº†ç”¨æˆ·ä¿®æ”¹çš„ã€æœ€æ–°çš„å®Œæ•´åª’ä½“è¯¦æƒ…
        item_details = emby.get_emby_item_details(
            item_id, 
            processor.emby_url, 
            processor.emby_api_key, 
            processor.emby_user_id,
            # è¯·æ±‚æ‰€æœ‰å¯èƒ½è¢«ç”¨æˆ·ä¿®æ”¹çš„å­—æ®µ
            fields="ProviderIds,Type,Name,OriginalTitle,Overview,Tagline,CommunityRating,OfficialRating,Genres,Studios,Tags,PremiereDate"
        )
        if not item_details:
            logger.error(f"  âœ {log_prefix} å¤±è´¥ï¼šæ— æ³•è·å–é¡¹ç›® {item_id} çš„æœ€æ–°è¯¦æƒ…ã€‚")
            return

        # æ­¥éª¤ 2: è°ƒç”¨æ–½å·¥é˜Ÿï¼Œæ›´æ–° override æ–‡ä»¶
        processor.sync_emby_updates_to_override_files(item_details)

        # æ­¥éª¤ 3: è°ƒç”¨å¦ä¸€ä¸ªæ–½å·¥é˜Ÿï¼Œæ›´æ–°æ•°æ®åº“ç¼“å­˜
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å¤ç”¨ç°æœ‰çš„ task_sync_metadata_cache é€»è¾‘
        processor.sync_single_item_to_metadata_cache(item_id, item_name=item_name)

        logger.trace(f"  âœ ä»»åŠ¡æˆåŠŸï¼š{log_prefix}")
    except Exception as e:
        logger.error(f"  âœ ä»»åŠ¡å¤±è´¥ï¼š{log_prefix} æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise

# â˜…â˜…â˜… é‡æ–°å¤„ç†å•ä¸ªé¡¹ç›® â˜…â˜…â˜…
def task_reprocess_single_item(processor, item_id: str, item_name_for_ui: str):
    """
    ã€æœ€ç»ˆç‰ˆ - èŒè´£åˆ†ç¦»ã€‘åå°ä»»åŠ¡ã€‚
    æ­¤ç‰ˆæœ¬è´Ÿè´£åœ¨ä»»åŠ¡å¼€å§‹æ—¶è®¾ç½®â€œæ­£åœ¨å¤„ç†â€çš„çŠ¶æ€ï¼Œå¹¶æ‰§è¡Œæ ¸å¿ƒé€»è¾‘ã€‚
    """
    logger.trace(f"  âœ åå°ä»»åŠ¡å¼€å§‹æ‰§è¡Œ ({item_name_for_ui})")
    
    try:
        # âœ¨ å…³é”®ä¿®æ”¹ï¼šä»»åŠ¡ä¸€å¼€å§‹ï¼Œå°±ç”¨â€œæ­£åœ¨å¤„ç†â€çš„çŠ¶æ€è¦†ç›–æ‰æ—§çŠ¶æ€
        task_manager.update_status_from_thread(0, f"æ­£åœ¨å¤„ç†: {item_name_for_ui}")

        # ç°åœ¨æ‰å¼€å§‹çœŸæ­£çš„å·¥ä½œ
        processor.process_single_item(
            item_id, 
            force_full_update=True
        )
        # ä»»åŠ¡æˆåŠŸå®Œæˆåçš„çŠ¶æ€æ›´æ–°ä¼šè‡ªåŠ¨ç”±ä»»åŠ¡é˜Ÿåˆ—å¤„ç†ï¼Œæˆ‘ä»¬æ— éœ€å…³å¿ƒ
        logger.trace(f"  âœ åå°ä»»åŠ¡å®Œæˆ ({item_name_for_ui})")

    except Exception as e:
        logger.error(f"åå°ä»»åŠ¡å¤„ç† '{item_name_for_ui}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, f"å¤„ç†å¤±è´¥: {item_name_for_ui}")

# â˜…â˜…â˜… é‡æ–°å¤„ç†æ‰€æœ‰å¾…å¤æ ¸é¡¹ â˜…â˜…â˜…
def task_reprocess_all_review_items(processor):
    """
    ã€å·²å‡çº§ã€‘åå°ä»»åŠ¡ï¼šéå†æ‰€æœ‰å¾…å¤æ ¸é¡¹å¹¶é€ä¸€ä»¥â€œå¼ºåˆ¶åœ¨çº¿è·å–â€æ¨¡å¼é‡æ–°å¤„ç†ã€‚
    """
    logger.trace("--- å¼€å§‹æ‰§è¡Œâ€œé‡æ–°å¤„ç†æ‰€æœ‰å¾…å¤æ ¸é¡¹â€ä»»åŠ¡ [å¼ºåˆ¶åœ¨çº¿è·å–æ¨¡å¼] ---")
    try:
        # +++ æ ¸å¿ƒä¿®æ”¹ 1ï¼šåŒæ—¶æŸ¥è¯¢ item_id å’Œ item_name +++
        with connection.get_db_connection() as conn:
            cursor = conn.cursor()
            # ä» failed_log ä¸­åŒæ—¶è·å– ID å’Œ Name
            cursor.execute("SELECT item_id, item_name FROM failed_log")
            # å°†ç»“æœä¿å­˜ä¸ºä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨
            all_items = [{'id': row['item_id'], 'name': row['item_name']} for row in cursor.fetchall()]
        
        total = len(all_items)
        if total == 0:
            logger.info("å¾…å¤æ ¸åˆ—è¡¨ä¸­æ²¡æœ‰é¡¹ç›®ï¼Œä»»åŠ¡ç»“æŸã€‚")
            task_manager.update_status_from_thread(100, "å¾…å¤æ ¸åˆ—è¡¨ä¸ºç©ºã€‚")
            return

        logger.info(f"å…±æ‰¾åˆ° {total} ä¸ªå¾…å¤æ ¸é¡¹éœ€è¦ä»¥â€œå¼ºåˆ¶åœ¨çº¿è·å–â€æ¨¡å¼é‡æ–°å¤„ç†ã€‚")

        # +++ æ ¸å¿ƒä¿®æ”¹ 2ï¼šåœ¨å¾ªç¯ä¸­è§£åŒ… item_id å’Œ item_name +++
        for i, item in enumerate(all_items):
            if processor.is_stop_requested():
                logger.info("  ğŸš« ä»»åŠ¡è¢«ä¸­æ­¢ã€‚")
                break
            
            item_id = item['id']
            item_name = item['name'] or f"ItemID: {item_id}" # å¦‚æœåå­—ä¸ºç©ºï¼Œæä¾›ä¸€ä¸ªå¤‡ç”¨å

            task_manager.update_status_from_thread(int((i/total)*100), f"æ­£åœ¨é‡æ–°å¤„ç† {i+1}/{total}: {item_name}")
            
            # +++ æ ¸å¿ƒä¿®æ”¹ 3ï¼šä¼ é€’æ‰€æœ‰å¿…éœ€çš„å‚æ•° +++
            task_reprocess_single_item(processor, item_id, item_name)
            
            # æ¯ä¸ªé¡¹ç›®ä¹‹é—´ç¨ä½œåœé¡¿
            time.sleep(2) 

    except Exception as e:
        logger.error(f"é‡æ–°å¤„ç†æ‰€æœ‰å¾…å¤æ ¸é¡¹æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, "ä»»åŠ¡å¤±è´¥")

# â˜…â˜…â˜… é‡é‡çº§çš„å…ƒæ•°æ®ç¼“å­˜å¡«å……ä»»åŠ¡ (å†…å­˜ä¼˜åŒ–ç‰ˆ) â˜…â˜…â˜…
def task_populate_metadata_cache(processor, batch_size: int = 50, force_full_update: bool = False):
    """
    - é‡é‡çº§çš„å…ƒæ•°æ®ç¼“å­˜å¡«å……ä»»åŠ¡ (å†…å­˜ä¼˜åŒ–ç‰ˆ)ã€‚
    - é€»è¾‘å‡çº§ï¼š
      1. ä½¿ç”¨åˆ†é¡µç”Ÿæˆå™¨æ›¿ä»£ä¸€æ¬¡æ€§å…¨é‡æ‹‰å–ï¼Œå¤§å¹…é™ä½å†…å­˜å³°å€¼ã€‚
      2. æ‰«ææ—¶å³æ—¶ä¸¢å¼ƒæœªå˜åŠ¨çš„é¡¹ç›®æ•°æ®ï¼Œåªä¿ç•™ ID ç”¨äºå·®å¼‚æ¯”å¯¹ã€‚
      3. å¯¹æ ‡è®°ä¸ºâ€œè„â€çš„å‰§é›†ï¼Œåœ¨å¤„ç†é˜¶æ®µæŒ‰éœ€é‡æ–°æ‹‰å–å®Œæ•´å­é›†ä¿¡æ¯ã€‚
    """
    task_name = "åŒæ­¥åª’ä½“å…ƒæ•°æ®"
    sync_mode = "æ·±åº¦åŒæ­¥ (å…¨é‡)" if force_full_update else "å¿«é€ŸåŒæ­¥ (å¢é‡)"
    logger.info(f"--- æ¨¡å¼: {sync_mode} (åˆ†æ‰¹å¤§å°: {batch_size}) ---")
    
    # --- ç»Ÿè®¡è®¡æ•°å™¨ ---
    total_updated_count = 0
    total_offline_count = 0

    try:
        task_manager.update_status_from_thread(0, f"é˜¶æ®µ1/3: å»ºç«‹å·®å¼‚åŸºå‡† ({sync_mode})...")
        
        libs_to_process_ids = processor.config.get("libraries_to_process", [])
        if not libs_to_process_ids:
            raise ValueError("æœªåœ¨é…ç½®ä¸­æŒ‡å®šè¦å¤„ç†çš„åª’ä½“åº“ã€‚")

        # 1. è·å–æ•°æ®åº“ä¸­æ‰€æœ‰å·²çŸ¥çš„ Emby ID (ç”¨äºæ¯”å¯¹)
        known_emby_ids = set()
        if not force_full_update:
            with connection.get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT jsonb_array_elements_text(emby_item_ids_json) AS emby_id
                    FROM media_metadata 
                    WHERE in_library = TRUE
                """)
                # ä½¿ç”¨ set å­˜å‚¨ IDï¼Œå†…å­˜å ç”¨æå° (50ä¸‡ä¸ªIDçº¦å ç”¨ 50MB)
                known_emby_ids = set(row['emby_id'] for row in cursor.fetchall())
            logger.info(f"  âœ æœ¬åœ°æ•°æ®åº“åœ¨çº¿ {len(known_emby_ids)} ä¸ªåª’ä½“é¡¹ã€‚")

        # 2. æ‰«æ Emby (æµå¼å¤„ç†)
        task_manager.update_status_from_thread(10, f"é˜¶æ®µ2/3: æ‰«æ Emby å¹¶è®¡ç®—å·®å¼‚...")
        
        # ä»…ä¿ç•™éœ€è¦å¤„ç†çš„æ•°æ®ç»“æ„
        top_level_items_map = defaultdict(list)       
        series_to_seasons_map = defaultdict(list)     
        series_to_episode_map = defaultdict(list)     
        
        emby_top_level_keys = set() 
        dirty_series_tmdb_ids = set()
        emby_sid_to_tmdb_id = {}
        current_scan_emby_ids = set() # è®°å½•æœ¬æ¬¡æ‰«æåˆ°çš„æ‰€æœ‰ ID

        scan_count = 0
        
        # å®šä¹‰éœ€è¦çš„å­—æ®µ
        req_fields = "ProviderIds,Type,DateCreated,Name,OriginalTitle,PremiereDate,CommunityRating,Genres,Studios,Tags,DateModified,OfficialRating,ProductionYear,Path,PrimaryImageAspectRatio,Overview,MediaStreams,Container,Size,SeriesId,ParentIndexNumber,IndexNumber,ParentId,RunTimeTicks,_SourceLibraryId"

        # ä½¿ç”¨ç”Ÿæˆå™¨æµå¼è·å–
        item_generator = emby.fetch_all_emby_items_generator(
            base_url=processor.emby_url, 
            api_key=processor.emby_api_key, 
            library_ids=libs_to_process_ids, 
            fields=req_fields
        )

        for item in item_generator:
            scan_count += 1
            if scan_count % 5000 == 0:
                task_manager.update_status_from_thread(10, f"æ­£åœ¨ç´¢å¼• Emby åº“ ({scan_count} å·²æ‰«æ)...")
            
            item_id = str(item.get("Id"))
            if not item_id: continue
            
            current_scan_emby_ids.add(item_id)
            
            item_type = item.get("Type")
            tmdb_id = item.get("ProviderIds", {}).get("Tmdb")
            
            # å»ºç«‹ Series ID -> TMDb ID æ˜ å°„ (è½»é‡çº§)
            if item_type == "Series" and tmdb_id:
                emby_sid_to_tmdb_id[item_id] = str(tmdb_id)

            # --- æ ¸å¿ƒä¼˜åŒ–ï¼šå³æ—¶ä¸¢å¼ƒåˆ¤æ–­ ---
            # å¦‚æœä¸æ˜¯å¼ºåˆ¶å…¨é‡æ›´æ–°ï¼Œä¸” ID å·²çŸ¥ï¼Œåˆ™è§†ä¸ºâ€œå¹²å‡€â€ï¼Œç›´æ¥è·³è¿‡å­˜å‚¨è¯¦ç»†ä¿¡æ¯
            # æ³¨æ„ï¼šæˆ‘ä»¬åªè®°å½• ID åˆ° current_scan_emby_ids ç”¨äºåç»­çš„åˆ é™¤æ£€æµ‹
            if not force_full_update and item_id in known_emby_ids:
                continue

            # --- ä»¥ä¸‹é€»è¾‘ä»…é’ˆå¯¹ æ–°å¢ æˆ– å˜æ›´ çš„é¡¹ç›® ---
            is_new_item = True # èƒ½èµ°åˆ°è¿™é‡Œè¯´æ˜æ˜¯æ–°çš„æˆ–è€…å¼ºåˆ¶æ›´æ–°çš„

            # A. é¡¶å±‚åª’ä½“
            if item_type in ["Movie", "Series"]:
                if tmdb_id:
                    composite_key = (str(tmdb_id), item_type)
                    top_level_items_map[composite_key].append(item)
                    emby_top_level_keys.add(composite_key)
                    
                    if item_type == "Series":
                        dirty_series_tmdb_ids.add(str(tmdb_id))

            # B. å­é›†åª’ä½“ (Season)
            elif item_type == 'Season':
                s_id = str(item.get('SeriesId') or item.get('ParentId'))
                if s_id: 
                    series_to_seasons_map[s_id].append(item)
                    has_valid_index = item.get('IndexNumber') is not None
                    # å¦‚æœå‘ç°æ–°å­£ï¼Œæ ‡è®°çˆ¶å‰§é›†ä¸ºè„
                    if s_id in emby_sid_to_tmdb_id and has_valid_index:
                        dirty_series_tmdb_ids.add(emby_sid_to_tmdb_id[s_id])

            # C. å­é›†åª’ä½“ (Episode)
            elif item_type == 'Episode':
                s_id = str(item.get('SeriesId'))
                if s_id: 
                    series_to_episode_map[s_id].append(item)
                    has_valid_index = item.get('ParentIndexNumber') is not None and item.get('IndexNumber') is not None
                    # å¦‚æœå‘ç°æ–°é›†ï¼Œæ ‡è®°çˆ¶å‰§é›†ä¸ºè„
                    if s_id in emby_sid_to_tmdb_id and has_valid_index:
                        dirty_series_tmdb_ids.add(emby_sid_to_tmdb_id[s_id])

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

        # â˜…â˜…â˜… åå‘å·®å¼‚æ£€æµ‹ (åˆ é™¤) â˜…â˜…â˜…
        if not force_full_update:
            missing_emby_ids = known_emby_ids - current_scan_emby_ids
            # é‡Šæ”¾å¤§é›†åˆå†…å­˜
            del known_emby_ids
            del current_scan_emby_ids
            gc.collect()

            if missing_emby_ids:
                logger.info(f"  âœ æ£€æµ‹åˆ° {len(missing_emby_ids)} ä¸ª Emby ID å·²æ¶ˆå¤±ï¼Œæ­£åœ¨åæŸ¥æ‰€å±å‰§é›†...")
                missing_ids_list = list(missing_emby_ids)
                
                with connection.get_db_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT DISTINCT parent_series_tmdb_id AS pid
                        FROM media_metadata 
                        WHERE item_type IN ('Season', 'Episode') 
                          AND in_library = TRUE 
                          AND EXISTS (
                              SELECT 1 
                              FROM jsonb_array_elements_text(emby_item_ids_json) as eid 
                              WHERE eid = ANY(%s)
                          )
                    """, (missing_ids_list,))
                    
                    affected_parents = set(row['pid'] for row in cursor.fetchall() if row['pid'])
                    
                    if affected_parents:
                        logger.info(f"  âœ å› å†…å®¹åˆ é™¤ï¼Œ{len(affected_parents)} éƒ¨å‰§é›†è¢«æ ‡è®°ä¸ºå¾…åˆ·æ–°ã€‚")
                        dirty_series_tmdb_ids.update(affected_parents)

        logger.info(f"  âœ Emby æ‰«æå®Œæˆï¼Œå…± {scan_count} ä¸ªé¡¹ã€‚æœ‰ {len(dirty_series_tmdb_ids)} éƒ¨å‰§é›†æ¶‰åŠå˜æ›´ã€‚")

        # 4. æ•°æ®åº“æ¯”å¯¹ (ç”¨äºæ£€æµ‹é¡¶å±‚ç¦»çº¿)
        with connection.get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT tmdb_id, item_type FROM media_metadata WHERE in_library = TRUE AND item_type IN ('Movie', 'Series')")
            db_top_level_keys = {(row["tmdb_id"], row["item_type"]) for row in cursor.fetchall()}
        
        # 5. å¤„ç†é¡¶å±‚ç¦»çº¿
        keys_to_delete = db_top_level_keys - emby_top_level_keys
        # å¤„ç†å­¤å„¿åˆ†é›† (å¦‚æœå‰§é›†è¢«æ ‡è®°ä¸ºè„ï¼Œä½†åœ¨æœ¬æ¬¡æ‰«æä¸­æœªä½œä¸º Series ç±»å‹å‡ºç°ï¼Œè¯´æ˜å‰§é›†æœ¬èº«å¯èƒ½è¢«åˆ äº†æˆ–è€… Emby ID å˜äº†)
        # æ³¨æ„ï¼šç”±äºæˆ‘ä»¬è·³è¿‡äº† clean itemsï¼Œæ‰€ä»¥ emby_top_level_keys åªåŒ…å« dirty itemsã€‚
        # è¿™é‡Œé€»è¾‘éœ€è¦å¾®è°ƒï¼šå¦‚æœä¸€ä¸ªå‰§é›†åœ¨ DB é‡Œï¼Œä¸”è¢«æ ‡è®°ä¸º dirtyï¼Œä½†ä¸åœ¨ emby_top_level_keys é‡Œï¼Œ
        # å¯èƒ½æ˜¯å› ä¸ºå®ƒæ²¡å˜ï¼ˆcleanï¼‰ï¼Œä¹Ÿå¯èƒ½æ˜¯è¢«åˆ äº†ã€‚
        # ä½†å¦‚æœæ˜¯ cleanï¼Œå®ƒä¸ä¼šè¿›å…¥ dirty_series_tmdb_idsï¼Œé™¤éæ˜¯å­é›†å˜åŠ¨è§¦å‘ã€‚
        # å¦‚æœå­é›†å˜åŠ¨è§¦å‘äº† dirtyï¼Œè¯´æ˜å‰§é›†ä¸»ä½“è¿˜åœ¨ã€‚
        # æ‰€ä»¥è¿™é‡Œä¸»è¦å¤„ç†çš„æ˜¯ï¼šDBé‡Œæœ‰ï¼Œä½† Emby å½»åº•æ²¡äº†çš„æƒ…å†µã€‚
        # ç”±äºæˆ‘ä»¬æ²¡æœ‰å…¨é‡ emby_top_level_keysï¼Œæˆ‘ä»¬åªèƒ½ä¾èµ– missing_emby_ids çš„åæŸ¥ç»“æœã€‚
        # ä¹‹å‰çš„ missing_emby_ids é€»è¾‘å·²ç»å¤„ç†äº†å¤§éƒ¨åˆ†åˆ é™¤ã€‚
        # è¿™é‡Œä¸»è¦å¤„ç†æ•´éƒ¨å‰§/ç”µå½± ID æ¶ˆå¤±çš„æƒ…å†µã€‚
        
        # ä¿®æ­£ï¼šç”±äºæˆ‘ä»¬è·³è¿‡äº† clean itemsï¼Œdb_top_level_keys - emby_top_level_keys ä¼šåŒ…å«æ‰€æœ‰æœªå˜åŠ¨çš„é¡¹ç›®ã€‚
        # æˆ‘ä»¬ä¸èƒ½ç›´æ¥åˆ é™¤å®ƒä»¬ã€‚
        # åˆ é™¤é€»è¾‘åº”å®Œå…¨ä¾èµ– missing_emby_ids (å·²åœ¨ä¸Šæ–¹å¤„ç†) å’Œ æ˜¾å¼çš„ç¦»çº¿æ£€æµ‹ã€‚
        # åªæœ‰å½“ force_full_update=True æ—¶ï¼Œkeys_to_delete æ‰æ˜¯å‡†ç¡®çš„ã€‚
        if force_full_update:
             if keys_to_delete:
                count_top_offline = len(keys_to_delete)
                total_offline_count += count_top_offline
                logger.info(f"  âœ [å…¨é‡æ¨¡å¼] å‘ç° {count_top_offline} ä¸ªé¡¶å±‚é¡¹ç›®å·²å®Œå…¨ç¦»çº¿ï¼Œæ­£åœ¨æ¸…ç†...")
                ids_to_del = defaultdict(list)
                for t_id, t_type in keys_to_delete:
                    ids_to_del[t_type].append(t_id)
                
                with connection.get_db_connection() as conn:
                    cursor = conn.cursor()
                    for i_type, id_list in ids_to_del.items():
                        cursor.execute(
                            "UPDATE media_metadata SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = '[]'::jsonb WHERE item_type = %s AND tmdb_id = ANY(%s)",
                            (i_type, id_list)
                        )
                        if i_type == 'Series':
                            cursor.execute(
                                "UPDATE media_metadata SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = '[]'::jsonb WHERE parent_series_tmdb_id = ANY(%s)",
                                (id_list,)
                            )
                    conn.commit()

        if processor.is_stop_requested(): return

        # 6. ç¡®å®šå¤„ç†é˜Ÿåˆ—
        items_to_process = []
        
        # ç­–ç•¥ï¼š
        # 1. å¤„ç† top_level_items_map ä¸­çš„æ‰€æœ‰é¡¹ï¼ˆè¿™äº›éƒ½æ˜¯æ–°å¢æˆ–å¼ºåˆ¶æ›´æ–°çš„ï¼‰
        # 2. å¯¹äº dirty_series_tmdb_ids ä¸­çš„é¡¹ï¼Œå¦‚æœä¸åœ¨ top_level_items_map ä¸­ï¼ˆè¯´æ˜å‰§é›†ä¸»ä½“æœªå˜ï¼Œä½†å­é›†å˜äº†ï¼‰ï¼Œ
        #    æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æ„é€ ä¸€ä¸ªä»»åŠ¡ï¼Œå¹¶æ ‡è®°éœ€è¦â€œé‡æ–°è·å–è¯¦æƒ…â€ã€‚
        
        processed_tmdb_ids = set()

        # A. å¤„ç† map ä¸­çš„é¡¹ (New / Dirty Parent)
        for composite_key, items in top_level_items_map.items():
            items_to_process.append({'items': items, 'refetch': False})
            processed_tmdb_ids.add(composite_key[0])

        # B. å¤„ç†ä»…å­é›†å˜åŠ¨çš„å‰§é›† (Dirty Children, Clean Parent)
        for tmdb_id in dirty_series_tmdb_ids:
            if tmdb_id not in processed_tmdb_ids:
                # è¿™æ˜¯ä¸€ä¸ªâ€œéšå½¢â€çš„è„å‰§é›†ï¼Œæˆ‘ä»¬åœ¨æ‰«æé˜¶æ®µè·³è¿‡äº†å®ƒçš„ä¸»ä½“ä¿¡æ¯
                # æˆ‘ä»¬éœ€è¦æ„é€ ä¸€ä¸ªå ä½ç¬¦ï¼Œå‘Šè¯‰åç»­é€»è¾‘å» Emby é‡æ–°æ‹‰å–å®ƒçš„å®Œæ•´ä¿¡æ¯
                items_to_process.append({
                    'tmdb_id': tmdb_id,
                    'type': 'Series',
                    'refetch': True # æ ‡è®°éœ€è¦é‡æ–°æ‹‰å–
                })

        total_to_process = len(items_to_process)
        task_manager.update_status_from_thread(20, f"é˜¶æ®µ3/3: æ­£åœ¨åŒæ­¥ {total_to_process} ä¸ªå˜æ›´é¡¹ç›®...")
        logger.info(f"  âœ æœ€ç»ˆå¤„ç†é˜Ÿåˆ—: {total_to_process} ä¸ªé¡¶å±‚é¡¹ç›®")

        # 7. æ‰¹é‡å¤„ç†
        processed_count = 0
        for i in range(0, total_to_process, batch_size):
            if processor.is_stop_requested(): break
            batch_tasks = items_to_process[i:i + batch_size]
            
            # --- é¢„å¤„ç†ï¼šæ‹‰å–ç¼ºå¤±çš„ Emby æ•°æ® ---
            # å¯¹äºæ ‡è®°ä¸º refetch=True çš„å‰§é›†ï¼Œæˆ‘ä»¬éœ€è¦å…ˆä» Emby æ‹‰å–å®ƒä»¬åŠå…¶å­é›†çš„æ•°æ®
            batch_item_groups = []
            
            for task in batch_tasks:
                if task.get('refetch'):
                    # é‡æ–°æ‹‰å–å‰§é›†è¯¦æƒ…
                    try:
                        # 1. æ‰¾å› Emby ID (é€šè¿‡ TMDb ID åæŸ¥ DB æˆ– ä¹‹å‰çš„ map)
                        # è¿™é‡Œæœ€ç¨³å¦¥çš„æ˜¯é€šè¿‡ TMDb ID åœ¨ DB æŸ¥ Emby IDï¼Œæˆ–è€…åˆ©ç”¨ emby_sid_to_tmdb_id çš„åå‘
                        # ç”±äº emby_sid_to_tmdb_id ä¹Ÿæ˜¯æ‰«æç”Ÿæˆçš„ï¼Œå¯èƒ½ä¸å…¨ã€‚
                        # æœ€å¥½æ˜¯ç”¨ DB åæŸ¥
                        t_id = task['tmdb_id']
                        with connection.get_db_connection() as conn:
                            with conn.cursor() as cursor:
                                cursor.execute("SELECT jsonb_array_elements_text(emby_item_ids_json) as eid FROM media_metadata WHERE tmdb_id = %s AND item_type='Series' LIMIT 1", (t_id,))
                                row = cursor.fetchone()
                                if row:
                                    e_id = row['eid']
                                    # æ‹‰å–è¯¥å‰§é›†åŠå…¶æ‰€æœ‰å­é›†
                                    full_series_items = emby.get_emby_item_details(e_id, processor.emby_url, processor.emby_api_key, processor.emby_user_id)
                                    if full_series_items:
                                        # è¿™é‡Œ get_emby_item_details åªè¿”å›å•ä¸ªé¡¹ï¼Œæˆ‘ä»¬éœ€è¦æ‰€æœ‰å­é›†
                                        # ä½¿ç”¨ fetch_all_emby_items_generator çš„é€»è¾‘ï¼Œä½†æŒ‡å®š ParentId
                                        children_gen = emby.fetch_all_emby_items_generator(
                                            base_url=processor.emby_url,
                                            api_key=processor.emby_api_key,
                                            library_ids=[e_id],  # è¿™é‡Œå°†å‰§é›†IDä½œä¸º ParentId ä¼ å…¥ï¼Œä»¥è·å–å…¶å­é›†
                                            fields=req_fields
                                        )
                                        group = [full_series_items] + list(children_gen)
                                        
                                        # è¡¥å……åˆ° map ä¸­ï¼Œä»¥ä¾¿åç»­é€»è¾‘å¤ç”¨
                                        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ›´æ–° series_to_seasons_map ç­‰ï¼Œå› ä¸ºä¹‹å‰è·³è¿‡äº†
                                        for child in group:
                                            ct = child.get('Type')
                                            if ct == 'Season': series_to_seasons_map[e_id].append(child)
                                            elif ct == 'Episode': series_to_episode_map[e_id].append(child)
                                        
                                        batch_item_groups.append(group)
                    except Exception as e:
                        logger.error(f"é‡æ–°æ‹‰å–å‰§é›† {task.get('tmdb_id')} å¤±è´¥: {e}")
                else:
                    batch_item_groups.append(task['items'])

            # --- å¹¶å‘è·å– TMDB è¯¦æƒ… ---
            tmdb_details_map = {}
            def fetch_tmdb_details(item_group):
                item = item_group[0]
                t_id = item.get("ProviderIds", {}).get("Tmdb")
                i_type = item.get("Type")
                if not t_id: return None, None
                details = None
                try:
                    if i_type == 'Movie': details = tmdb.get_movie_details(t_id, processor.tmdb_api_key)
                    elif i_type == 'Series': details = tmdb.get_tv_details(t_id, processor.tmdb_api_key)
                except Exception: pass
                return str(t_id), details

            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = {executor.submit(fetch_tmdb_details, grp): grp for grp in batch_item_groups}
                for future in concurrent.futures.as_completed(futures):
                    t_id_str, details = future.result()
                    if t_id_str and details: tmdb_details_map[t_id_str] = details

            metadata_batch = []
            series_ids_processed_in_batch = set()

            for item_group in batch_item_groups:
                item = item_group[0]
                tmdb_id_str = str(item.get("ProviderIds", {}).get("Tmdb"))
                item_type = item.get("Type")
                tmdb_details = tmdb_details_map.get(tmdb_id_str)
                
                # --- 1. æ„å»ºé¡¶å±‚è®°å½• ---
                asset_details_list = []
                if item_type == "Movie":
                    asset_details_list = []
                    for v in item_group:
                        details = parse_full_asset_details(v)
                        details['source_library_id'] = v.get('_SourceLibraryId') 
                        asset_details_list.append(details)

                emby_runtime = round(item['RunTimeTicks'] / 600000000) if item.get('RunTimeTicks') else None

                top_record = {
                    "tmdb_id": tmdb_id_str, "item_type": item_type, "title": item.get('Name'),
                    "original_title": item.get('OriginalTitle'), "release_year": item.get('ProductionYear'),
                    "in_library": True, 
                    "subscription_status": "NONE",
                    "emby_item_ids_json": json.dumps(list(set(v.get('Id') for v in item_group if v.get('Id'))), ensure_ascii=False),
                    "asset_details_json": json.dumps(asset_details_list, ensure_ascii=False),
                    "rating": item.get('CommunityRating'),
                    "date_added": item.get('DateCreated'),
                    "genres_json": json.dumps(item.get('Genres', []), ensure_ascii=False),
                    "official_rating": item.get('OfficialRating'), 
                    "unified_rating": get_unified_rating(item.get('OfficialRating')),
                    "runtime_minutes": emby_runtime if (item_type == 'Movie' and emby_runtime) else tmdb_details.get('runtime') if (item_type == 'Movie' and tmdb_details) else None
                }
                if tmdb_details:
                    top_record['poster_path'] = tmdb_details.get('poster_path')
                    top_record['overview'] = tmdb_details.get('overview')
                    top_record['studios_json'] = json.dumps([s['name'] for s in tmdb_details.get('production_companies', [])], ensure_ascii=False)
                    if item_type == 'Movie':
                        top_record['runtime_minutes'] = tmdb_details.get('runtime')
                    
                    directors, countries, keywords = [], [], []
                    if item_type == 'Movie':
                        credits_data = tmdb_details.get("credits", {}) or tmdb_details.get("casts", {})
                        directors = [{'id': p.get('id'), 'name': p.get('name')} for p in credits_data.get('crew', []) if p.get('job') == 'Director']
                        country_codes = [c.get('iso_3166_1') for c in tmdb_details.get('production_countries', [])]
                        countries = translate_country_list(country_codes)
                        keywords_data = tmdb_details.get('keywords', {})
                        keyword_list = keywords_data.get('keywords', []) if isinstance(keywords_data, dict) else []
                        keywords = [k['name'] for k in keyword_list if k.get('name')]
                    elif item_type == 'Series':
                        directors = [{'id': c.get('id'), 'name': c.get('name')} for c in tmdb_details.get('created_by', [])]
                        countries = translate_country_list(tmdb_details.get('origin_country', []))
                        keywords_data = tmdb_details.get('keywords', {})
                        keyword_list = keywords_data.get('results', []) if isinstance(keywords_data, dict) else []
                        keywords = [k['name'] for k in keyword_list if k.get('name')]
                    top_record['directors_json'] = json.dumps(directors, ensure_ascii=False)
                    top_record['countries_json'] = json.dumps(countries, ensure_ascii=False)
                    top_record['keywords_json'] = json.dumps(keywords, ensure_ascii=False)
                else:
                    top_record['poster_path'] = None
                    top_record['studios_json'] = '[]'
                    top_record['directors_json'] = '[]'; top_record['countries_json'] = '[]'; top_record['keywords_json'] = '[]'

                metadata_batch.append(top_record)

                # --- 2. å¤„ç† Series çš„å­é›† ---
                if item_type == "Series":
                    series_ids_processed_in_batch.add(tmdb_id_str)
                    
                    series_emby_ids = [str(v.get('Id')) for v in item_group if v.get('Id')]
                    my_seasons = []
                    my_episodes = []
                    for s_id in series_emby_ids:
                        my_seasons.extend(series_to_seasons_map.get(s_id, []))
                        my_episodes.extend(series_to_episode_map.get(s_id, []))
                    
                    tmdb_children_map = {}
                    processed_season_numbers = set()
                    
                    if tmdb_details and 'seasons' in tmdb_details:
                        for s_info in tmdb_details.get('seasons', []):
                            try:
                                s_num = int(s_info.get('season_number'))
                            except (ValueError, TypeError):
                                continue
                            
                            matched_emby_seasons = []
                            for s in my_seasons:
                                try:
                                    if int(s.get('IndexNumber')) == s_num:
                                        matched_emby_seasons.append(s)
                                except (ValueError, TypeError):
                                    continue
                            
                            if matched_emby_seasons:
                                processed_season_numbers.add(s_num)
                                real_season_tmdb_id = str(s_info.get('id'))
                                season_poster = s_info.get('poster_path')
                                if not season_poster and tmdb_details:
                                    season_poster = tmdb_details.get('poster_path')

                                season_record = {
                                    "tmdb_id": real_season_tmdb_id,
                                    "item_type": "Season",
                                    "parent_series_tmdb_id": tmdb_id_str,
                                    "season_number": s_num,
                                    "title": s_info.get('name'),
                                    "overview": s_info.get('overview'),
                                    "poster_path": season_poster,
                                    "in_library": True,
                                    "subscription_status": "NONE",
                                    "emby_item_ids_json": json.dumps([s.get('Id') for s in matched_emby_seasons]),
                                    "ignore_reason": None
                                }
                                metadata_batch.append(season_record)
                                tmdb_children_map[f"S{s_num}"] = s_info

                                has_eps = any(e.get('ParentIndexNumber') == s_num for e in my_episodes)
                                if has_eps:
                                    try:
                                        s_details = tmdb.get_tv_season_details(tmdb_id_str, s_num, processor.tmdb_api_key)
                                        if s_details and 'episodes' in s_details:
                                            for ep in s_details['episodes']:
                                                if ep.get('episode_number') is not None:
                                                    tmdb_children_map[f"S{s_num}E{ep.get('episode_number')}"] = ep
                                    except: pass

                    # â˜…â˜…â˜… B. å…œåº•å¤„ç†ï¼šEmby æœ‰ä½† TMDb æ²¡æœ‰çš„å­£ â˜…â˜…â˜…
                    for s in my_seasons:
                        try:
                            s_num = int(s.get('IndexNumber'))
                        except (ValueError, TypeError):
                            continue

                        if s_num not in processed_season_numbers:
                            fallback_season_tmdb_id = f"{tmdb_id_str}-S{s_num}"
                            season_record = {
                                "tmdb_id": fallback_season_tmdb_id,
                                "item_type": "Season",
                                "parent_series_tmdb_id": tmdb_id_str,
                                "season_number": s_num,
                                "title": s.get('Name') or f"Season {s_num}",
                                "overview": None,
                                "poster_path": tmdb_details.get('poster_path') if tmdb_details else None,
                                "in_library": True,
                                "subscription_status": "NONE",
                                "emby_item_ids_json": json.dumps([s.get('Id')]),
                                "ignore_reason": "Local Season Only"
                            }
                            metadata_batch.append(season_record)
                            processed_season_numbers.add(s_num)

                    # C. å¤„ç†åˆ†é›†
                    ep_grouped = defaultdict(list)
                    for ep in my_episodes:
                        s_n, e_n = ep.get('ParentIndexNumber'), ep.get('IndexNumber')
                        if s_n is not None and e_n is not None:
                            ep_grouped[(s_n, e_n)].append(ep)
                    
                    for (s_n, e_n), versions in ep_grouped.items():
                        emby_ep = versions[0]
                        emby_ep_runtime = round(emby_ep['RunTimeTicks'] / 600000000) if emby_ep.get('RunTimeTicks') else None
                        lookup_key = f"S{s_n}E{e_n}"
                        tmdb_ep_info = tmdb_children_map.get(lookup_key)
                        
                        ep_asset_details_list = []
                        for v in versions:
                            details = parse_full_asset_details(v)
                            details['source_library_id'] = v.get('_SourceLibraryId')
                            ep_asset_details_list.append(details)

                        child_record = {
                            "item_type": "Episode",
                            "parent_series_tmdb_id": tmdb_id_str,
                            "season_number": s_n,
                            "episode_number": e_n,
                            "in_library": True,
                            "emby_item_ids_json": json.dumps([v.get('Id') for v in versions]),
                            "asset_details_json": json.dumps(ep_asset_details_list, ensure_ascii=False),
                            "ignore_reason": None
                        }

                        if tmdb_ep_info and tmdb_ep_info.get('id'):
                            child_record['tmdb_id'] = str(tmdb_ep_info.get('id'))
                            child_record['title'] = tmdb_ep_info.get('name')
                            child_record['overview'] = tmdb_ep_info.get('overview')
                            child_record['poster_path'] = tmdb_ep_info.get('still_path')
                            child_record['runtime_minutes'] = emby_ep_runtime if emby_ep_runtime else tmdb_ep_info.get('runtime')
                        else:
                            child_record['tmdb_id'] = f"{tmdb_id_str}-S{s_n}E{e_n}"
                            child_record['title'] = versions[0].get('Name')
                            child_record['overview'] = versions[0].get('Overview')
                            child_record['runtime_minutes'] = emby_ep_runtime
                        
                        metadata_batch.append(child_record)

            # 7. å†™å…¥æ•°æ®åº“ & å­é›†ç¦»çº¿å¯¹è´¦
            if metadata_batch:
                total_updated_count += len(metadata_batch)

                with connection.get_db_connection() as conn:
                    cursor = conn.cursor()
                    
                    # --- A. æ‰§è¡Œå†™å…¥ ---
                    for idx, metadata in enumerate(metadata_batch):
                        savepoint_name = f"sp_{idx}"
                        try:
                            cursor.execute(f"SAVEPOINT {savepoint_name};")
                            columns = [k for k, v in metadata.items() if v is not None]
                            values = [v for v in metadata.values() if v is not None]
                            cols_str = ', '.join(columns)
                            vals_str = ', '.join(['%s'] * len(values))
                            
                            update_clauses = []
                            for col in columns:
                                if col in ('tmdb_id', 'item_type', 'subscription_sources_json'): continue
                                update_clauses.append(f"{col} = EXCLUDED.{col}")
                            
                            sql = f"""
                                INSERT INTO media_metadata ({cols_str}, last_synced_at) 
                                VALUES ({vals_str}, NOW()) 
                                ON CONFLICT (tmdb_id, item_type) 
                                DO UPDATE SET {', '.join(update_clauses)}, last_synced_at = NOW()
                            """
                            cursor.execute(sql, tuple(values))
                        except Exception as e:
                            cursor.execute(f"ROLLBACK TO SAVEPOINT {savepoint_name};")
                            logger.error(f"å†™å…¥å¤±è´¥ {metadata.get('tmdb_id')}: {e}")
                    
                    # --- B. æ‰§è¡Œå­é›†ç¦»çº¿å¯¹è´¦ ---
                    if series_ids_processed_in_batch:
                        active_child_ids = {
                            m['tmdb_id'] for m in metadata_batch 
                            if m['item_type'] in ('Season', 'Episode')
                        }
                        active_child_ids_list = list(active_child_ids)
                        
                        if active_child_ids_list:
                            cursor.execute("""
                                UPDATE media_metadata
                                SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = '[]'::jsonb
                                WHERE parent_series_tmdb_id = ANY(%s)
                                  AND item_type IN ('Season', 'Episode')
                                  AND in_library = TRUE
                                  AND tmdb_id != ALL(%s)
                            """, (list(series_ids_processed_in_batch), active_child_ids_list))
                            total_offline_count += cursor.rowcount
                        else:
                            cursor.execute("""
                                UPDATE media_metadata
                                SET in_library = FALSE, emby_item_ids_json = '[]'::jsonb, asset_details_json = '[]'::jsonb
                                WHERE parent_series_tmdb_id = ANY(%s)
                                  AND item_type IN ('Season', 'Episode')
                                  AND in_library = TRUE
                            """, (list(series_ids_processed_in_batch),))
                            total_offline_count += cursor.rowcount

                    conn.commit()
            
            # æ‰¹æ¬¡å¤„ç†å®Œåæ¸…ç†ä¸´æ—¶æ•°æ®
            del batch_item_groups
            del tmdb_details_map
            del metadata_batch
            gc.collect()

            processed_count += len(batch_tasks)
            task_manager.update_status_from_thread(20 + int((processed_count / total_to_process) * 80), f"å¤„ç†è¿›åº¦ {processed_count}/{total_to_process}...")

        # æœ€ç»ˆæ—¥å¿—
        final_msg = f"åŒæ­¥å®Œæˆï¼æ–°å¢/æ›´æ–°: {total_updated_count} ä¸ªåª’ä½“é¡¹, æ ‡è®°ç¦»çº¿: {total_offline_count} ä¸ªåª’ä½“é¡¹ã€‚"
        logger.info(f"  âœ… {final_msg}")
        task_manager.update_status_from_thread(100, final_msg)

    except Exception as e:
        logger.error(f"æ‰§è¡Œ '{task_name}' ä»»åŠ¡æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        task_manager.update_status_from_thread(-1, f"ä»»åŠ¡å¤±è´¥: {e}")

def task_apply_main_cast_to_episodes(processor, series_id: str, episode_ids: list):
    """
    ã€V2 - æ–‡ä»¶ä¸­å¿ƒåŒ–é‡æ„ç‰ˆã€‘
    è½»é‡çº§ä»»åŠ¡ï¼šå½“å‰§é›†è¿½æ›´æ–°å¢åˆ†é›†æ—¶ï¼Œå°†ä¸»é¡¹ç›®çš„å®Œç¾æ¼”å‘˜è¡¨æ³¨å…¥åˆ°æ–°åˆ†é›†çš„ override å…ƒæ•°æ®æ–‡ä»¶ä¸­ã€‚
    æ­¤ä»»åŠ¡ä¸å†è¯»å†™ Emby APIï¼Œè€Œæ˜¯å§”æ‰˜æ ¸å¿ƒå¤„ç†å™¨çš„ sync_single_item_assets æ–¹æ³•æ‰§è¡Œç²¾å‡†çš„æ–‡ä»¶åŒæ­¥æ“ä½œã€‚
    """
    try:
        if not episode_ids:
            logger.info(f"  âœ å‰§é›† {series_id} è¿½æ›´ä»»åŠ¡è·³è¿‡ï¼šæœªæä¾›éœ€è¦æ›´æ–°çš„åˆ†é›†IDã€‚")
            return
        
        series_details_for_log = emby.get_emby_item_details(series_id, processor.emby_url, processor.emby_api_key, processor.emby_user_id, fields="Name,ProviderIds")
        series_name = series_details_for_log.get("Name", f"ID:{series_id}") if series_details_for_log else f"ID:{series_id}"

        logger.info(f"  âœ è¿½æ›´ä»»åŠ¡å¯åŠ¨ï¼šå‡†å¤‡ä¸ºå‰§é›† ã€Š{series_name}ã€‹ çš„ {len(episode_ids)} ä¸ªæ–°åˆ†é›†åŒæ­¥å…ƒæ•°æ®...")

        processor.sync_single_item_assets(
            item_id=series_id,
            update_description=f"è¿½æ›´æ–°å¢ {len(episode_ids)} ä¸ªåˆ†é›†",
            sync_timestamp_iso=datetime.now(timezone.utc).isoformat(),
            episode_ids_to_sync=episode_ids
        )

        logger.info(f"  âœ å¤„ç†å®Œæˆï¼Œæ­£åœ¨é€šçŸ¥ Emby åˆ·æ–°...")
        emby.refresh_emby_item_metadata(
            item_emby_id=series_id,
            emby_server_url=processor.emby_url,
            emby_api_key=processor.emby_api_key,
            user_id_for_ops=processor.emby_user_id,
            replace_all_metadata_param=True,
            item_name_for_log=series_name
        )

        # TGé€šçŸ¥
        if series_details_for_log:
            logger.info(f"  âœ æ­£åœ¨ä¸ºã€Š{series_name}ã€‹è§¦å‘è¿½æ›´é€šçŸ¥...")
            telegram.send_media_notification(
                item_details=series_details_for_log,
                notification_type='update',
                new_episode_ids=episode_ids
            )

        # æ­¥éª¤ 3: æ›´æ–°çˆ¶å‰§é›†åœ¨å…ƒæ•°æ®ç¼“å­˜ä¸­çš„ last_synced_at æ—¶é—´æˆ³ (è¿™ä¸ªé€»è¾‘å¯ä»¥ä¿ç•™)
        if series_details_for_log:
            tmdb_id = series_details_for_log.get("ProviderIds", {}).get("Tmdb")
            if tmdb_id:
                try:
                    with connection.get_db_connection() as conn:
                        with conn.cursor() as cursor:
                            cursor.execute(
                                "UPDATE media_metadata SET last_synced_at = %s WHERE tmdb_id = %s AND item_type = 'Series'",
                                (datetime.now(timezone.utc), tmdb_id)
                            )
                except Exception as db_e:
                    logger.error(f"  âœ æ›´æ–°å‰§é›†ã€Š{series_name}ã€‹çš„æ—¶é—´æˆ³æ—¶å‘ç”Ÿæ•°æ®åº“é”™è¯¯: {db_e}", exc_info=True)

    except Exception as e:
        logger.error(f"  âœ ä¸ºå‰§é›† {series_id} çš„æ–°åˆ†é›†åº”ç”¨ä¸»æ¼”å‘˜è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        raise