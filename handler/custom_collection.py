# handler/custom_collection.py
import logging
import requests
import xml.etree.ElementTree as ET
import re
import os
import time
import gevent
import numpy as np
import sys
import gevent
from typing import List, Dict, Any, Optional, Tuple
import json
from datetime import datetime, timedelta, date
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
from gevent import subprocess, Timeout

import handler.tmdb as tmdb
import handler.emby as emby
import config_manager
from tasks.helpers import parse_series_title_and_season
from database import custom_collection_db, watchlist_db, media_db, connection
from handler.douban import DoubanApi
from handler.tmdb import search_media, get_tv_details
from ai_translator import AITranslator

logger = logging.getLogger(__name__)


class ListImporter:
    """
    (V9.1 - æœ€ç»ˆå¼‚æ­¥ç‰ˆ)
    ä½¿ç”¨ gevent.subprocessï¼Œå¹¶ç¡®ä¿åœ¨ç‹¬ç«‹çš„ greenlet ä¸­è¿è¡Œï¼Œ
    ä»è€Œå®ç°çœŸæ­£çš„éé˜»å¡å¼‚æ­¥æ‰§è¡Œã€‚
    """
    
    SEASON_PATTERN = re.compile(r'(.*?)\s*[ï¼ˆ(]?\s*(ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+)\s*å­£\s*[)ï¼‰]?')
    
    # â–¼â–¼â–¼ ä¼˜åŒ–ï¼šæ‰©å±•æ•°å­—æ˜ å°„ï¼Œæ”¯æŒåˆ°äºŒåå­£ï¼Œå¢å¼ºå…¼å®¹æ€§ â–¼â–¼â–¼
    CHINESE_NUM_MAP = {
        'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
        'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15, 'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18, 'åä¹': 19, 'äºŒå': 20,
        'ç¬¬ä¸€': 1, 'ç¬¬äºŒ': 2, 'ç¬¬ä¸‰': 3, 'ç¬¬å››': 4, 'ç¬¬äº”': 5, 'ç¬¬å…­': 6, 'ç¬¬ä¸ƒ': 7, 'ç¬¬å…«': 8, 'ç¬¬ä¹': 9, 'ç¬¬å': 10,
        'ç¬¬åä¸€': 11, 'ç¬¬åäºŒ': 12, 'ç¬¬åä¸‰': 13, 'ç¬¬åå››': 14, 'ç¬¬åäº”': 15, 'ç¬¬åå…­': 16, 'ç¬¬åä¸ƒ': 17, 'ç¬¬åå…«': 18, 'ç¬¬åä¹': 19, 'ç¬¬äºŒå': 20
    }
    VALID_MAOYAN_PLATFORMS = {'tencent', 'iqiyi', 'youku', 'mango'}

    def __init__(self, tmdb_api_key: str):
        self.tmdb_api_key = tmdb_api_key
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})

    # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šè¿™ä¸ªå‡½æ•°ç°åœ¨æ˜¯çº¯ç²¹çš„é˜»å¡æ‰§è¡Œé€»è¾‘ â˜…â˜…â˜…
    def _execute_maoyan_fetch(self, definition: Dict) -> List[Dict[str, str]]:
        maoyan_url = definition.get('url', '')
        temp_output_file = os.path.join(config_manager.PERSISTENT_DATA_PATH, f"maoyan_temp_output_{hash(maoyan_url)}.json")
        
        content_key = maoyan_url.replace('maoyan://', '')
        parts = content_key.split('-')
        
        platform = 'all'
        if len(parts) > 1 and parts[-1] in self.VALID_MAOYAN_PLATFORMS:
            platform = parts[-1]
            type_part = '-'.join(parts[:-1])
        else:
            type_part = content_key

        types_to_fetch = [t.strip() for t in type_part.split(',') if t.strip()]
        
        if not types_to_fetch:
            logger.error(f"  âœ æ— æ³•ä»çŒ«çœ¼URL '{maoyan_url}' ä¸­è§£æå‡ºæœ‰æ•ˆçš„ç±»å‹ã€‚")
            return []
            
        limit = definition.get('limit')
        if not limit:
            limit = 50

        fetcher_script_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'maoyan_fetcher.py')
        if not os.path.exists(fetcher_script_path):
            logger.error(f"  âœ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•æ‰¾åˆ°çŒ«çœ¼è·å–è„šæœ¬ '{fetcher_script_path}'ã€‚")
            return []

        command = [
            sys.executable,
            fetcher_script_path,
            '--api-key', self.tmdb_api_key,
            '--output-file', temp_output_file,
            '--num', str(limit),
            '--platform', platform,
            '--types', *types_to_fetch
        ]
        
        try:
            logger.debug(f"  âœ (åœ¨ä¸€ä¸ªç‹¬ç«‹çš„ Greenlet ä¸­) æ‰§è¡Œå‘½ä»¤: {' '.join(command)}")
            
            result_bytes = subprocess.check_output(
                command, 
                stderr=subprocess.STDOUT, 
                timeout=600
            )
            
            result_output = result_bytes.decode('utf-8', errors='ignore')
            logger.info("  âœ çŒ«çœ¼è·å–è„šæœ¬æˆåŠŸå®Œæˆã€‚")
            if result_output:
                logger.debug(f"  âœ è„šæœ¬è¾“å‡º:\n{result_output}")
            
            with open(temp_output_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            return results

        except Timeout:
            logger.error("  âœ æ‰§è¡ŒçŒ«çœ¼è·å–è„šæœ¬è¶…æ—¶ï¼ˆè¶…è¿‡10åˆ†é’Ÿï¼‰ã€‚")
            return []
        except subprocess.CalledProcessError as e:
            error_output = e.output.decode('utf-8', errors='ignore') if e.output else "No output captured."
            logger.error(f"  âœ æ‰§è¡ŒçŒ«çœ¼è·å–è„šæœ¬å¤±è´¥ã€‚è¿”å›ç : {e.returncode}")
            logger.error(f"  âœ è„šæœ¬çš„å®Œæ•´é”™è¯¯è¾“å‡º:\n{error_output}")
            return []
        except Exception as e:
            logger.error(f"  âœ å¤„ç†çŒ«çœ¼æ¦œå•æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return []
        finally:
            if os.path.exists(temp_output_file):
                os.remove(temp_output_file)

    # ... å…¶ä»–æ‰€æœ‰æ–¹æ³• (_match_by_ids, process, FilterEngineç­‰) ä¿æŒå®Œå…¨ä¸å˜ ...
    def _match_by_ids(self, imdb_id: Optional[str], tmdb_id: Optional[str], item_type: str) -> Optional[str]:
        if tmdb_id:
            logger.debug(f"  âœ é€šè¿‡TMDb IDç›´æ¥åŒ¹é…ï¼š{tmdb_id}")
            return tmdb_id
        if imdb_id:
            logger.debug(f"  âœ é€šè¿‡IMDb IDæŸ¥æ‰¾TMDb IDï¼š{imdb_id}")
            try:
                tmdb_id_from_imdb = tmdb.get_tmdb_id_by_imdb_id(imdb_id, self.tmdb_api_key, item_type)
                if tmdb_id_from_imdb:
                    logger.debug(f"  âœ IMDb ID {imdb_id} å¯¹åº” TMDb ID: {tmdb_id_from_imdb}")
                    return str(tmdb_id_from_imdb)
                else:
                    logger.warning(f"  âœ æ— æ³•é€šè¿‡IMDb ID {imdb_id} æŸ¥æ‰¾åˆ°å¯¹åº”çš„TMDb IDã€‚")
            except Exception as e:
                logger.error(f"  âœ é€šè¿‡IMDb IDæŸ¥æ‰¾TMDb IDæ—¶å‡ºé”™: {e}")
        return None
    
    def _extract_ids_from_title_or_line(self, title_line: str) -> Tuple[Optional[str], Optional[str]]:
        imdb_id = None
        tmdb_id = None
        imdb_match = re.search(r'(tt\d{7,8})', title_line, re.I)
        if imdb_match:
            imdb_id = imdb_match.group(1)
        tmdb_match = re.search(r'tmdb://(\d+)', title_line, re.I)
        if tmdb_match:
            tmdb_id = tmdb_match.group(1)
        return imdb_id, tmdb_id
    
    def _get_items_from_douban_doulist(self, url: str) -> List[Dict[str, str]]:
        """ä¸“é—¨ç”¨äºè§£æå’Œåˆ†é¡µè·å–è±†ç“£è±†åˆ—å†…å®¹çš„å‡½æ•°"""
        all_items = []
        # ä»URLä¸­ç§»é™¤åˆ†é¡µå‚æ•°ï¼Œå¾—åˆ°åŸºç¡€URL
        base_url = url.split('?')[0]
        page_start = 0
        # è®¾ç½®ä¸€ä¸ªæœ€å¤§é¡µæ•°é™åˆ¶ï¼Œé˜²æ­¢æ„å¤–çš„æ— é™å¾ªç¯
        max_pages = 50 
        items_per_page = 25

        logger.info(f"  âœ æ£€æµ‹åˆ°è±†ç“£è±†åˆ—é“¾æ¥ï¼Œå¼€å§‹åˆ†é¡µè·å–: {base_url}")

        for page in range(max_pages):
            current_start = page * items_per_page
            paginated_url = f"{base_url}?start={current_start}&sort=seq&playable=0&sub_type="
            
            try:
                logger.debug(f"    âœ æ­£åœ¨è·å–ç¬¬ {page + 1} é¡µ: {paginated_url}")
                response = self.session.get(paginated_url, timeout=20)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'lxml')
                
                # æŸ¥æ‰¾é¡µé¢ä¸Šæ‰€æœ‰çš„æ¡ç›®å®¹å™¨
                doulist_items = soup.find_all('div', class_='doulist-item')

                # å¦‚æœå½“å‰é¡µæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¡ç›®ï¼Œè¯´æ˜åˆ°è¾¾äº†æœ€åä¸€é¡µ
                if not doulist_items:
                    logger.info(f"  âœ åœ¨ç¬¬ {page + 1} é¡µæœªå‘ç°æ›´å¤šé¡¹ç›®ï¼Œè·å–ç»“æŸã€‚")
                    break

                for item in doulist_items:
                    title_div = item.find('div', class_='title')
                    if not title_div: continue
                    
                    link_tag = title_div.find('a')
                    if not link_tag: continue
                    
                    # æå–æ ‡é¢˜
                    title = link_tag.get_text(strip=True)
                    # æå–è±†ç“£é“¾æ¥
                    douban_link = link_tag.get('href')
                    
                    # å°è¯•æå–å¹´ä»½
                    year = None
                    abstract_div = item.find('div', class_='abstract')
                    if abstract_div:
                        # å¹´ä»½é€šå¸¸åœ¨ abstract å†…å®¹ä¸­ä»¥ (YYYY) æˆ– YYYY-MM-DD çš„å½¢å¼å‡ºç°
                        year_match = re.search(r'\b(19\d{2}|20\d{2})\b', abstract_div.get_text())
                        if year_match:
                            year = year_match.group(1)
                    
                    if title:
                        all_items.append({
                            'title': title,
                            'imdb_id': None, # è±†åˆ—é¡µé¢ä¸ç›´æ¥æä¾›IMDb ID
                            'year': year,
                            'douban_link': douban_link # âœ¨ å…³é”®ä¿¡æ¯ï¼šæˆ‘ä»¬æ‹¿åˆ°äº†æ¯ä¸ªæ¡ç›®çš„è±†ç“£é“¾æ¥
                        })

            except Exception as e:
                logger.error(f"  âœ è·å–æˆ–è§£æè±†ç“£è±†åˆ—é¡µé¢ '{paginated_url}' æ—¶å‡ºé”™: {e}")
                # å‡ºç°é”™è¯¯æ—¶ï¼Œä¸­æ–­åç»­æ‰€æœ‰é¡µé¢çš„è·å–
                break
        
        logger.info(f"  âœ è±†ç“£è±†åˆ—è·å–å®Œæˆï¼Œä» {page} ä¸ªé¡µé¢ä¸­æ€»å…±è§£æå‡º {len(all_items)} ä¸ªé¡¹ç›®ã€‚")
        return all_items
    
    def _get_items_from_tmdb_list(self, url: str) -> List[Dict[str, str]]:
        """ä¸“é—¨ç”¨äºè§£æå’Œåˆ†é¡µè·å–TMDbç‰‡å•å†…å®¹çš„å‡½æ•°"""
        match = re.search(r'themoviedb\.org/list/(\d+)', url)
        if not match:
            logger.error(f"  âœ æ— æ³•ä»URL '{url}' ä¸­è§£æå‡ºTMDbç‰‡å•IDã€‚")
            return []

        list_id = int(match.group(1))
        all_items = []
        current_page = 1
        total_pages = 1 # å…ˆå‡è®¾åªæœ‰ä¸€é¡µ

        logger.info(f"  âœ æ£€æµ‹åˆ°TMDbç‰‡å•é“¾æ¥ï¼Œå¼€å§‹åˆ†é¡µè·å–: {url}")

        while current_page <= total_pages:
            try:
                logger.debug(f"    âœ æ­£åœ¨è·å–ç¬¬ {current_page} / {total_pages} é¡µ...")
                list_data = tmdb.get_list_details_tmdb(list_id, self.tmdb_api_key, page=current_page)

                if not list_data or not list_data.get('items'):
                    logger.warning(f"  âœ åœ¨ç¬¬ {current_page} é¡µæœªå‘ç°æ›´å¤šé¡¹ç›®ï¼Œè·å–ç»“æŸã€‚")
                    break

                # ä»ç¬¬ä¸€é¡µçš„è¿”å›ç»“æœä¸­æ›´æ–°æ€»é¡µæ•°
                if current_page == 1:
                    total_pages = list_data.get('total_pages', 1)

                for item in list_data['items']:
                    media_type = item.get('media_type')
                    tmdb_id = item.get('id')
                    
                    # å°†TMDbçš„ 'tv' æ˜ å°„ä¸ºæˆ‘ä»¬ç³»ç»Ÿå†…éƒ¨çš„ 'Series'
                    item_type_mapped = 'Series' if media_type == 'tv' else 'Movie'

                    title = item.get('title') if item_type_mapped == 'Movie' else item.get('name')

                    if tmdb_id:
                        all_items.append({
                            'id': str(tmdb_id), 
                            'type': item_type_mapped,
                            'title': title # æ–°å¢å­—æ®µ
                        })

                current_page += 1

            except Exception as e:
                logger.error(f"  âœ è·å–æˆ–è§£æTMDbç‰‡å•é¡µé¢ {current_page} æ—¶å‡ºé”™: {e}")
                break
        
        logger.info(f"  âœ TMDbç‰‡å•è·å–å®Œæˆï¼Œä» {total_pages} ä¸ªé¡µé¢ä¸­æ€»å…±è§£æå‡º {len(all_items)} ä¸ªé¡¹ç›®ã€‚")
        return all_items
    
    def _get_items_from_tmdb_discover(self, url: str) -> List[Dict[str, str]]:
        """ä¸“é—¨ç”¨äºè§£æTMDb Discover URLå¹¶è·å–ç»“æœçš„å‡½æ•°ï¼Œæ”¯æŒè‡ªåŠ¨åˆ†é¡µå¹¶è¿‡æ»¤æ— æµ·æŠ¥/æ— ä¸­æ–‡å…ƒæ•°æ®çš„é¡¹ç›®"""
        from urllib.parse import urlparse, parse_qs
        from datetime import datetime, timedelta
        import re

        logger.info(f"  âœ æ£€æµ‹åˆ°TMDb Discoveré“¾æ¥ï¼Œå¼€å§‹åŠ¨æ€è·å– (æ”¯æŒåˆ†é¡µå’Œè¿‡æ»¤): {url}")
        
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)
        params = {k: v[0] for k, v in query_params.items()}

        today = datetime.now()
        date_pattern = re.compile(r'{today([+-]\d+)?}')

        for key, value in params.items():
            match = date_pattern.search(value)
            if match:
                offset_str = match.group(1) 
                target_date = today
                if offset_str:
                    days = int(offset_str)
                    target_date = today + timedelta(days=days)
                params[key] = value.replace(match.group(0), target_date.strftime('%Y-%m-%d'))

        all_items = []
        current_page = 1
        total_pages = 1
        MAX_PAGES_TO_FETCH = 10

        while current_page <= total_pages and current_page <= MAX_PAGES_TO_FETCH:
            try:
                params['page'] = current_page
                logger.debug(f"    âœ æ­£åœ¨è·å–ç¬¬ {current_page} / {total_pages} é¡µ...")

                discover_data = None
                item_type_for_result = None

                if '/discover/movie' in url:
                    discover_data = tmdb.discover_movie_tmdb(self.tmdb_api_key, params)
                    item_type_for_result = 'Movie'
                elif '/discover/tv' in url:
                    discover_data = tmdb.discover_tv_tmdb(self.tmdb_api_key, params)
                    item_type_for_result = 'Series'
                else:
                    logger.warning(f"  ğŸš« æ— æ³•ä»URL '{url}' åˆ¤æ–­æ˜¯ç”µå½±è¿˜æ˜¯ç”µè§†å‰§ï¼Œdiscoverä»»åŠ¡ä¸­æ­¢ã€‚")
                    break

                if not discover_data or not discover_data.get('results'):
                    logger.info("    âœ åœ¨å½“å‰é¡µæœªå‘ç°æ›´å¤šé¡¹ç›®ï¼Œè·å–ç»“æŸã€‚")
                    break

                if current_page == 1:
                    total_pages = discover_data.get('total_pages', 1)

                for item in discover_data['results']:
                    # ç­›é€‰æ¡ä»¶ 1: å¿…é¡»æœ‰æµ·æŠ¥ (poster_pathä¸ä¸ºNoneæˆ–ç©º)
                    if not item.get('poster_path'):
                        logger.debug(f"  âœ ç­›é€‰TMDb Discoverç»“æœï¼šè·³è¿‡é¡¹ç›® '{item.get('title') or item.get('name')}' (ID: {item.get('id')})ï¼Œå› ä¸ºå®ƒæ²¡æœ‰æµ·æŠ¥ã€‚")
                        continue

                    # ç­›é€‰æ¡ä»¶ 2: å¿…é¡»æœ‰ä¸­æ–‡å…ƒæ•°æ® (ä»¥overviewå­—æ®µä¸ä¸ºç©ºä½œä¸ºåˆ¤æ–­ä¾æ®)
                    # TMDB APIåœ¨æŒ‡å®šlanguage=zh-CNæ—¶ï¼Œè‹¥æ— ä¸­æ–‡ç®€ä»‹ï¼Œæ­¤å­—æ®µé€šå¸¸ä¸ºç©º
                    if not item.get('overview'):
                        logger.debug(f"  âœ ç­›é€‰TMDb Discoverç»“æœï¼šè·³è¿‡é¡¹ç›® '{item.get('title') or item.get('name')}' (ID: {item.get('id')})ï¼Œå› ä¸ºå®ƒæ²¡æœ‰ä¸­æ–‡ç®€ä»‹ã€‚")
                        continue
                    
                    tmdb_id = item.get('id')
                    if tmdb_id and item_type_for_result:
                        title = item.get('title') if item_type_for_result == 'Movie' else item.get('name')
                        date_str = item.get('release_date') if item_type_for_result == 'Movie' else item.get('first_air_date')
                        year = date_str[:4] if date_str else None
                        all_items.append({
                            'id': str(tmdb_id), 
                            'type': item_type_for_result,
                            'title': title,
                            'release_date': date_str, 
                            'year': year
                        })
                
                current_page += 1

            except Exception as e:
                logger.error(f"  âœ è·å–æˆ–è§£æTMDb Discoveré“¾æ¥çš„ç¬¬ {current_page} é¡µæ—¶å‡ºé”™: {e}")
                break

        logger.info(f"  âœ TMDb Discover è·å–å®Œæˆï¼Œä» {total_pages} ä¸ªé¡µé¢ä¸­æ€»å…±è§£æå‡º {len(all_items)} ä¸ªé¡¹ç›®ã€‚")
        return all_items
    
    def _get_titles_and_imdbids_from_url(self, url: str) -> Tuple[List[Dict[str, str]], str]:
        source_type = 'list_rss' 
        items = []

        if 'themoviedb.org/discover/' in url:
            source_type = 'list_discover'
            items = self._get_items_from_tmdb_discover(url)
        elif 'themoviedb.org/list/' in url:
            source_type = 'list_tmdb'
            items = self._get_items_from_tmdb_list(url)
        elif 'douban.com/doulist' in url:
            source_type = 'list_douban'
            items = self._get_items_from_douban_doulist(url)
        else:
            logger.info(f"  âœ å¼€å§‹è·å–æ ‡å‡†RSSæ¦œå•: {url}")
            try:
                response = self.session.get(url, timeout=20)
                response.raise_for_status()
                content = response.text
                if 'encoding="gb2312"' in content.lower():
                     content = response.content.decode('gb2312', errors='ignore')
                
                root = ET.fromstring(content)
                channel = root.find('channel')
                if channel is None: return [], source_type

                for item in channel.findall('item'):
                    title_elem = item.find('title')
                    guid_elem = item.find('guid')
                    link_elem = item.find('link')
                    description_elem = item.find('description')
                    
                    title = title_elem.text if title_elem is not None else None
                    description = description_elem.text if description_elem is not None else ''
                    
                    douban_link = None
                    if link_elem is not None and link_elem.text and 'douban.com' in link_elem.text:
                        douban_link = link_elem.text
                    elif guid_elem is not None and guid_elem.text and 'douban.com' in guid_elem.text:
                        douban_link = guid_elem.text

                    year = None
                    year_match = re.search(r'\b(20\d{2})\b', description)
                    if year_match: year = year_match.group(1)

                    imdb_id = None
                    if guid_elem is not None and guid_elem.text:
                        match = re.search(r'tt\d{7,8}', guid_elem.text)
                        if match: imdb_id = match.group(0)
                    if not imdb_id and link_elem is not None and link_elem.text:
                        match = re.search(r'tt\d{7,8}', link_elem.text)
                        if match: imdb_id = match.group(0)
                    
                    if title:
                        items.append({'title': title.strip(), 'imdb_id': imdb_id, 'year': year, 'douban_link': douban_link})
            except Exception as e:
                logger.error(f"ä»RSS URL '{url}' è·å–æ¦œå•æ—¶å‡ºé”™: {e}")
        
        return items, source_type

    def _match_title_to_tmdb(self, title: str, item_type: str, year: Optional[str] = None) -> Optional[Tuple[str, str, Optional[int]]]:
        """
        - å¢åŠ äº†å¯¹å‰§é›†æœç´¢ç»“æœçš„ç²¾ç¡®éªŒè¯é€»è¾‘ï¼Œé¿å…å› TMDbå­˜åœ¨ä¸è§„èŒƒæ¡ç›®ï¼ˆå¦‚ "å‰§å2"ï¼‰æ—¶ï¼Œé”™è¯¯åœ°åŒ¹é…åˆ°éåŸºç¡€å‰§é›†ã€‚
        - ç»Ÿä¸€äº†è¿”å›å€¼æ ¼å¼ä¸ºå…ƒç»„ (tmdb_id, item_type, season_number)ã€‚
        """
        def normalize_string(s: str) -> str:
            if not s: return ""
            # ç§»é™¤äº†ä¸­æ–‡å¥å·ï¼Œå¢åŠ äº†å¯¹æ›´å¤šåˆ†éš”ç¬¦çš„å…¼å®¹
            return re.sub(r'[\s:ï¼šÂ·\-*\'!,?.ã€‚]+', '', s).lower()

        if item_type == 'Movie':
            # --- ç”µå½±åŒ¹é…é€»è¾‘ä¿æŒä¸å˜ ---
            titles_to_try = set([title.strip()])
            match = re.match(r'([\u4e00-\u9fa5\sÂ·0-9]+)[\s:ï¼š*]*(.*)', title.strip())
            if match:
                part1 = match.group(1).strip()
                part2 = match.group(2).strip()
                if part1: titles_to_try.add(part1)
                if part2: titles_to_try.add(part2)

            num_map = {'1': 'ä¸€', '2': 'äºŒ', '3': 'ä¸‰', '4': 'å››', '5': 'äº”', '6': 'å…­', '7': 'ä¸ƒ', '8': 'å…«', '9': 'ä¹'}
            current_titles = list(titles_to_try) 
            for t in current_titles:
                if any(num in t for num in num_map.keys()):
                    new_title = t
                    for num, char in num_map.items():
                        new_title = new_title.replace(num, char)
                    titles_to_try.add(new_title)
            
            final_titles = list(titles_to_try)
            logger.debug(f"  âœ ä¸º '{title}' ç”Ÿæˆçš„æœ€ç»ˆå€™é€‰æœç´¢æ ‡é¢˜: {final_titles}")

            first_search_results = None
            year_info = f" (å¹´ä»½: {year})" if year else ""

            for title_variation in final_titles:
                if not title_variation: continue
                
                results = search_media(title_variation, self.tmdb_api_key, 'Movie', year=year)
                
                if first_search_results is None:
                    first_search_results = results

                if not results:
                    continue

                norm_variation = normalize_string(title_variation)

                for result in results:
                    norm_title = normalize_string(result.get('title'))
                    norm_original_title = normalize_string(result.get('original_title'))

                    if norm_variation == norm_title or norm_variation == norm_original_title:
                        tmdb_id = str(result.get('id'))
                        logger.info(f"  âœ ç”µå½±æ ‡é¢˜ '{title}'{year_info} é€šè¿‡ã€ç²¾ç¡®è§„èŒƒåŒ¹é…ã€‘(ä½¿ç”¨'{title_variation}') æˆåŠŸåŒ¹é…åˆ°: {result.get('title')} (ID: {tmdb_id})")
                        return tmdb_id, 'Movie', None
                
                for result in results:
                    norm_title = normalize_string(result.get('title'))
                    norm_original_title = normalize_string(result.get('original_title'))

                    if norm_variation in norm_title or norm_variation in norm_original_title:
                        tmdb_id = str(result.get('id'))
                        logger.info(f"  âœ ç”µå½±æ ‡é¢˜ '{title}'{year_info} é€šè¿‡ã€åŒ…å«åŒ¹é…ã€‘(ä½¿ç”¨'{title_variation}') æˆåŠŸåŒ¹é…åˆ°: {result.get('title')} (ID: {tmdb_id})")
                        return tmdb_id, 'Movie', None

            if first_search_results:
                first_result = first_search_results[0]
                tmdb_id = str(first_result.get('id'))
                logger.warning(f"  âœ ç”µå½±æ ‡é¢˜ '{title}'{year_info} æ‰€æœ‰ç²¾ç¡®åŒ¹é…å’ŒåŒ…å«åŒ¹é…å‡å¤±è´¥ã€‚å°†ã€å›é€€ä½¿ç”¨ã€‘æœ€ç›¸å…³çš„æœç´¢ç»“æœ: {first_result.get('title')} (ID: {tmdb_id})")
                return tmdb_id, 'Movie', None

            logger.error(f"  âœ ç”µå½±æ ‡é¢˜ '{title}'{year_info} æœªèƒ½åœ¨TMDbä¸Šæ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœã€‚")
            return None
        
        elif item_type == 'Series':
            # 1. è§£ææ ‡é¢˜
            show_name_parsed, season_number_to_validate = parse_series_title_and_season(title, api_key=self.tmdb_api_key)
            show_name = show_name_parsed if show_name_parsed else title
            
            # 2. æœç´¢
            results = search_media(show_name, self.tmdb_api_key, 'Series', year=year)

            # å›é€€æœç´¢é€»è¾‘ 
            if not results and year and season_number_to_validate is not None:
                logger.debug(f"  âœ å¸¦å¹´ä»½ '{year}' æœç´¢å‰§é›† '{show_name}' æœªæ‰¾åˆ°ç»“æœï¼Œå¯èƒ½æ˜¯åç»­å­£ã€‚å°è¯•ä¸å¸¦å¹´ä»½è¿›è¡Œå›é€€æœç´¢...")
                results = search_media(show_name, self.tmdb_api_key, 'Series', year=None)

            if not results:
                year_info = f" (å¹´ä»½: {year})" if year else ""
                logger.warning(f"  âœ å‰§é›†æ ‡é¢˜ '{title}' (æœç´¢è¯: '{show_name}'){year_info} æœªèƒ½åœ¨TMDbä¸Šæ‰¾åˆ°åŒ¹é…é¡¹ã€‚")
                return None
            
            # æƒ…å†µ A: ä¸éœ€è¦éªŒè¯å­£å· (ç›´æ¥æ‰¾æœ€åƒçš„)
            if season_number_to_validate is None:
                series_result = None
                norm_show_name = normalize_string(show_name)
                
                # ä¼˜å…ˆæ‰¾ç²¾ç¡®åŒ¹é…
                for result in results:
                    if normalize_string(result.get('name', '')) == norm_show_name:
                        series_result = result
                        logger.debug(f"  âœ å‰§é›† '{show_name}' é€šè¿‡ã€ç²¾ç¡®è§„èŒƒåŒ¹é…ã€‘æ‰¾åˆ°äº†: {result.get('name')} (ID: {result.get('id')})")
                        break 
                
                # æ²¡æ‰¾åˆ°ç²¾ç¡®çš„å°±ç”¨ç¬¬ä¸€ä¸ª
                if not series_result:
                    series_result = results[0]
                    logger.warning(f"  âœ å‰§é›† '{show_name}' æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨é¦–ä¸ªç»“æœ: {series_result.get('name')} (ID: {series_result.get('id')})")

                return str(series_result.get('id')), 'Series', None

            # æƒ…å†µ B: éœ€è¦éªŒè¯å­£å· (éå†å‰ 5 ä¸ªç»“æœï¼Œè°æœ‰è¿™ä¸€å­£å°±ç®—è°çš„)
            else:
                # å®šä¹‰ä¸€ä¸ªå†…éƒ¨å‡½æ•°æ¥æ‰§è¡ŒéªŒè¯é€»è¾‘ï¼Œé¿å…ä»£ç é‡å¤
                def verify_season_in_results(candidates_list, source_desc=""):
                    if not candidates_list:
                        return None
                    
                    # æ’åºï¼šåå­—å®Œå…¨åŒ¹é…çš„æ’åœ¨æœ€å‰é¢
                    norm_show_name = normalize_string(show_name)
                    candidates_list.sort(key=lambda x: 0 if normalize_string(x.get('name', '')) == norm_show_name else 1)
                    
                    logger.info(f"  âœ å‰§é›† '{show_name}'{source_desc} éœ€è¦éªŒè¯ç¬¬ {season_number_to_validate} å­£ï¼Œæ­£åœ¨æ‰«æ {len(candidates_list)} ä¸ªå€™é€‰ç»“æœ...")

                    for candidate in candidates_list:
                        candidate_id = str(candidate.get('id'))
                        candidate_name = candidate.get('name')
                        
                        # è·å–è¯¥å‰§é›†çš„è¯¦æƒ…ï¼ˆåŒ…å«å­£ä¿¡æ¯ï¼‰
                        series_details = get_tv_details(int(candidate_id), self.tmdb_api_key, append_to_response="seasons")
                        
                        if series_details and 'seasons' in series_details:
                            # æ£€æŸ¥æ˜¯å¦æœ‰ç›®æ ‡å­£
                            has_season = False
                            for season in series_details['seasons']:
                                if season.get('season_number') == season_number_to_validate:
                                    has_season = True
                                    break
                            
                            if has_season:
                                logger.info(f"  âœ åŒ¹é…æˆåŠŸï¼åœ¨å€™é€‰ç»“æœ '{candidate_name}' (ID: {candidate_id}) ä¸­æ‰¾åˆ°äº†ç¬¬ {season_number_to_validate} å­£ã€‚")
                                return candidate_id
                            else:
                                logger.debug(f"    - å€™é€‰ '{candidate_name}' (ID: {candidate_id}) æ²¡æœ‰ç¬¬ {season_number_to_validate} å­£ï¼Œè·³è¿‡ã€‚")
                    return None

                # 1. ç¬¬ä¸€æ¬¡å°è¯•ï¼šä½¿ç”¨å½“å‰çš„æœç´¢ç»“æœï¼ˆå¯èƒ½å¸¦å¹´ä»½ï¼‰
                matched_id = verify_season_in_results(results[:5])
                if matched_id:
                    return matched_id, 'Series', season_number_to_validate

                # 2. å¦‚æœå¸¦å¹´ä»½éªŒè¯å¤±è´¥ï¼Œå°è¯•å»æ‰å¹´ä»½é‡æœ 
                if year:
                    logger.info(f"  âœ å‰§é›† '{show_name}' å¸¦å¹´ä»½ ({year}) æœç´¢ç»“æœä¸­æœªæ‰¾åˆ°ç¬¬ {season_number_to_validate} å­£ï¼Œå°è¯•ç§»é™¤å¹´ä»½é‡æœ...")
                    results_no_year = search_media(show_name, self.tmdb_api_key, 'Series', year=None)
                    
                    if results_no_year:
                        # æ’é™¤æ‰å·²ç»åœ¨ç¬¬ä¸€æ¬¡æœç´¢ä¸­éªŒè¯è¿‡çš„IDï¼Œé¿å…é‡å¤APIè¯·æ±‚
                        checked_ids = set(str(r.get('id')) for r in results[:5])
                        candidates_no_year = [r for r in results_no_year if str(r.get('id')) not in checked_ids][:5]
                        
                        if candidates_no_year:
                            matched_id = verify_season_in_results(candidates_no_year, source_desc=" (æ— å¹´ä»½é‡æœ)")
                            if matched_id:
                                return matched_id, 'Series', season_number_to_validate

                # 3. å¦‚æœå¾ªç¯å®Œäº†éƒ½æ²¡æ‰¾åˆ°
                logger.warning(f"  âœ éªŒè¯å¤±è´¥ï¼åœ¨ '{show_name}' çš„æ‰€æœ‰æœç´¢ç»“æœä¸­ï¼Œå‡æœªæ‰¾åˆ°ç¬¬ {season_number_to_validate} å­£ã€‚")
                    
                # ==============================================================================
                # â˜…â˜…â˜… å…œåº•å›é€€æœºåˆ¶ â˜…â˜…â˜…
                # å¦‚æœè§£æåçš„æœç´¢+éªŒè¯å¤±è´¥äº†ï¼Œå°è¯•ç”¨â€œåŸå§‹æ ‡é¢˜â€ç›´æ¥æœä¸€æ¬¡
                # ==============================================================================
                if show_name != title:
                    logger.info(f"  âœ [å…œåº•æœºåˆ¶] å°è¯•ä½¿ç”¨åŸå§‹æ ‡é¢˜ '{title}' è¿›è¡Œå›é€€æœç´¢...")
                    fallback_results = search_media(title, self.tmdb_api_key, 'Series', year=None)
                    
                    if fallback_results:
                        # å¦‚æœåŸå§‹æ ‡é¢˜èƒ½æœåˆ°ç»“æœï¼Œé€šå¸¸ç¬¬ä¸€ä¸ªå°±æ˜¯æœ€åŒ¹é…çš„
                        # è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨ï¼šè§£æå™¨é”™è¯¯åœ°æŠŠå‰§åçš„ä¸€éƒ¨åˆ†å½“æˆäº†å­£å·
                        # ä¾‹å¦‚ï¼š"Love 101" è¢«è§£ææˆäº† "Love" ç¬¬ 101 å­£ï¼ŒéªŒè¯å¤±è´¥ -> å›é€€æœ "Love 101" -> æˆåŠŸ
                        best_match = fallback_results[0]
                        logger.info(f"  âœ [å…œåº•æˆåŠŸ] åŸå§‹æ ‡é¢˜ '{title}' åŒ¹é…åˆ°äº†: {best_match.get('name')} (ID: {best_match.get('id')})")
                        return str(best_match.get('id')), 'Series', None
            
            return None
                
        return None

    def process(self, definition: Dict) -> Tuple[List[Dict[str, str]], str]:
        raw_url = definition.get('url')
        urls = []
        if isinstance(raw_url, list):
            urls = [u for u in raw_url if u]
        elif isinstance(raw_url, str) and raw_url:
            urls = [raw_url]
            
        if not urls: return [], 'empty'
        
        # â˜…â˜…â˜… ä¿®æ”¹ 1: æ”¹ä¸ºåˆ—è¡¨çš„åˆ—è¡¨ï¼Œæš‚å­˜æ¯ä¸ªæºçš„æ•°æ® â˜…â˜…â˜…
        collected_lists = [] 
        last_source_type = 'mixed'
        
        total_urls = len(urls)
        for i, url in enumerate(urls):
            temp_def = definition.copy()
            temp_def['url'] = url
            
            # è·å–å•ä¸ªæ¦œå•æ•°æ®
            items, source_type = self._process_single_url(url, temp_def)
            
            # å­˜å…¥æš‚å­˜åŒº
            collected_lists.append(items)
            last_source_type = source_type
            
            # é˜²å°æ§ä¼‘çœ 
            if isinstance(url, str) and url.startswith('maoyan://'):
                if i < total_urls - 1:
                    logger.info(f"  âœ [é˜²å°æ§] å•ä¸ªçŒ«çœ¼æ¦œå•é‡‡é›†å®Œæ¯•ï¼Œä¸ºå®‰å…¨èµ·è§ï¼Œå¼ºåˆ¶ä¼‘çœ  10 ç§’åå†é‡‡é›†ä¸‹ä¸€ä¸ª...")
                    time.sleep(10)
        
        # â˜…â˜…â˜… ä¿®æ”¹ 2: æ‰§è¡Œâ€œé›¨éœ²å‡æ²¾â€ (Round-Robin) åˆå¹¶ç®—æ³• â˜…â˜…â˜…
        all_items = []
        if collected_lists:
            # æ‰¾å‡ºæœ€é•¿çš„é‚£ä¸ªæ¦œå•çš„é•¿åº¦
            max_length = max(len(l) for l in collected_lists) if collected_lists else 0
            
            # è½®è¯¢æå–ï¼šç¬¬1è½®å–æ‰€æœ‰æ¦œå•çš„ç¬¬1ä¸ªï¼Œç¬¬2è½®å–æ‰€æœ‰æ¦œå•çš„ç¬¬2ä¸ª...
            for i in range(max_length):
                for sublist in collected_lists:
                    if i < len(sublist):
                        all_items.append(sublist[i])
            
            logger.info(f"  âœ å·²å°† {len(collected_lists)} ä¸ªæ¦œå•æºäº¤å‰åˆå¹¶ï¼Œæ€»è®¡ {len(all_items)} ä¸ªå€™é€‰é¡¹ã€‚")
            
        # ç»Ÿä¸€å»é‡
        unique_items = []
        seen_keys = set()
        for item in all_items:
            tmdb_id = item.get('id')
            item_type = item.get('type')
            title = item.get('title')
            season = item.get('season')
            
            if tmdb_id:
                key = f"{item_type}-{tmdb_id}-{season}"
            else:
                key = f"unidentified-{title}"
            
            if key not in seen_keys:
                seen_keys.add(key)
                unique_items.append(item)
        
        # ç»Ÿä¸€é™åˆ¶æ•°é‡
        limit = definition.get('limit')
        if limit and isinstance(limit, int) and limit > 0:
            unique_items = unique_items[:limit]
            
        return unique_items, last_source_type

    def _process_single_url(self, url: str, definition: Dict) -> Tuple[List[Dict[str, str]], str]:
        definition = definition.copy()
        definition['url'] = url
        source_type = 'list_rss'
        
        if not url:
            return [], source_type
            

        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ­£ï¼šåœ¨è¿™é‡Œç›´æ¥å¤„ç†çŒ«çœ¼é€»è¾‘ â˜…â˜…â˜…
        if url.startswith('maoyan://'):
            source_type = 'list_maoyan'
            logger.info(f"  âœ æ£€æµ‹åˆ°çŒ«çœ¼æ¦œå•ï¼Œå°†å¯åŠ¨å¼‚æ­¥åå°è„šæœ¬...")
            # ä½¿ç”¨ gevent å¼‚æ­¥æ‰§è¡Œè€—æ—¶çš„å­è¿›ç¨‹è°ƒç”¨
            greenlet = gevent.spawn(self._execute_maoyan_fetch, definition)
            # .get() ä¼šç­‰å¾… greenlet æ‰§è¡Œå®Œæ¯•å¹¶è¿”å›ç»“æœ
            tmdb_items = greenlet.get()
            return tmdb_items, source_type

        # --- å¯¹äºéçŒ«çœ¼æ¦œå•ï¼Œä¿æŒåŸæœ‰é€»è¾‘ä¸å˜ ---
        item_types = definition.get('item_type', ['Movie'])
        if isinstance(item_types, str): item_types = [item_types]
        limit = definition.get('limit')
        
        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 2/2: æ¥æ”¶ _get_titles_and_imdbids_from_url è¿”å›çš„ source_type â˜…â˜…â˜…
        items, source_type = self._get_titles_and_imdbids_from_url(url)
        
        if not items: return [], source_type
        
        if items and 'id' in items[0] and 'type' in items[0]:
            logger.info(f"  âœ æ£€æµ‹åˆ°æ¥è‡ªTMDbæº ({source_type}) çš„é¢„åŒ¹é…IDï¼Œå°†è·³è¿‡æ ‡é¢˜åŒ¹é…ã€‚")
            if limit and isinstance(limit, int) and limit > 0:
                items = items[:limit]
            return items, source_type # ç›´æ¥è¿”å›ç»“æœå’Œç±»å‹

        if limit and isinstance(limit, int) and limit > 0:
            items = items[:limit]
        
        tmdb_items = []
        douban_api = DoubanApi()

        with ThreadPoolExecutor(max_workers=5) as executor:
            def find_first_match(item: Dict[str, str], types_to_check):
                original_source_title = item.get('title', '').strip()
                year = item.get('year')
                rss_imdb_id = item.get('imdb_id')
                douban_link = item.get('douban_link')

                # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 1ï¼šå®šä¹‰ä¸€ä¸ªåŒ…å«åŸå§‹æ ‡é¢˜çš„è¾…åŠ©å‡½æ•° â˜…â˜…â˜…
                def create_result(tmdb_id, item_type, confirmed_season=None):
                    result = {
                        'id': tmdb_id, 
                        'type': item_type, 
                        'title': original_source_title,
                        'year': year
                    }
                    # åªæœ‰å½“ä¼ å…¥äº†æœ‰æ•ˆçš„å­£å·æ—¶ï¼Œæ‰æ·»åŠ 
                    if item_type == 'Series' and confirmed_season is not None:
                        result['season'] = confirmed_season
                    return result

                # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 2ï¼šé»˜è®¤çš„ fallback ä¹Ÿè¦å¸¦ä¸ŠåŸå§‹æ ‡é¢˜ â˜…â˜…â˜…
                fallback_result = {
                    'id': None, 
                    'type': types_to_check[0] if types_to_check else 'Movie', 
                    'title': original_source_title,
                    'year': year
                }

                # 1. å°è¯• IMDb ID
                if rss_imdb_id:
                    for item_type in types_to_check:
                        tmdb_id = self._match_by_ids(rss_imdb_id, None, item_type)
                        if tmdb_id:
                            _, s_num = parse_series_title_and_season(original_source_title, api_key=self.tmdb_api_key)
                            return create_result(tmdb_id, item_type, s_num)

                # 2. å°è¯• æ ‡é¢˜åŒ¹é…
                cleaned_title = re.sub(r'^\s*\d+\.\s*', '', original_source_title)
                cleaned_title = re.sub(r'\s*\(\d{4}\)$', '', cleaned_title).strip()
                
                for item_type in types_to_check:
                    match_result = self._match_title_to_tmdb(cleaned_title, item_type, year=year)
                    
                    if match_result:
                        tmdb_id, matched_type, matched_season = match_result
                        return create_result(tmdb_id, matched_type, matched_season)
                
                # 3. å°è¯• è±†ç“£é“¾æ¥
                if douban_link:
                    logger.info(f"  âœ ç‰‡å+å¹´ä»½åŒ¹é… '{original_source_title}' å¤±è´¥ï¼Œå¯åŠ¨å¤‡ç”¨æ–¹æ¡ˆï¼šé€šè¿‡è±†ç“£é“¾æ¥è·å–æ›´å¤šä¿¡æ¯...")
                    douban_details = douban_api.get_details_from_douban_link(douban_link, mtype=types_to_check[0] if types_to_check else None)
                    
                    if douban_details:
                        imdb_id_from_douban = douban_details.get("imdb_id")
                        if not imdb_id_from_douban and douban_details.get("attrs", {}).get("imdb"):
                            imdb_ids = douban_details["attrs"]["imdb"]
                            if isinstance(imdb_ids, list) and len(imdb_ids) > 0:
                                imdb_id_from_douban = imdb_ids[0]

                        if imdb_id_from_douban:
                            logger.info(f"  âœ è±†ç“£å¤‡ç”¨æ–¹æ¡ˆ(3a)æˆåŠŸï¼æ‹¿åˆ°IMDb ID: {imdb_id_from_douban}ï¼Œç°åœ¨ç”¨å®ƒåŒ¹é…TMDb...")
                            for item_type in types_to_check:
                                tmdb_id = self._match_by_ids(imdb_id_from_douban, None, item_type)
                                if tmdb_id:
                                    return create_result(tmdb_id, item_type)
                        
                        logger.info(f"  âœ è±†ç“£å¤‡ç”¨æ–¹æ¡ˆ(3a)å¤±è´¥ï¼Œå°è¯•æ–¹æ¡ˆ(3b): ä½¿ç”¨ original_title...")
                        original_title = douban_details.get("original_title")
                        if original_title:
                            for item_type in types_to_check:
                                match_result = self._match_title_to_tmdb(original_title, item_type, year=year)
                                if match_result:
                                    tmdb_id, matched_type, matched_season = match_result
                                    logger.info(f"  âœ è±†ç“£å¤‡ç”¨æ–¹æ¡ˆ(3b)æˆåŠŸï¼é€šè¿‡ original_title '{original_title}' åŒ¹é…æˆåŠŸã€‚")
                                    return create_result(tmdb_id, matched_type, matched_season)

                logger.debug(f"  âœ æ‰€æœ‰ä¼˜å…ˆæ–¹æ¡ˆå‡å¤±è´¥ï¼Œå°è¯•ä¸å¸¦å¹´ä»½è¿›è¡Œæœ€åçš„å›é€€æœç´¢: '{original_source_title}'")
                for item_type in types_to_check:
                    match_result = self._match_title_to_tmdb(cleaned_title, item_type, year=None)
                    if match_result:
                        tmdb_id, matched_type, matched_season = match_result
                        logger.warning(f"  âœ æ³¨æ„ï¼š'{original_source_title}' åœ¨æœ€åçš„å›é€€æœç´¢ä¸­åŒ¹é…æˆåŠŸï¼Œä½†å¹´ä»½å¯èƒ½ä¸å‡†ã€‚")
                        return create_result(tmdb_id, matched_type, matched_season)

                logger.error(f"  âœ å½»åº•å¤±è´¥ï¼šæ‰€æœ‰æ–¹æ¡ˆéƒ½æ— æ³•ä¸º '{original_source_title}' æ‰¾åˆ°åŒ¹é…é¡¹ã€‚")
                return fallback_result

            results_in_order = executor.map(lambda item: find_first_match(item, item_types), items)
            tmdb_items = [result for result in results_in_order if result is not None]
        
        douban_api.close()
        logger.info(f"  âœ RSSåŒ¹é…å®Œæˆï¼ŒæˆåŠŸè·å¾— {len(tmdb_items)} ä¸ªTMDbé¡¹ç›®ã€‚")
        
        unique_items = []
        seen_keys = set()
        
        for item in tmdb_items:
            tmdb_id = item.get('id')
            item_type = item.get('type')
            title = item.get('title')
            season = item.get('season')
            
            if tmdb_id:
                # 1. å¦‚æœæœ‰ IDï¼Œä¼˜å…ˆç”¨ ID + å­£å·å»é‡ (é˜²æ­¢åŒä¸€å‰§é›†ä¸åŒå­£è¢«å»é‡)
                # ä¾‹å¦‚: Series-12345-1, Series-12345-2
                key = f"{item_type}-{tmdb_id}-{season}"
            else:
                # 2. å¦‚æœæ²¡æœ‰ IDï¼Œå¿…é¡»ç”¨ æ ‡é¢˜ å»é‡ï¼
                # ä¾‹å¦‚: unidentified-æ€ªå¥‡ç‰©è¯­ ç¬¬äº”å­£
                # è¿™æ ·ã€Šæ€ªå¥‡ç‰©è¯­ã€‹å’Œã€Šé»‘è¢çº å¯Ÿé˜Ÿã€‹å°±ä¸ä¼šå› ä¸ºéƒ½æ˜¯ None è€Œæ‰“æ¶äº†
                key = f"unidentified-{title}"
            
            if key not in seen_keys:
                seen_keys.add(key)
                unique_items.append(item)
                
        logger.info(f"  âœ å»é‡åå‰©ä½™ {len(unique_items)} ä¸ªæœ‰æ•ˆé¡¹ç›®ã€‚")

        return unique_items, source_type

class FilterEngine:
    """
    ã€V4 - PG JSON å…¼å®¹æœ€ç»ˆç‰ˆã€‘
    - ä¿®å¤äº† _item_matches_rules æ–¹æ³•ä¸­å›  psycopg2 è‡ªåŠ¨è§£æ JSON å­—æ®µè€Œå¯¼è‡´çš„ TypeErrorã€‚
    - ç§»é™¤äº†æ‰€æœ‰å¯¹ _json å­—æ®µçš„å¤šä½™ json.loads() è°ƒç”¨ï¼Œè§£å†³äº†ç­›é€‰è§„åˆ™é™é»˜å¤±æ•ˆçš„é—®é¢˜ã€‚
    """
    def __init__(self):
        self.airing_series_ids = None
        self.series_runtime_cache = {}

    def _get_airing_ids(self): # â—€â—€â—€ å‡½æ•°åä¹Ÿæ”¹äº†
        """è¾…åŠ©å‡½æ•°ï¼Œå¸¦ç¼“å­˜åœ°è·å–è¿è½½ä¸­ID"""
        if self.airing_series_ids is None:
            logger.debug("  âœ ç­›é€‰å¼•æ“ï¼šé¦–æ¬¡éœ€è¦â€œè¿è½½ä¸­â€æ•°æ®ï¼Œæ­£åœ¨ä»æ•°æ®åº“æŸ¥è¯¢...")
            self.airing_series_ids = watchlist_db.get_airing_series_tmdb_ids()
            logger.debug(f"  âœ ç¼“å­˜äº† {len(self.airing_series_ids)} ä¸ªâ€œè¿è½½ä¸­â€å‰§é›†IDã€‚")
        return self.airing_series_ids

    def _item_matches_rules(self, item_metadata: Dict[str, Any], rules: List[Dict[str, Any]], logic: str) -> bool:
        if not rules: return True
        
        results = []
        for rule in rules:
            field, op, value = rule.get("field"), rule.get("operator"), rule.get("value")
            match = False
            
            # 1. å¤„ç†åˆ—è¡¨å­—æ®µ
            if field in ['actors', 'directors', 'genres', 'countries', 'studios', 'tags', 'keywords']:
                item_value_list = item_metadata.get(f"{field}_json")
                if not item_value_list or not isinstance(item_value_list, list):
                    results.append(False)
                    continue

                values_to_check = item_value_list
                if op == 'is_primary':
                    if field == 'actors':
                        values_to_check = item_value_list[:3] #æ¼”å‘˜åªå–å‰3
                    else:
                        values_to_check = item_value_list[:1]
                
                try:
                    if field in ['actors', 'directors']:
                        if not isinstance(value, list):
                            results.append(False); continue
                        
                        rule_person_ids = set(str(p['id']) for p in value if isinstance(p, dict) and 'id' in p)
                        if not rule_person_ids:
                            results.append(False); continue

                        item_person_ids = set()
                        for p in values_to_check:
                            person_id = p.get('tmdb_id') or p.get('id') # <-- ä¿®æ­£ç‚¹ï¼
                            if person_id is not None:
                                item_person_ids.add(str(person_id))
                        
                        if op in ['is_one_of', 'contains', 'is_primary']:
                            if not rule_person_ids.isdisjoint(item_person_ids):
                                match = True
                        elif op == 'is_none_of':
                            if rule_person_ids.isdisjoint(item_person_ids):
                                match = True
                    
                    # å¤„ç†å…¶ä»–æ™®é€šåˆ—è¡¨å­—æ®µ
                    else:
                        if op == 'is_primary':
                            if values_to_check and values_to_check[0] == value:
                                match = True
                        elif op == 'is_one_of':
                            if isinstance(value, list) and any(v in values_to_check for v in value):
                                match = True
                        elif op == 'is_none_of':
                            if isinstance(value, list) and not any(v in values_to_check for v in value):
                                match = True
                        elif op == 'contains':
                            if value in values_to_check:
                                match = True

                except (TypeError, KeyError) as e:
                    logger.warning(f"  âœ å¤„ç† {field}_json æ—¶é‡åˆ°æ„å¤–çš„æ ¼å¼é”™è¯¯: {e}, å†…å®¹: {item_value_list}")

            # 2. å¤„ç†å…¶ä»–æ‰€æœ‰éåˆ—è¡¨å­—æ®µ
            elif field in ['release_date', 'date_added']:
                item_date_val = item_metadata.get(field)
                if item_date_val and str(value).isdigit():
                    try:
                        if isinstance(item_date_val, datetime):
                            item_date = item_date_val.date()
                        elif isinstance(item_date_val, date):
                            item_date = item_date_val
                        else:
                            item_date = datetime.strptime(str(item_date_val), '%Y-%m-%d').date()

                        today = datetime.now().date()
                        days = int(value)
                        cutoff_date = today - timedelta(days=days)

                        if op == 'in_last_days':
                            if cutoff_date <= item_date <= today:
                                match = True
                        elif op == 'not_in_last_days':
                            if item_date < cutoff_date:
                                match = True
                    except (ValueError, TypeError):
                        pass

            # 3. å¤„ç†åˆ†çº§å­—æ®µ
            elif field == 'unified_rating':
                item_unified_rating = item_metadata.get('unified_rating')
                if item_unified_rating:
                    if op == 'is_one_of':
                        if isinstance(value, list) and item_unified_rating in value:
                            match = True
                    elif op == 'is_none_of':
                        if isinstance(value, list) and item_unified_rating not in value:
                            match = True
                    elif op == 'eq':
                        if str(value) == item_unified_rating:
                            match = True

            # 5. å¤„ç†è¿è½½å‰§é›†
            elif field == 'is_in_progress':
                if item_metadata.get('item_type') == 'Series':
                    airing_ids = self._get_airing_ids()
                    is_item_airing = str(item_metadata.get('tmdb_id')) in airing_ids

                    if (op == 'is' and value is True) or (op == 'is_not' and value is False):
                        if is_item_airing:
                            match = True
                    elif (op == 'is' and value is False) or (op == 'is_not' and value is True):
                        if not is_item_airing:
                            match = True

            elif field == 'title':
                item_title = item_metadata.get('title')
                if item_title and isinstance(value, str):
                    item_title_lower = item_title.lower()
                    value_lower = value.lower()
                    if op == 'contains':
                        if value_lower in item_title_lower: match = True
                    elif op == 'does_not_contain':
                        if value_lower not in item_title_lower: match = True
                    elif op == 'starts_with':
                        if item_title_lower.startswith(value_lower): match = True
                    elif op == 'ends_with':
                        if item_title_lower.endswith(value_lower): match = True
            
            # å¤„ç†æ—¶é•¿ç­›é€‰ 
            elif field == 'runtime':
                try:
                    threshold_minutes = float(value)
                    item_runtime = 0.0
                    
                    # 1. ç”µå½±å¤„ç†é€»è¾‘ (ä¸å˜)
                    if item_metadata.get('item_type') == 'Movie':
                        item_runtime = float(item_metadata.get('runtime_minutes') or 0)
                        if item_runtime <= 0:
                            assets = item_metadata.get('asset_details_json')
                            if assets and isinstance(assets, list) and len(assets) > 0:
                                item_runtime = float(assets[0].get('runtime_minutes') or 0)

                    # 2. å‰§é›†å¤„ç†é€»è¾‘ (â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ â˜…â˜…â˜…)
                    elif item_metadata.get('item_type') == 'Series':
                        tmdb_id = str(item_metadata.get('tmdb_id'))
                        
                        # A. ä¼˜å…ˆå°è¯•ä»ç¼“å­˜è·å– (ç”¨äºæ‰¹é‡ç”Ÿæˆä»»åŠ¡ï¼Œæé€Ÿ)
                        if self.series_runtime_cache and tmdb_id in self.series_runtime_cache:
                            item_runtime = self.series_runtime_cache[tmdb_id]
                        
                        # B. å¦‚æœç¼“å­˜æœªå‘½ä¸­ï¼Œè¯´æ˜æ˜¯å®æ—¶å…¥åº“åŒ¹é… (å•æ¬¡æŸ¥è¯¢ï¼Œæ€§èƒ½æ— æŸ)
                        else:
                            # è°ƒç”¨æˆ‘ä»¬åœ¨ç¬¬ä¸€æ­¥ä¸­æ·»åŠ çš„å•é¡¹æŸ¥è¯¢å‡½æ•°
                            item_runtime = media_db.get_series_average_runtime(tmdb_id)
                            logger.debug(f"    âœ [å®æ—¶ç­›é€‰] å‰§é›† {tmdb_id} å®æ—¶è®¡ç®—å¹³å‡æ—¶é•¿: {item_runtime} åˆ†é’Ÿ")
                    
                    # 3. æ‰§è¡Œæ¯”è¾ƒ (ä¸å˜)
                    if op == 'gte': 
                        if item_runtime >= threshold_minutes: match = True
                    elif op == 'lte': 
                        if item_runtime > 0 and item_runtime <= threshold_minutes: match = True
                        
                except (ValueError, TypeError):
                    pass

            else:
                actual_item_value = item_metadata.get(field)
                if actual_item_value is not None:
                    try:
                        if op == 'gte' and float(actual_item_value) >= float(value): match = True
                        elif op == 'lte' and float(actual_item_value) <= float(value): match = True
                        elif op == 'eq' and str(actual_item_value) == str(value): match = True
                    except (ValueError, TypeError): pass

            results.append(match)

        if logic.upper() == 'AND': return all(results)
        else: return any(results)

    def execute_filter(self, definition: Dict[str, Any]) -> List[Dict[str, str]]:
        logger.info("  âœ ç­›é€‰å¼•æ“ï¼šå¼€å§‹æ‰§è¡Œåˆé›†ç”Ÿæˆ...")
        rules = definition.get('rules', [])
        logic = definition.get('logic', 'AND')
        item_types_to_process = definition.get('item_type', ['Movie'])
        if isinstance(item_types_to_process, str):
            item_types_to_process = [item_types_to_process]
        if not rules:
            logger.warning("  âœ åˆé›†å®šä¹‰ä¸­æ²¡æœ‰ä»»ä½•è§„åˆ™ï¼Œå°†è¿”å›ç©ºåˆ—è¡¨ã€‚")
            return []

        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šæ ¹æ®å®šä¹‰åˆ¤æ–­æ•°æ®æº â˜…â˜…â˜…
        library_ids = definition.get('library_ids') # åœ¨æ–°ç‰ˆUIä¸­ï¼Œè¿™ä¸ªå­—æ®µå« target_library_idsï¼Œä½†æˆ‘ä»¬å…¼å®¹æ—§çš„
        if not library_ids:
            library_ids = definition.get('target_library_ids')

        all_media_metadata = []

        # æŒ‡å®šåª’ä½“åº“
        if library_ids and isinstance(library_ids, list) and len(library_ids) > 0:
            logger.info(f"  âœ å·²æŒ‡å®š {len(library_ids)} ä¸ªåª’ä½“åº“ï¼Œæ­£åœ¨ä»æœ¬åœ°æ•°æ®åº“ç­›é€‰...")
            
            # 1. è·å–ç¬¦åˆåº“æ¡ä»¶çš„ TMDB ID é›†åˆ
            tmdb_ids_in_libs = custom_collection_db.get_tmdb_ids_by_library_ids(library_ids)
            
            if not tmdb_ids_in_libs:
                logger.warning("  âœ æŒ‡å®šçš„åª’ä½“åº“ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¡ä»¶çš„åª’ä½“é¡¹ã€‚")
                return []

            # 2. æ‰¹é‡è·å–è¿™äº› ID çš„è¯¦ç»†å…ƒæ•°æ®
            # æ³¨æ„ï¼šget_media_details_by_tmdb_ids è¿”å›çš„æ˜¯ {tmdb_id: metadata} å­—å…¸
            media_metadata_map = media_db.get_media_details_by_tmdb_ids(list(tmdb_ids_in_libs))
            
            # 3. æŒ‰éœ€è¦çš„ç±»å‹è¿‡æ»¤å¹¶æ·»åŠ åˆ°æ€»åˆ—è¡¨
            for item_type in item_types_to_process:
                metadata_for_type = [
                    meta for meta in media_metadata_map.values() 
                    if meta.get('item_type') == item_type
                ]
                all_media_metadata.extend(metadata_for_type)
        # æœªæŒ‡å®šåª’ä½“åº“
        else:
            # --- åˆ†æ”¯2ï¼šä¿æŒåŸæœ‰é€»è¾‘ï¼Œæ‰«æå…¨åº“ ---
            logger.info("  âœ æœªæŒ‡å®šåª’ä½“åº“ï¼Œå°†æ‰«ææ‰€æœ‰åª’ä½“åº“çš„å…ƒæ•°æ®ç¼“å­˜...")
            for item_type in item_types_to_process:
                all_media_metadata.extend(media_db.get_all_media_metadata(item_type=item_type))

        if all_media_metadata:
            # 1. æŒ‘å‡ºæ‰€æœ‰å‰§é›†çš„ TMDB ID
            series_ids_to_fetch = [
                str(m['tmdb_id']) 
                for m in all_media_metadata 
                if m.get('item_type') == 'Series' and m.get('tmdb_id')
            ]
            
            # 2. å¦‚æœæœ¬æ¬¡ç­›é€‰åŒ…å«å‰§é›†ï¼Œä¸”è§„åˆ™é‡Œæœ‰æ—¶é•¿ç­›é€‰ï¼Œåˆ™è¿›è¡Œç²¾å‡†é¢„å–
            # (ä¸ºäº†ç®€å•ç¨³å¥ï¼Œåªè¦æœ‰å‰§é›†å°±é¢„å–ï¼Œå¼€é”€æå°)
            if series_ids_to_fetch:
                logger.info(f"  âœ æ­£åœ¨ä¸ºæœ¬æ¬¡ç­›é€‰èŒƒå›´å†…çš„ {len(series_ids_to_fetch)} éƒ¨å‰§é›†ç²¾å‡†è®¡ç®—å¹³å‡æ—¶é•¿...")
                start_time = datetime.now()
                self.series_runtime_cache = media_db.get_runtimes_for_series_list(series_ids_to_fetch)
                duration = (datetime.now() - start_time).total_seconds()
                logger.info(f"  âœ æ—¶é•¿è®¡ç®—å®Œæˆï¼Œè€—æ—¶ {duration:.3f}ç§’ã€‚")
        
        # --- åç»­çš„ç­›é€‰é€»è¾‘ä¿æŒä¸å˜ ---
        matched_items = []
        if not all_media_metadata:
            logger.warning("  âœ æœªèƒ½åŠ è½½ä»»ä½•åª’ä½“å…ƒæ•°æ®è¿›è¡Œç­›é€‰ã€‚")
            return []
        
        logger.info(f"  âœ å·²åŠ è½½ {len(all_media_metadata)} æ¡å…ƒæ•°æ®ï¼Œå¼€å§‹åº”ç”¨ç­›é€‰è§„åˆ™...")
        for media_metadata in all_media_metadata:
            if self._item_matches_rules(media_metadata, rules, logic):
                tmdb_id = media_metadata.get('tmdb_id')
                item_type = media_metadata.get('item_type')
                if tmdb_id and item_type:
                    emby_ids = media_metadata.get('emby_item_ids_json')
                    first_emby_id = emby_ids[0] if emby_ids and isinstance(emby_ids, list) else None
                    
                    matched_items.append({
                        'id': str(tmdb_id), 
                        'type': item_type,
                        'emby_id': first_emby_id 
                    })
                    
        unique_items = list({f"{item['type']}-{item['id']}": item for item in matched_items}.values())
        logger.info(f"  âœ ç­›é€‰å®Œæˆï¼å…±æ‰¾åˆ° {len(unique_items)} éƒ¨åŒ¹é…çš„åª’ä½“é¡¹ç›®ã€‚")
        return unique_items
    
    def find_matching_collections(self, item_metadata: Dict[str, Any], media_library_id: Optional[str] = None) -> List[Dict[str, Any]]:
        media_item_type = item_metadata.get('item_type')
        media_type_cn = "å‰§é›†" if media_item_type == "Series" else "å½±ç‰‡"
        logger.info(f"  âœ æ­£åœ¨ä¸º{media_type_cn}ã€Š{item_metadata.get('title')}ã€‹å®æ—¶åŒ¹é…è‡ªå®šä¹‰åˆé›†...")
        matched_collections = []
        all_filter_collections = [
            c for c in custom_collection_db.get_all_custom_collections() 
            if c['type'] == 'filter' and c['status'] == 'active' and c['emby_collection_id']
        ]
        if not all_filter_collections:
            logger.debug("  âœ æ²¡æœ‰å‘ç°ä»»ä½•å·²å¯ç”¨çš„ç­›é€‰ç±»åˆé›†ï¼Œè·³è¿‡åŒ¹é…ã€‚")
            return []
        for collection_def in all_filter_collections:
            try:
                definition = collection_def['definition_json']
                defined_library_ids = definition.get('library_ids')
                if defined_library_ids and media_library_id and media_library_id not in defined_library_ids:
                    logger.debug(f"  âœ è·³è¿‡åˆé›†ã€Š{collection_def['name']}ã€‹ï¼Œå› ä¸ºåª’ä½“åº“ä¸åŒ¹é… (åˆé›†è¦æ±‚: {defined_library_ids}, å®é™…æ¥è‡ª: '{media_library_id}')ã€‚")
                    continue 
                collection_item_types = definition.get('item_type', ['Movie'])
                if isinstance(collection_item_types, str):
                    collection_item_types = [collection_item_types]
                if media_item_type not in collection_item_types:
                    logger.debug(f"  âœ è·³è¿‡åˆé›†ã€Š{collection_def['name']}ã€‹ï¼Œå› ä¸ºå†…å®¹ç±»å‹ä¸åŒ¹é… (åˆé›†éœ€è¦: {collection_item_types}, å®é™…æ˜¯: '{media_item_type}')ã€‚")
                    continue
                rules = definition.get('rules', [])
                logic = definition.get('logic', 'AND')
                if self._item_matches_rules(item_metadata, rules, logic):
                    logger.info(f"  âœ åŒ¹é…æˆåŠŸï¼{media_type_cn}ã€Š{item_metadata.get('title')}ã€‹å±äºåˆé›†ã€Š{collection_def['name']}ã€‹ã€‚")
                    matched_collections.append({
                        'id': collection_def['id'],
                        'name': collection_def['name'],
                        'emby_collection_id': collection_def['emby_collection_id']
                    })
            except TypeError as e:
                logger.warning(f"  âœ è§£æåˆé›†ã€Š{collection_def['name']}ã€‹çš„å®šä¹‰æ—¶å‡ºé”™: {e}ï¼Œè·³è¿‡ã€‚")
                continue
        return matched_collections
    
class RecommendationEngine:
    """
    ã€AI æ¨èå¼•æ“ (åŒæ¨¡ç‰ˆ)ã€‘
    æ¨¡å¼ A (LLM): åŸºäºå¤§æ¨¡å‹çŸ¥è¯†åº“æ¨è (é€‚åˆå‘ç°æ–°ç‰‡)ã€‚
    æ¨¡å¼ B (Vector): åŸºäºæœ¬åœ°æ•°æ®åº“å‘é‡ç›¸ä¼¼åº¦æ¨è (é€‚åˆç²¾å‡†åŒ¹é…å£å‘³)ã€‚
    """
    # å®šä¹‰ç±»çº§åˆ«çš„ç¼“å­˜å˜é‡ (æ‰€æœ‰å®ä¾‹å…±äº«) 
    _cache_matrix = None
    _cache_ids = None
    _cache_titles = None
    _cache_types = None
    # åˆ·æ–°é—´éš” (ç§’) - æ¯”å¦‚ 4 å°æ—¶
    _REFRESH_INTERVAL = 14400
    _is_refreshing_loop_running = False 

    def __init__(self, tmdb_api_key: str):
        self.tmdb_api_key = tmdb_api_key
        self.list_importer = ListImporter(tmdb_api_key) # å¤ç”¨æœç´¢åŒ¹é…é€»è¾‘

    @classmethod
    def refresh_cache(cls):
        """
        ã€ç±»æ–¹æ³•ã€‘å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ (æ‰§è¡Œæ•°æ®åº“è¯»å–å’ŒçŸ©é˜µæ„å»º)
        """
        logger.info("  ğŸ”„ [å‘é‡å¼•æ“] å¼€å§‹åå°åˆ·æ–°å‘é‡ç¼“å­˜...")
        start_t = time.time()
        
        try:
            # å®ä¾‹åŒ–ä¸€ä¸ªä¸´æ—¶çš„å¯¹è±¡æ¥è°ƒç”¨ _load_vectors_from_db (æˆ–è€…æŠŠé‚£ä¸ªå‡½æ•°æ”¹æˆé™æ€æ–¹æ³•ä¹Ÿå¯ä»¥)
            # è¿™é‡Œä¸ºäº†å¤ç”¨ä»£ç ï¼Œæˆ‘ä»¬æŠŠ _load_vectors_from_db çš„é€»è¾‘æ¬è¿‡æ¥æˆ–è€…åšæˆé™æ€çš„
            # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ç›´æ¥åœ¨è¿™é‡Œå†™é€»è¾‘ï¼Œæˆ–è€…å‡è®¾ _load_vectors_from_db å˜æˆäº† @staticmethod
            
            # --- ä¸‹é¢æ˜¯åŠ è½½é€»è¾‘çš„å¤åˆ» ---
            with connection.get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT tmdb_id, title, item_type, overview_embedding 
                    FROM media_metadata 
                    WHERE overview_embedding IS NOT NULL
                      AND item_type IN ('Movie', 'Series')
                      AND in_library = TRUE
                """)
                all_data = cursor.fetchall()
            
            if not all_data:
                logger.warning("  âš ï¸ [å‘é‡å¼•æ“] æ•°æ®åº“ä¸ºç©ºï¼Œæ— æ³•åˆ·æ–°ç¼“å­˜ã€‚")
                return

            ids, vectors, titles, types = [], [], [], []
            for row in all_data:
                vec = row.get('overview_embedding')
                if vec and len(vec) > 0:
                    ids.append(str(row['tmdb_id']))
                    titles.append(row['title'])
                    types.append(row['item_type'])
                    vectors.append(np.array(vec, dtype=np.float32))
            
            if not vectors:
                return

            matrix = np.stack(vectors)
            norm = np.linalg.norm(matrix, axis=1, keepdims=True)
            matrix = matrix / (norm + 1e-10)
            # ---------------------------

            # â˜…â˜…â˜… åŸå­æ›¿æ¢ï¼šè¿™ä¸€ç¬é—´å®Œæˆæ›´æ–°ï¼Œä¸ä¼šé˜»å¡è¯»æ“ä½œ â˜…â˜…â˜…
            cls._cache_matrix = matrix
            cls._cache_ids = ids
            cls._cache_titles = titles
            cls._cache_types = types
            
            logger.info(f"  âœ… [å‘é‡å¼•æ“] ç¼“å­˜åˆ·æ–°å®Œæˆã€‚å…± {len(ids)} æ¡ï¼Œè€—æ—¶ {time.time() - start_t:.2f}sã€‚")

        except Exception as e:
            logger.error(f"  âŒ [å‘é‡å¼•æ“] åˆ·æ–°ç¼“å­˜å¤±è´¥: {e}", exc_info=True)

    @classmethod
    def start_auto_refresh_loop(cls):
        """
        ã€ç±»æ–¹æ³•ã€‘å¯åŠ¨è‡ªåŠ¨åˆ·æ–°å¾ªç¯
        """
        if cls._is_refreshing_loop_running:
            return
        
        cls._is_refreshing_loop_running = True
        
        def loop():
            logger.info("  ğŸš€ [å‘é‡å¼•æ“] è‡ªåŠ¨åˆ·æ–°å®ˆæŠ¤çº¿ç¨‹å·²å¯åŠ¨ã€‚")
            # ç¬¬ä¸€æ¬¡ç«‹å³æ‰§è¡Œ
            cls.refresh_cache()
            
            while True:
                # ä¼‘çœ æŒ‡å®šé—´éš”
                gevent.sleep(cls._REFRESH_INTERVAL)
                # é†’æ¥åˆ·æ–°
                cls.refresh_cache()
        
        gevent.spawn(loop)

    def _get_vector_data(self):
        """
        ã€å†…éƒ¨æ–¹æ³•ã€‘è·å–å‘é‡æ•°æ® (æé€Ÿç‰ˆ)
        """
        # å¦‚æœç¼“å­˜è¿˜æ²¡åˆå§‹åŒ–ï¼ˆæ¯”å¦‚åˆšå¯åŠ¨è¿˜æ²¡è·‘å®Œç¬¬ä¸€æ¬¡åˆ·æ–°ï¼‰ï¼Œå°±é˜»å¡åŠ è½½ä¸€æ¬¡
        if RecommendationEngine._cache_matrix is None:
            RecommendationEngine.refresh_cache()
            
        # ç›´æ¥è¿”å›ï¼Œä¸å†åˆ¤æ–­æ—¶é—´ï¼Œå®Œå…¨ä¿¡ä»»åå°çº¿ç¨‹
        return (RecommendationEngine._cache_matrix, 
                RecommendationEngine._cache_ids, 
                RecommendationEngine._cache_titles, 
                RecommendationEngine._cache_types)

    def _vector_search(self, user_history_items: List[Dict], exclusion_ids: set = None, limit: int = 10, allowed_types: List[str] = None) -> List[Dict]:
        """
        ã€å†…éƒ¨æ–¹æ³•ã€‘åŸºäºå‘é‡ç›¸ä¼¼åº¦æœç´¢æœ¬åœ°æ•°æ®åº“ã€‚
        å¢åŠ  allowed_types å‚æ•°è¿›è¡Œç±»å‹è¿‡æ»¤ã€‚
        """
        if exclusion_ids is None: exclusion_ids = set()
        # é»˜è®¤å…è®¸æ‰€æœ‰ï¼Œé˜²æ­¢æŠ¥é”™
        if not allowed_types: allowed_types = ['Movie', 'Series']
        if exclusion_ids is None:
            exclusion_ids = set()

        # 1. æå–å†å²è®°å½• ID (ç”¨äºç”»åƒè®¡ç®—)
        history_tmdb_ids = set()
        history_titles = set()
        for item in user_history_items:
            if isinstance(item, dict):
                if item.get('tmdb_id'): history_tmdb_ids.add(str(item.get('tmdb_id')))
                if item.get('title'): history_titles.add(item.get('title'))
            elif isinstance(item, str):
                history_titles.add(item)

        if not history_tmdb_ids and not history_titles:
            return []

        # â˜…â˜…â˜… 2. è·å–çŸ©é˜µæ•°æ® (èµ°ç¼“å­˜) â˜…â˜…â˜…
        matrix, ids, titles, types = self._get_vector_data()
        
        if matrix is None:
            logger.warning("  âœ [å‘é‡æœç´¢] æ— æ³•è·å–å‘é‡æ•°æ® (æ•°æ®åº“ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥)ã€‚")
            return []

        try:
            # 3. å®šä½ç”¨æˆ·å£å‘³ (User Profile)
            # è¿™é‡Œéœ€è¦åœ¨å†…å­˜é‡Œå¿«é€Ÿæ‰¾åˆ°ç”¨æˆ·çœ‹è¿‡çš„ç‰‡å­å¯¹åº”çš„å‘é‡
            # ä¸ºäº†é€Ÿåº¦ï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹ä¸€ä¸ªä¸´æ—¶çš„ id -> index æ˜ å°„ï¼Œæˆ–è€…ç›´æ¥éå†
            # ç”±äº ids åˆ—è¡¨é€šå¸¸å‡ åƒå‡ ä¸‡æ¡ï¼Œéå†ä¹Ÿå¾ˆå¿«
            
            user_vectors = []
            
            # ä¼˜åŒ–ï¼šä½¿ç”¨ set åŠ é€ŸæŸ¥æ‰¾
            # æ³¨æ„ï¼šids å’Œ matrix æ˜¯å¯¹åº”çš„
            for idx, db_tmdb_id in enumerate(ids):
                is_match = False
                if db_tmdb_id in history_tmdb_ids:
                    is_match = True
                elif titles[idx] and any(h_t in titles[idx] for h_t in history_titles):
                    is_match = True
                
                if is_match:
                    user_vectors.append(matrix[idx])
            
            if not user_vectors:
                logger.warning(f"  âœ [å‘é‡æœç´¢] åŒ¹é…å¤±è´¥ï¼šç”¨æˆ·çš„å†å²è®°å½•æœªåœ¨å‘é‡åº“ä¸­æ‰¾åˆ°å¯¹åº”æ•°æ®ã€‚")
                return []
            
            # è®¡ç®—ç”¨æˆ·å¹³å‡å‘é‡
            user_profile_vector = np.mean(user_vectors, axis=0)
            user_profile_vector = user_profile_vector / (np.linalg.norm(user_profile_vector) + 1e-10)

            # 4. è®¡ç®—ç›¸ä¼¼åº¦ (æé€Ÿ)
            scores = np.dot(matrix, user_profile_vector)
            top_indices = np.argsort(scores)[::-1]
            
            results = []
            count = 0
            
            # 5. æå–ç»“æœ
            for idx in top_indices:
                # è¿™é‡Œçš„ limit ä¾ç„¶æ”¾å®½ï¼Œä¿è¯åç»­æœ‰è¶³å¤Ÿçš„éšæœºç©ºé—´
                if count >= max(limit, 200): break
                
                # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ 1ï¼šåœ¨æºå¤´è¿‡æ»¤ç±»å‹ â˜…â˜…â˜…
                # å¦‚æœå½“å‰æ¡ç›®çš„ç±»å‹ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼Œç›´æ¥è·³è¿‡
                if types[idx] not in allowed_types:
                    continue

                score = float(scores[idx])
                if score < 0.45: break 
                if score > 0.999: continue 
                
                current_id = ids[idx]
                
                if current_id in exclusion_ids: continue
                if current_id in history_tmdb_ids: continue
                if any(h_t in titles[idx] for h_t in history_titles): continue

                results.append({
                    'id': current_id,
                    'type': types[idx],
                    'title': titles[idx],
                    'score': score
                })
                count += 1
                
            return results

        except Exception as e:
            logger.error(f"  âœ [å‘é‡æœç´¢] è®¡ç®—è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return []
        
    def generate_user_vector(self, user_id: str, limit: int = 50, allowed_types: List[str] = None) -> List[Dict]:
        """
        åªä½¿ç”¨å‘é‡æœç´¢ï¼Œé€Ÿåº¦å¿«ï¼Œé€‚åˆå®æ—¶ç”Ÿæˆã€‚
        """
        logger.debug(f"  âœ [ä¸ªäººå‘é‡æ¨è] æ­£åœ¨ä¸ºç”¨æˆ· {user_id} å®æ—¶è®¡ç®—...")
        
        # 1. è·å–ç”¨æˆ·å†å²
        # è·å–ç”¨æˆ·çœ‹è¿‡çš„ã€å–œæ¬¢çš„ (ç”¨äºè®¡ç®—å£å‘³)
        context_history_items = media_db.get_user_positive_history(user_id, limit=50)
        if not context_history_items:
            logger.warning(f"  âœ ç”¨æˆ· {user_id} å†å²è®°å½•ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆå‘é‡æ¨èã€‚")
            return []

        # è·å–ç”¨æˆ·æ‰€æœ‰äº¤äº’è¿‡çš„ (ç”¨äºå»é‡ï¼Œçœ‹è¿‡çš„å°±ä¸æ¨äº†)
        watched_tmdb_ids = set()
        full_interaction = media_db.get_user_all_interacted_history(user_id)
        for item in full_interaction:
            if item.get('tmdb_id'):
                watched_tmdb_ids.add(str(item.get('tmdb_id')))

        # 2. æ‰§è¡Œå‘é‡æœç´¢
        results = self._vector_search(
            user_history_items=context_history_items,
            exclusion_ids=watched_tmdb_ids,
            limit=limit,
            allowed_types=allowed_types 
        )
        
        return results

    def generate(self, definition: Dict) -> List[Dict[str, str]]:
        """
        æ¨èç”Ÿæˆå™¨ã€‚
        """
        ai_prompt = definition.get('ai_prompt')
        limit = definition.get('limit', 20)
        discovery_ratio = float(definition.get('ai_discovery_ratio', 0.2))
        allowed_types = definition.get('item_type', ['Movie', 'Series'])

        logger.debug("  âœ [æ™ºèƒ½æ¨è] å¯åŠ¨ (LLM + å‘é‡æ··åˆæ¨¡å¼)...")

        # 1. è·å–å…¨ç«™çƒ­åº¦æ•°æ®ä½œä¸ºä¸Šä¸‹æ–‡
        context_history_items = media_db.get_global_popular_items(limit=20)
        if not context_history_items:
            logger.warning("  âœ å…¨ç«™æ’­æ”¾æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆå…¨å±€æ¨èã€‚")
            return []

        # å…¨å±€æ¨¡å¼ä¸‹ï¼Œå»é‡é›†åˆåªåŒ…å«ç§å­æ•°æ®æœ¬èº«
        watched_tmdb_ids = set()
        for item in context_history_items:
            if item.get('tmdb_id'):
                watched_tmdb_ids.add(str(item.get('tmdb_id')))

        final_items_map = {}

        # 2. LLM æ¨èéƒ¨åˆ† (ç”¨äºå‘ç°æ–°ç‰‡)
        logger.info(f"  âœ [æ™ºèƒ½æ¨è] æ­£åœ¨è°ƒç”¨ LLM åˆ†æå…¨ç«™å£å‘³...")
        history_titles_for_llm = []
        for item in context_history_items:
            title = item.get('title')
            year = item.get('release_year')
            if year:
                history_titles_for_llm.append(f"{title} ({year})")
            else:
                history_titles_for_llm.append(title)

        try:
            translator = AITranslator(config_manager.APP_CONFIG)
            request_limit = int(limit * discovery_ratio)
            request_limit = max(request_limit, 2) # è‡³å°‘æ¨2ä¸ª

            # æç¤ºè¯å¾®è°ƒ
            system_prompt = "ä½ æ˜¯èµ„æ·±é€‰ç‰‡äººã€‚åŸºäºä»¥ä¸‹å¤§ä¼—å–œæ¬¢çš„å½±ç‰‡ï¼Œæ¨èåŒç±»é«˜åˆ†ä½œå“ã€‚ä¸è¦æ¨èåˆ—è¡¨ä¸­å·²æœ‰çš„ã€‚"
            if ai_prompt:
                system_prompt += f" é¢å¤–è¦æ±‚: {ai_prompt}"

            llm_recommendations = translator.get_recommendations(
                user_history=history_titles_for_llm, 
                user_instruction=system_prompt,
                allowed_types=allowed_types 
            )
                    
            if llm_recommendations:
                logger.info(f"  âœ [æ™ºèƒ½æ¨è] LLM è¿”å›äº† {len(llm_recommendations)} éƒ¨ä½œå“ï¼Œæ­£åœ¨åŒ¹é… TMDb ID...")
                with ThreadPoolExecutor(max_workers=5) as executor:
                    def resolve_item(rec_item):
                        try:
                            title = ""
                            original_title = ""
                            year = None
                            primary_type = 'Movie' 
                            
                            if isinstance(rec_item, dict):
                                title = rec_item.get('title')
                                original_title = rec_item.get('original_title')
                                year = str(rec_item.get('year')) if rec_item.get('year') else None
                                if rec_item.get('type'):
                                    primary_type = rec_item.get('type')
                            elif isinstance(rec_item, str):
                                title = rec_item
                            
                            if not title: return None

                            # æœç´¢ç­–ç•¥ï¼šå¦‚æœç”¨æˆ·åªé€‰äº† Movieï¼Œå°±åªæœ Movieï¼Œä¸è¿›è¡Œåå‘æœç´¢
                            # å¦‚æœç”¨æˆ·åªé€‰äº† Seriesï¼Œå°±åªæœ Series
                            # å¦‚æœéƒ½é€‰äº†ï¼Œæ‰è¿›è¡Œè·¨ç±»å‹æœç´¢
                            
                            search_types = []
                            if 'Movie' in allowed_types and 'Series' in allowed_types:
                                search_types = [primary_type, 'Series' if primary_type == 'Movie' else 'Movie']
                            elif 'Movie' in allowed_types:
                                search_types = ['Movie']
                            elif 'Series' in allowed_types:
                                search_types = ['Series']
                            
                            match_result = None
                            
                            # å®šä¹‰è¾…åŠ©æœç´¢å‡½æ•°
                            def has_chinese(text):
                                return any('\u4e00' <= char <= '\u9fff' for char in str(text))
                            search_query = original_title if original_title else title
                            if has_chinese(title): search_query = title

                            # éå†å…è®¸çš„ç±»å‹è¿›è¡Œæœç´¢
                            for try_type in search_types:
                                # 1. æœåŸå/æ™ºèƒ½å
                                match_result = self.list_importer._match_title_to_tmdb(search_query, try_type, year)
                                if match_result: break
                                
                                # 2. æœä¸­æ–‡å (å¦‚æœä¸ä¸€æ ·)
                                if search_query != title:
                                    match_result = self.list_importer._match_title_to_tmdb(title, try_type, year)
                                    if match_result: break

                            if match_result:
                                tmdb_id, matched_type, season_num = match_result
                                tmdb_id = str(tmdb_id)
                                
                                # â˜…â˜…â˜… äºŒæ¬¡æ ¡éªŒï¼šç¡®ä¿åŒ¹é…åˆ°çš„ç±»å‹åœ¨å…è®¸èŒƒå›´å†… â˜…â˜…â˜…
                                if matched_type not in allowed_types:
                                    return None

                                if tmdb_id in watched_tmdb_ids:
                                    return None
                                return {
                                    'id': tmdb_id,
                                    'type': matched_type,
                                    'title': title, 
                                    'season': season_num,
                                    'release_date': None 
                                }
                            return None
                        except Exception:
                            return None

                    results = executor.map(resolve_item, llm_recommendations)
                    for res in results:
                        if res:
                            final_items_map[res['id']] = res
            else:
                logger.info("  âœ [æ™ºèƒ½æ¨è] ç”¨æˆ·è®¾ç½®æ¢ç´¢æ¯”ä¾‹ä¸º 0%...")
        except Exception as e:
            logger.error(f"  âœ [æ™ºèƒ½æ¨è] LLM è°ƒç”¨å¤±è´¥: {e}")

        # 3. å‘é‡æ¨èéƒ¨åˆ† (ç”¨äºå¡«å……å‰©ä½™åé¢)
        if len(final_items_map) < limit:
            needed = limit - len(final_items_map)
            logger.info(f"  âœ [æ™ºèƒ½æ¨è] å¯ç”¨å‘é‡å¼•æ“è¡¥å…… {needed} éƒ¨ç›¸ä¼¼å½±ç‰‡...")
            vector_results = self._vector_search(
                user_history_items=context_history_items, 
                exclusion_ids=watched_tmdb_ids, 
                limit=needed + 10
            )
            for v in vector_results:
                if v['type'] in allowed_types and v['id'] not in final_items_map:
                    final_items_map[v['id']] = {
                        'id': v['id'], 'type': v['type'], 'title': v['title']
                    }

        final_items = list(final_items_map.values())[:limit]
        logger.info(f"  âœ [æ™ºèƒ½æ¨è] å®Œæˆï¼Œç”Ÿæˆ {len(final_items)} éƒ¨å½±ç‰‡ã€‚")
        return final_items