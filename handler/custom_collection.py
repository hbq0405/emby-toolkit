# handler/custom_collection.py
import logging
import requests
import xml.etree.ElementTree as ET
import re
import os
import time
import gevent
import numpy as np
import sys
from typing import List, Dict, Any, Optional, Tuple
import json
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from bs4 import BeautifulSoup
from gevent import subprocess, Timeout

import handler.tmdb as tmdb
import config_manager
from tasks.helpers import parse_series_title_and_season
from database import media_db, connection
from handler.douban import DoubanApi
from handler.tmdb import search_media
from ai_translator import AITranslator

logger = logging.getLogger(__name__)


class ListImporter:
    """
    (V9.1 - æœ€ç»ˆå¼‚æ­¥ç‰ˆ)
    è´Ÿè´£å¤„ç†å¤–éƒ¨æ¦œå•æº (RSS, TMDb List, Douban Doulist, Maoyan, etc.)
    """
    
    SEASON_PATTERN = re.compile(r'(.*?)\s*[ï¼ˆ(]?\s*(ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+)\s*å­£\s*[)ï¼‰]?')
    
    CHINESE_NUM_MAP = {
        'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
        'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15, 'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18, 'åä¹': 19, 'äºŒå': 20,
        'ç¬¬ä¸€': 1, 'ç¬¬äºŒ': 2, 'ç¬¬ä¸‰': 3, 'ç¬¬å››': 4, 'ç¬¬äº”': 5, 'ç¬¬å…­': 6, 'ç¬¬ä¸ƒ': 7, 'ç¬¬å…«': 8, 'ç¬¬ä¹': 9, 'ç¬¬å': 10,
        'ç¬¬åä¸€': 11, 'ç¬¬åäºŒ': 12, 'ç¬¬åä¸‰': 13, 'ç¬¬åå››': 14, 'ç¬¬åäº”': 15, 'ç¬¬åå…­': 16, 'ç¬¬åä¸ƒ': 17, 'ç¬¬åå…«': 18, 'ç¬¬åä¹': 19, 'ç¬¬äºŒå': 20
    }
    VALID_MAOYAN_PLATFORMS = {'tencent', 'iqiyi', 'youku', 'mango'}

    def __init__(self, tmdb_api_key: str):
        self.tmdb_api_key = tmdb_api_key
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})

    def _execute_maoyan_fetch(self, definition: Dict) -> List[Dict[str, str]]:
        maoyan_url = definition.get('url', '')
        temp_output_file = os.path.join(config_manager.PERSISTENT_DATA_PATH, f"maoyan_temp_output_{hash(maoyan_url)}.json")
        
        content_key = maoyan_url.replace('maoyan://', '')
        parts = content_key.split('-')
        
        platform = 'all'
        if len(parts) > 1 and parts[-1] in self.VALID_MAOYAN_PLATFORMS:
            platform = parts[-1]
            type_part = '-'.join(parts[:-1])
        else:
            type_part = content_key

        types_to_fetch = [t.strip() for t in type_part.split(',') if t.strip()]
        
        if not types_to_fetch:
            logger.error(f"  âœ æ— æ³•ä»çŒ«çœ¼URL '{maoyan_url}' ä¸­è§£æå‡ºæœ‰æ•ˆçš„ç±»å‹ã€‚")
            return []
            
        limit = definition.get('limit')
        if not limit:
            limit = 50

        fetcher_script_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'maoyan_fetcher.py')
        if not os.path.exists(fetcher_script_path):
            logger.error(f"  âœ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•æ‰¾åˆ°çŒ«çœ¼è·å–è„šæœ¬ '{fetcher_script_path}'ã€‚")
            return []

        command = [
            sys.executable,
            fetcher_script_path,
            '--api-key', self.tmdb_api_key,
            '--output-file', temp_output_file,
            '--num', str(limit),
            '--platform', platform,
            '--types', *types_to_fetch
        ]
        
        try:
            logger.debug(f"  âœ (åœ¨ä¸€ä¸ªç‹¬ç«‹çš„ Greenlet ä¸­) æ‰§è¡Œå‘½ä»¤: {' '.join(command)}")
            
            result_bytes = subprocess.check_output(
                command, 
                stderr=subprocess.STDOUT, 
                timeout=600
            )
            
            result_output = result_bytes.decode('utf-8', errors='ignore')
            logger.info("  âœ çŒ«çœ¼è·å–è„šæœ¬æˆåŠŸå®Œæˆã€‚")
            if result_output:
                logger.debug(f"  âœ è„šæœ¬è¾“å‡º:\n{result_output}")
            
            with open(temp_output_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            return results

        except Timeout:
            logger.error("  âœ æ‰§è¡ŒçŒ«çœ¼è·å–è„šæœ¬è¶…æ—¶ï¼ˆè¶…è¿‡10åˆ†é’Ÿï¼‰ã€‚")
            return []
        except subprocess.CalledProcessError as e:
            error_output = e.output.decode('utf-8', errors='ignore') if e.output else "No output captured."
            logger.error(f"  âœ æ‰§è¡ŒçŒ«çœ¼è·å–è„šæœ¬å¤±è´¥ã€‚è¿”å›ç : {e.returncode}")
            logger.error(f"  âœ è„šæœ¬çš„å®Œæ•´é”™è¯¯è¾“å‡º:\n{error_output}")
            return []
        except Exception as e:
            logger.error(f"  âœ å¤„ç†çŒ«çœ¼æ¦œå•æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return []
        finally:
            if os.path.exists(temp_output_file):
                os.remove(temp_output_file)

    def _match_by_ids(self, imdb_id: Optional[str], tmdb_id: Optional[str], item_type: str) -> Optional[str]:
        if tmdb_id:
            logger.debug(f"  âœ é€šè¿‡TMDb IDç›´æ¥åŒ¹é…ï¼š{tmdb_id}")
            return tmdb_id
        if imdb_id:
            logger.debug(f"  âœ é€šè¿‡IMDb IDæŸ¥æ‰¾TMDb IDï¼š{imdb_id}")
            try:
                tmdb_id_from_imdb = tmdb.get_tmdb_id_by_imdb_id(imdb_id, self.tmdb_api_key, item_type)
                if tmdb_id_from_imdb:
                    logger.debug(f"  âœ IMDb ID {imdb_id} å¯¹åº” TMDb ID: {tmdb_id_from_imdb}")
                    return str(tmdb_id_from_imdb)
                else:
                    logger.warning(f"  âœ æ— æ³•é€šè¿‡IMDb ID {imdb_id} æŸ¥æ‰¾åˆ°å¯¹åº”çš„TMDb IDã€‚")
            except Exception as e:
                logger.error(f"  âœ é€šè¿‡IMDb IDæŸ¥æ‰¾TMDb IDæ—¶å‡ºé”™: {e}")
        return None
    
    def _extract_ids_from_title_or_line(self, title_line: str) -> Tuple[Optional[str], Optional[str]]:
        imdb_id = None
        tmdb_id = None
        imdb_match = re.search(r'(tt\d{7,8})', title_line, re.I)
        if imdb_match:
            imdb_id = imdb_match.group(1)
        tmdb_match = re.search(r'tmdb://(\d+)', title_line, re.I)
        if tmdb_match:
            tmdb_id = tmdb_match.group(1)
        return imdb_id, tmdb_id
    
    def _get_items_from_douban_doulist(self, url: str) -> List[Dict[str, str]]:
        """ä¸“é—¨ç”¨äºè§£æå’Œåˆ†é¡µè·å–è±†ç“£è±†åˆ—å†…å®¹çš„å‡½æ•°"""
        all_items = []
        base_url = url.split('?')[0]
        page_start = 0
        max_pages = 50 
        items_per_page = 25

        logger.info(f"  âœ æ£€æµ‹åˆ°è±†ç“£è±†åˆ—é“¾æ¥ï¼Œå¼€å§‹åˆ†é¡µè·å–: {base_url}")

        for page in range(max_pages):
            current_start = page * items_per_page
            paginated_url = f"{base_url}?start={current_start}&sort=seq&playable=0&sub_type="
            
            try:
                logger.debug(f"    âœ æ­£åœ¨è·å–ç¬¬ {page + 1} é¡µ: {paginated_url}")
                response = self.session.get(paginated_url, timeout=20)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'lxml')
                doulist_items = soup.find_all('div', class_='doulist-item')

                if not doulist_items:
                    logger.info(f"  âœ åœ¨ç¬¬ {page + 1} é¡µæœªå‘ç°æ›´å¤šé¡¹ç›®ï¼Œè·å–ç»“æŸã€‚")
                    break

                for item in doulist_items:
                    title_div = item.find('div', class_='title')
                    if not title_div: continue
                    
                    link_tag = title_div.find('a')
                    if not link_tag: continue
                    
                    title = link_tag.get_text(strip=True)
                    douban_link = link_tag.get('href')
                    
                    year = None
                    abstract_div = item.find('div', class_='abstract')
                    if abstract_div:
                        year_match = re.search(r'\b(19\d{2}|20\d{2})\b', abstract_div.get_text())
                        if year_match:
                            year = year_match.group(1)
                    
                    if title:
                        all_items.append({
                            'title': title,
                            'imdb_id': None,
                            'year': year,
                            'douban_link': douban_link
                        })

            except Exception as e:
                logger.error(f"  âœ è·å–æˆ–è§£æè±†ç“£è±†åˆ—é¡µé¢ '{paginated_url}' æ—¶å‡ºé”™: {e}")
                break
        
        logger.info(f"  âœ è±†ç“£è±†åˆ—è·å–å®Œæˆï¼Œä» {page} ä¸ªé¡µé¢ä¸­æ€»å…±è§£æå‡º {len(all_items)} ä¸ªé¡¹ç›®ã€‚")
        return all_items
    
    def _get_items_from_tmdb_list(self, url: str) -> List[Dict[str, str]]:
        """ä¸“é—¨ç”¨äºè§£æå’Œåˆ†é¡µè·å–TMDbç‰‡å•å†…å®¹çš„å‡½æ•°"""
        match = re.search(r'themoviedb\.org/list/(\d+)', url)
        if not match:
            logger.error(f"  âœ æ— æ³•ä»URL '{url}' ä¸­è§£æå‡ºTMDbç‰‡å•IDã€‚")
            return []

        list_id = int(match.group(1))
        all_items = []
        current_page = 1
        total_pages = 1

        logger.info(f"  âœ æ£€æµ‹åˆ°TMDbç‰‡å•é“¾æ¥ï¼Œå¼€å§‹åˆ†é¡µè·å–: {url}")

        while current_page <= total_pages:
            try:
                logger.debug(f"    âœ æ­£åœ¨è·å–ç¬¬ {current_page} / {total_pages} é¡µ...")
                list_data = tmdb.get_list_details_tmdb(list_id, self.tmdb_api_key, page=current_page)

                if not list_data or not list_data.get('items'):
                    logger.warning(f"  âœ åœ¨ç¬¬ {current_page} é¡µæœªå‘ç°æ›´å¤šé¡¹ç›®ï¼Œè·å–ç»“æŸã€‚")
                    break

                if current_page == 1:
                    total_pages = list_data.get('total_pages', 1)

                for item in list_data['items']:
                    media_type = item.get('media_type')
                    tmdb_id = item.get('id')
                    
                    item_type_mapped = 'Series' if media_type == 'tv' else 'Movie'
                    title = item.get('title') if item_type_mapped == 'Movie' else item.get('name')

                    if tmdb_id:
                        all_items.append({
                            'id': str(tmdb_id), 
                            'type': item_type_mapped,
                            'title': title
                        })

                current_page += 1

            except Exception as e:
                logger.error(f"  âœ è·å–æˆ–è§£æTMDbç‰‡å•é¡µé¢ {current_page} æ—¶å‡ºé”™: {e}")
                break
        
        logger.info(f"  âœ TMDbç‰‡å•è·å–å®Œæˆï¼Œä» {total_pages} ä¸ªé¡µé¢ä¸­æ€»å…±è§£æå‡º {len(all_items)} ä¸ªé¡¹ç›®ã€‚")
        return all_items
    
    def _get_items_from_tmdb_discover(self, url: str) -> List[Dict[str, str]]:
        """ä¸“é—¨ç”¨äºè§£æTMDb Discover URLå¹¶è·å–ç»“æœçš„å‡½æ•°"""
        from urllib.parse import urlparse, parse_qs
        from datetime import datetime, timedelta
        import re

        logger.info(f"  âœ æ£€æµ‹åˆ°TMDb Discoveré“¾æ¥ï¼Œå¼€å§‹åŠ¨æ€è·å– (æ”¯æŒåˆ†é¡µå’Œè¿‡æ»¤): {url}")
        
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)
        params = {k: v[0] for k, v in query_params.items()}

        today = datetime.now()
        date_pattern = re.compile(r'{today([+-]\d+)?}')

        for key, value in params.items():
            match = date_pattern.search(value)
            if match:
                offset_str = match.group(1) 
                target_date = today
                if offset_str:
                    days = int(offset_str)
                    target_date = today + timedelta(days=days)
                params[key] = value.replace(match.group(0), target_date.strftime('%Y-%m-%d'))

        all_items = []
        current_page = 1
        total_pages = 1
        MAX_PAGES_TO_FETCH = 10

        while current_page <= total_pages and current_page <= MAX_PAGES_TO_FETCH:
            try:
                params['page'] = current_page
                logger.debug(f"    âœ æ­£åœ¨è·å–ç¬¬ {current_page} / {total_pages} é¡µ...")

                discover_data = None
                item_type_for_result = None

                if '/discover/movie' in url:
                    discover_data = tmdb.discover_movie_tmdb(self.tmdb_api_key, params)
                    item_type_for_result = 'Movie'
                elif '/discover/tv' in url:
                    discover_data = tmdb.discover_tv_tmdb(self.tmdb_api_key, params)
                    item_type_for_result = 'Series'
                else:
                    logger.warning(f"  ğŸš« æ— æ³•ä»URL '{url}' åˆ¤æ–­æ˜¯ç”µå½±è¿˜æ˜¯ç”µè§†å‰§ï¼Œdiscoverä»»åŠ¡ä¸­æ­¢ã€‚")
                    break

                if not discover_data or not discover_data.get('results'):
                    logger.info("    âœ åœ¨å½“å‰é¡µæœªå‘ç°æ›´å¤šé¡¹ç›®ï¼Œè·å–ç»“æŸã€‚")
                    break

                if current_page == 1:
                    total_pages = discover_data.get('total_pages', 1)

                for item in discover_data['results']:
                    if not item.get('poster_path'):
                        continue
                    if not item.get('overview'):
                        continue
                    
                    tmdb_id = item.get('id')
                    if tmdb_id and item_type_for_result:
                        title = item.get('title') if item_type_for_result == 'Movie' else item.get('name')
                        date_str = item.get('release_date') if item_type_for_result == 'Movie' else item.get('first_air_date')
                        year = date_str[:4] if date_str else None
                        all_items.append({
                            'id': str(tmdb_id), 
                            'type': item_type_for_result,
                            'title': title,
                            'release_date': date_str, 
                            'year': year
                        })
                
                current_page += 1

            except Exception as e:
                logger.error(f"  âœ è·å–æˆ–è§£æTMDb Discoveré“¾æ¥çš„ç¬¬ {current_page} é¡µæ—¶å‡ºé”™: {e}")
                break

        logger.info(f"  âœ TMDb Discover è·å–å®Œæˆï¼Œä» {total_pages} ä¸ªé¡µé¢ä¸­æ€»å…±è§£æå‡º {len(all_items)} ä¸ªé¡¹ç›®ã€‚")
        return all_items
    
    def _get_titles_and_imdbids_from_url(self, url: str) -> Tuple[List[Dict[str, str]], str]:
        source_type = 'list_rss' 
        items = []

        if 'themoviedb.org/discover/' in url:
            source_type = 'list_discover'
            items = self._get_items_from_tmdb_discover(url)
        elif 'themoviedb.org/list/' in url:
            source_type = 'list_tmdb'
            items = self._get_items_from_tmdb_list(url)
        elif 'douban.com/doulist' in url:
            source_type = 'list_douban'
            items = self._get_items_from_douban_doulist(url)
        else:
            logger.info(f"  âœ å¼€å§‹è·å–æ ‡å‡†RSSæ¦œå•: {url}")
            try:
                response = self.session.get(url, timeout=20)
                response.raise_for_status()
                content = response.text
                if 'encoding="gb2312"' in content.lower():
                     content = response.content.decode('gb2312', errors='ignore')
                
                root = ET.fromstring(content)
                channel = root.find('channel')
                if channel is None: return [], source_type

                for item in channel.findall('item'):
                    title_elem = item.find('title')
                    guid_elem = item.find('guid')
                    link_elem = item.find('link')
                    description_elem = item.find('description')
                    
                    title = title_elem.text if title_elem is not None else None
                    description = description_elem.text if description_elem is not None else ''
                    
                    douban_link = None
                    if link_elem is not None and link_elem.text and 'douban.com' in link_elem.text:
                        douban_link = link_elem.text
                    elif guid_elem is not None and guid_elem.text and 'douban.com' in guid_elem.text:
                        douban_link = guid_elem.text

                    year = None
                    year_match = re.search(r'\b(20\d{2})\b', description)
                    if year_match: year = year_match.group(1)

                    imdb_id = None
                    if guid_elem is not None and guid_elem.text:
                        match = re.search(r'tt\d{7,8}', guid_elem.text)
                        if match: imdb_id = match.group(0)
                    if not imdb_id and link_elem is not None and link_elem.text:
                        match = re.search(r'tt\d{7,8}', link_elem.text)
                        if match: imdb_id = match.group(0)
                    
                    if title:
                        items.append({'title': title.strip(), 'imdb_id': imdb_id, 'year': year, 'douban_link': douban_link})
            except Exception as e:
                logger.error(f"ä»RSS URL '{url}' è·å–æ¦œå•æ—¶å‡ºé”™: {e}")
        
        return items, source_type

    def _match_title_to_tmdb(self, title: str, item_type: str, year: Optional[str] = None) -> Optional[Tuple[str, str, Optional[int]]]:
        def normalize_string(s: str) -> str:
            if not s: return ""
            return re.sub(r'[\s:ï¼šÂ·\-*\'!,?.ã€‚]+', '', s).lower()

        if item_type == 'Movie':
            titles_to_try = set([title.strip()])
            match = re.match(r'([\u4e00-\u9fa5\sÂ·0-9]+)[\s:ï¼š*]*(.*)', title.strip())
            if match:
                part1 = match.group(1).strip()
                part2 = match.group(2).strip()
                if part1: titles_to_try.add(part1)
                if part2: titles_to_try.add(part2)

            num_map = {'1': 'ä¸€', '2': 'äºŒ', '3': 'ä¸‰', '4': 'å››', '5': 'äº”', '6': 'å…­', '7': 'ä¸ƒ', '8': 'å…«', '9': 'ä¹'}
            current_titles = list(titles_to_try) 
            for t in current_titles:
                if any(num in t for num in num_map.keys()):
                    new_title = t
                    for num, char in num_map.items():
                        new_title = new_title.replace(num, char)
                    titles_to_try.add(new_title)
            
            final_titles = list(titles_to_try)
            logger.debug(f"  âœ ä¸º '{title}' ç”Ÿæˆçš„æœ€ç»ˆå€™é€‰æœç´¢æ ‡é¢˜: {final_titles}")

            first_search_results = None
            year_info = f" (å¹´ä»½: {year})" if year else ""

            for title_variation in final_titles:
                if not title_variation: continue
                
                results = search_media(title_variation, self.tmdb_api_key, 'Movie', year=year)
                
                if first_search_results is None:
                    first_search_results = results

                if not results:
                    continue

                norm_variation = normalize_string(title_variation)

                for result in results:
                    norm_title = normalize_string(result.get('title'))
                    norm_original_title = normalize_string(result.get('original_title'))

                    if norm_variation == norm_title or norm_variation == norm_original_title:
                        tmdb_id = str(result.get('id'))
                        logger.info(f"  âœ ç”µå½±æ ‡é¢˜ '{title}'{year_info} é€šè¿‡ã€ç²¾ç¡®è§„èŒƒåŒ¹é…ã€‘(ä½¿ç”¨'{title_variation}') æˆåŠŸåŒ¹é…åˆ°: {result.get('title')} (ID: {tmdb_id})")
                        return tmdb_id, 'Movie', None
                
                for result in results:
                    norm_title = normalize_string(result.get('title'))
                    norm_original_title = normalize_string(result.get('original_title'))

                    if norm_variation in norm_title or norm_variation in norm_original_title:
                        tmdb_id = str(result.get('id'))
                        logger.info(f"  âœ ç”µå½±æ ‡é¢˜ '{title}'{year_info} é€šè¿‡ã€åŒ…å«åŒ¹é…ã€‘(ä½¿ç”¨'{title_variation}') æˆåŠŸåŒ¹é…åˆ°: {result.get('title')} (ID: {tmdb_id})")
                        return tmdb_id, 'Movie', None

            if first_search_results:
                first_result = first_search_results[0]
                tmdb_id = str(first_result.get('id'))
                logger.warning(f"  âœ ç”µå½±æ ‡é¢˜ '{title}'{year_info} æ‰€æœ‰ç²¾ç¡®åŒ¹é…å’ŒåŒ…å«åŒ¹é…å‡å¤±è´¥ã€‚å°†ã€å›é€€ä½¿ç”¨ã€‘æœ€ç›¸å…³çš„æœç´¢ç»“æœ: {first_result.get('title')} (ID: {tmdb_id})")
                return tmdb_id, 'Movie', None

            logger.error(f"  âœ ç”µå½±æ ‡é¢˜ '{title}'{year_info} æœªèƒ½åœ¨TMDbä¸Šæ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœã€‚")
            return None
        
        elif item_type == 'Series':
            show_name_parsed, season_number_to_validate = parse_series_title_and_season(title, api_key=self.tmdb_api_key)
            show_name = show_name_parsed if show_name_parsed else title
            
            results = search_media(show_name, self.tmdb_api_key, 'Series', year=year)

            if not results and year and season_number_to_validate is not None:
                logger.debug(f"  âœ å¸¦å¹´ä»½ '{year}' æœç´¢å‰§é›† '{show_name}' æœªæ‰¾åˆ°ç»“æœï¼Œå¯èƒ½æ˜¯åç»­å­£ã€‚å°è¯•ä¸å¸¦å¹´ä»½è¿›è¡Œå›é€€æœç´¢...")
                results = search_media(show_name, self.tmdb_api_key, 'Series', year=None)

            if not results:
                year_info = f" (å¹´ä»½: {year})" if year else ""
                logger.warning(f"  âœ å‰§é›†æ ‡é¢˜ '{title}' (æœç´¢è¯: '{show_name}'){year_info} æœªèƒ½åœ¨TMDbä¸Šæ‰¾åˆ°åŒ¹é…é¡¹ã€‚")
                return None
            
            if season_number_to_validate is None:
                series_result = None
                norm_show_name = normalize_string(show_name)
                
                for result in results:
                    if normalize_string(result.get('name', '')) == norm_show_name:
                        series_result = result
                        logger.debug(f"  âœ å‰§é›† '{show_name}' é€šè¿‡ã€ç²¾ç¡®è§„èŒƒåŒ¹é…ã€‘æ‰¾åˆ°äº†: {result.get('name')} (ID: {result.get('id')})")
                        break 
                
                if not series_result:
                    series_result = results[0]
                    logger.warning(f"  âœ å‰§é›† '{show_name}' æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨é¦–ä¸ªç»“æœ: {series_result.get('name')} (ID: {series_result.get('id')})")

                return str(series_result.get('id')), 'Series', None

            else:
                def verify_season_in_results(candidates_list, source_desc=""):
                    if not candidates_list:
                        return None
                    
                    norm_show_name = normalize_string(show_name)
                    candidates_list.sort(key=lambda x: 0 if normalize_string(x.get('name', '')) == norm_show_name else 1)
                    
                    logger.info(f"  âœ å‰§é›† '{show_name}'{source_desc} éœ€è¦éªŒè¯ç¬¬ {season_number_to_validate} å­£ï¼Œæ­£åœ¨æ‰«æ {len(candidates_list)} ä¸ªå€™é€‰ç»“æœ...")

                    for candidate in candidates_list:
                        candidate_id = str(candidate.get('id'))
                        candidate_name = candidate.get('name')
                        
                        series_details = tmdb.get_tv_details(int(candidate_id), self.tmdb_api_key, append_to_response="seasons")
                        
                        if series_details and 'seasons' in series_details:
                            has_season = False
                            for season in series_details['seasons']:
                                if season.get('season_number') == season_number_to_validate:
                                    has_season = True
                                    break
                            
                            if has_season:
                                logger.info(f"  âœ åŒ¹é…æˆåŠŸï¼åœ¨å€™é€‰ç»“æœ '{candidate_name}' (ID: {candidate_id}) ä¸­æ‰¾åˆ°äº†ç¬¬ {season_number_to_validate} å­£ã€‚")
                                return candidate_id
                            else:
                                logger.debug(f"    - å€™é€‰ '{candidate_name}' (ID: {candidate_id}) æ²¡æœ‰ç¬¬ {season_number_to_validate} å­£ï¼Œè·³è¿‡ã€‚")
                    return None

                matched_id = verify_season_in_results(results[:5])
                if matched_id:
                    return matched_id, 'Series', season_number_to_validate

                if year:
                    logger.info(f"  âœ å‰§é›† '{show_name}' å¸¦å¹´ä»½ ({year}) æœç´¢ç»“æœä¸­æœªæ‰¾åˆ°ç¬¬ {season_number_to_validate} å­£ï¼Œå°è¯•ç§»é™¤å¹´ä»½é‡æœ...")
                    results_no_year = search_media(show_name, self.tmdb_api_key, 'Series', year=None)
                    
                    if results_no_year:
                        checked_ids = set(str(r.get('id')) for r in results[:5])
                        candidates_no_year = [r for r in results_no_year if str(r.get('id')) not in checked_ids][:5]
                        
                        if candidates_no_year:
                            matched_id = verify_season_in_results(candidates_no_year, source_desc=" (æ— å¹´ä»½é‡æœ)")
                            if matched_id:
                                return matched_id, 'Series', season_number_to_validate

                logger.warning(f"  âœ éªŒè¯å¤±è´¥ï¼åœ¨ '{show_name}' çš„æ‰€æœ‰æœç´¢ç»“æœä¸­ï¼Œå‡æœªæ‰¾åˆ°ç¬¬ {season_number_to_validate} å­£ã€‚")
                    
                if show_name != title:
                    logger.info(f"  âœ [å…œåº•æœºåˆ¶] å°è¯•ä½¿ç”¨åŸå§‹æ ‡é¢˜ '{title}' è¿›è¡Œå›é€€æœç´¢...")
                    fallback_results = search_media(title, self.tmdb_api_key, 'Series', year=None)
                    
                    if fallback_results:
                        best_match = fallback_results[0]
                        logger.info(f"  âœ [å…œåº•æˆåŠŸ] åŸå§‹æ ‡é¢˜ '{title}' åŒ¹é…åˆ°äº†: {best_match.get('name')} (ID: {best_match.get('id')})")
                        return str(best_match.get('id')), 'Series', None
            
            return None
                
        return None

    def process(self, definition: Dict) -> Tuple[List[Dict[str, str]], str]:
        raw_url = definition.get('url')
        urls = []
        if isinstance(raw_url, list):
            urls = [u for u in raw_url if u]
        elif isinstance(raw_url, str) and raw_url:
            urls = [raw_url]
            
        if not urls: return [], 'empty'
        
        collected_lists = [] 
        last_source_type = 'mixed'
        
        total_urls = len(urls)
        for i, url in enumerate(urls):
            temp_def = definition.copy()
            temp_def['url'] = url
            
            items, source_type = self._process_single_url(url, temp_def)
            
            collected_lists.append(items)
            last_source_type = source_type
            
            if isinstance(url, str) and url.startswith('maoyan://'):
                if i < total_urls - 1:
                    logger.info(f"  âœ [é˜²å°æ§] å•ä¸ªçŒ«çœ¼æ¦œå•é‡‡é›†å®Œæ¯•ï¼Œä¸ºå®‰å…¨èµ·è§ï¼Œå¼ºåˆ¶ä¼‘çœ  10 ç§’åå†é‡‡é›†ä¸‹ä¸€ä¸ª...")
                    time.sleep(10)
        
        all_items = []
        if collected_lists:
            max_length = max(len(l) for l in collected_lists) if collected_lists else 0
            for i in range(max_length):
                for sublist in collected_lists:
                    if i < len(sublist):
                        all_items.append(sublist[i])
            
            logger.info(f"  âœ å·²å°† {len(collected_lists)} ä¸ªæ¦œå•æºäº¤å‰åˆå¹¶ï¼Œæ€»è®¡ {len(all_items)} ä¸ªå€™é€‰é¡¹ã€‚")
            
        unique_items = []
        seen_keys = set()
        for item in all_items:
            tmdb_id = item.get('id')
            item_type = item.get('type')
            title = item.get('title')
            season = item.get('season')
            
            if tmdb_id:
                key = f"{item_type}-{tmdb_id}-{season}"
            else:
                key = f"unidentified-{title}"
            
            if key not in seen_keys:
                seen_keys.add(key)
                unique_items.append(item)
        
        limit = definition.get('limit')
        if limit and isinstance(limit, int) and limit > 0:
            unique_items = unique_items[:limit]
            
        return unique_items, last_source_type

    def _process_single_url(self, url: str, definition: Dict) -> Tuple[List[Dict[str, str]], str]:
        definition = definition.copy()
        definition['url'] = url
        source_type = 'list_rss'
        
        if not url:
            return [], source_type
            
        if url.startswith('maoyan://'):
            source_type = 'list_maoyan'
            logger.info(f"  âœ æ£€æµ‹åˆ°çŒ«çœ¼æ¦œå•ï¼Œå°†å¯åŠ¨å¼‚æ­¥åå°è„šæœ¬...")
            greenlet = gevent.spawn(self._execute_maoyan_fetch, definition)
            tmdb_items = greenlet.get()
            return tmdb_items, source_type

        item_types = definition.get('item_type', ['Movie'])
        if isinstance(item_types, str): item_types = [item_types]
        limit = definition.get('limit')
        
        items, source_type = self._get_titles_and_imdbids_from_url(url)
        
        if not items: return [], source_type
        
        if items and 'id' in items[0] and 'type' in items[0]:
            logger.info(f"  âœ æ£€æµ‹åˆ°æ¥è‡ªTMDbæº ({source_type}) çš„é¢„åŒ¹é…IDï¼Œå°†è·³è¿‡æ ‡é¢˜åŒ¹é…ã€‚")
            if limit and isinstance(limit, int) and limit > 0:
                items = items[:limit]
            return items, source_type

        if limit and isinstance(limit, int) and limit > 0:
            items = items[:limit]
        
        tmdb_items = []
        douban_api = DoubanApi()

        with ThreadPoolExecutor(max_workers=5) as executor:
            def find_first_match(item: Dict[str, str], types_to_check):
                original_source_title = item.get('title', '').strip()
                year = item.get('year')
                rss_imdb_id = item.get('imdb_id')
                douban_link = item.get('douban_link')

                def create_result(tmdb_id, item_type, confirmed_season=None):
                    result = {
                        'id': tmdb_id, 
                        'type': item_type, 
                        'title': original_source_title,
                        'year': year
                    }
                    if item_type == 'Series' and confirmed_season is not None:
                        result['season'] = confirmed_season
                    return result

                fallback_result = {
                    'id': None, 
                    'type': types_to_check[0] if types_to_check else 'Movie', 
                    'title': original_source_title,
                    'year': year
                }

                if rss_imdb_id:
                    for item_type in types_to_check:
                        tmdb_id = self._match_by_ids(rss_imdb_id, None, item_type)
                        if tmdb_id:
                            _, s_num = parse_series_title_and_season(original_source_title, api_key=self.tmdb_api_key)
                            return create_result(tmdb_id, item_type, s_num)

                cleaned_title = re.sub(r'^\s*\d+\.\s*', '', original_source_title)
                cleaned_title = re.sub(r'\s*\(\d{4}\)$', '', cleaned_title).strip()
                
                for item_type in types_to_check:
                    match_result = self._match_title_to_tmdb(cleaned_title, item_type, year=year)
                    
                    if match_result:
                        tmdb_id, matched_type, matched_season = match_result
                        return create_result(tmdb_id, matched_type, matched_season)
                
                if douban_link:
                    logger.info(f"  âœ ç‰‡å+å¹´ä»½åŒ¹é… '{original_source_title}' å¤±è´¥ï¼Œå¯åŠ¨å¤‡ç”¨æ–¹æ¡ˆï¼šé€šè¿‡è±†ç“£é“¾æ¥è·å–æ›´å¤šä¿¡æ¯...")
                    douban_details = douban_api.get_details_from_douban_link(douban_link, mtype=types_to_check[0] if types_to_check else None)
                    
                    if douban_details:
                        imdb_id_from_douban = douban_details.get("imdb_id")
                        if not imdb_id_from_douban and douban_details.get("attrs", {}).get("imdb"):
                            imdb_ids = douban_details["attrs"]["imdb"]
                            if isinstance(imdb_ids, list) and len(imdb_ids) > 0:
                                imdb_id_from_douban = imdb_ids[0]

                        if imdb_id_from_douban:
                            logger.info(f"  âœ è±†ç“£å¤‡ç”¨æ–¹æ¡ˆ(3a)æˆåŠŸï¼æ‹¿åˆ°IMDb ID: {imdb_id_from_douban}ï¼Œç°åœ¨ç”¨å®ƒåŒ¹é…TMDb...")
                            for item_type in types_to_check:
                                tmdb_id = self._match_by_ids(imdb_id_from_douban, None, item_type)
                                if tmdb_id:
                                    return create_result(tmdb_id, item_type)
                        
                        logger.info(f"  âœ è±†ç“£å¤‡ç”¨æ–¹æ¡ˆ(3a)å¤±è´¥ï¼Œå°è¯•æ–¹æ¡ˆ(3b): ä½¿ç”¨ original_title...")
                        original_title = douban_details.get("original_title")
                        if original_title:
                            for item_type in types_to_check:
                                match_result = self._match_title_to_tmdb(original_title, item_type, year=year)
                                if match_result:
                                    tmdb_id, matched_type, matched_season = match_result
                                    logger.info(f"  âœ è±†ç“£å¤‡ç”¨æ–¹æ¡ˆ(3b)æˆåŠŸï¼é€šè¿‡ original_title '{original_title}' åŒ¹é…æˆåŠŸã€‚")
                                    return create_result(tmdb_id, matched_type, matched_season)

                logger.debug(f"  âœ æ‰€æœ‰ä¼˜å…ˆæ–¹æ¡ˆå‡å¤±è´¥ï¼Œå°è¯•ä¸å¸¦å¹´ä»½è¿›è¡Œæœ€åçš„å›é€€æœç´¢: '{original_source_title}'")
                for item_type in types_to_check:
                    match_result = self._match_title_to_tmdb(cleaned_title, item_type, year=None)
                    if match_result:
                        tmdb_id, matched_type, matched_season = match_result
                        logger.warning(f"  âœ æ³¨æ„ï¼š'{original_source_title}' åœ¨æœ€åçš„å›é€€æœç´¢ä¸­åŒ¹é…æˆåŠŸï¼Œä½†å¹´ä»½å¯èƒ½ä¸å‡†ã€‚")
                        return create_result(tmdb_id, matched_type, matched_season)

                logger.error(f"  âœ å½»åº•å¤±è´¥ï¼šæ‰€æœ‰æ–¹æ¡ˆéƒ½æ— æ³•ä¸º '{original_source_title}' æ‰¾åˆ°åŒ¹é…é¡¹ã€‚")
                return fallback_result

            results_in_order = executor.map(lambda item: find_first_match(item, item_types), items)
            tmdb_items = [result for result in results_in_order if result is not None]
        
        douban_api.close()
        logger.info(f"  âœ RSSåŒ¹é…å®Œæˆï¼ŒæˆåŠŸè·å¾— {len(tmdb_items)} ä¸ªTMDbé¡¹ç›®ã€‚")
        
        unique_items = []
        seen_keys = set()
        
        for item in tmdb_items:
            tmdb_id = item.get('id')
            item_type = item.get('type')
            title = item.get('title')
            season = item.get('season')
            
            if tmdb_id:
                key = f"{item_type}-{tmdb_id}-{season}"
            else:
                key = f"unidentified-{title}"
            
            if key not in seen_keys:
                seen_keys.add(key)
                unique_items.append(item)
                
        logger.info(f"  âœ å»é‡åå‰©ä½™ {len(unique_items)} ä¸ªæœ‰æ•ˆé¡¹ç›®ã€‚")

        return unique_items, source_type


class RecommendationEngine:
    """
    ã€AI æ¨èå¼•æ“ (åŒæ¨¡ç‰ˆ)ã€‘
    æ¨¡å¼ A (LLM): åŸºäºå¤§æ¨¡å‹çŸ¥è¯†åº“æ¨è (é€‚åˆå‘ç°æ–°ç‰‡)ã€‚
    æ¨¡å¼ B (Vector): åŸºäºæœ¬åœ°æ•°æ®åº“å‘é‡ç›¸ä¼¼åº¦æ¨è (é€‚åˆç²¾å‡†åŒ¹é…å£å‘³)ã€‚
    """
    _cache_matrix = None
    _cache_ids = None
    _cache_titles = None
    _cache_types = None
    _REFRESH_INTERVAL = 14400
    _is_refreshing_loop_running = False 

    def __init__(self, tmdb_api_key: str):
        self.tmdb_api_key = tmdb_api_key
        self.list_importer = ListImporter(tmdb_api_key) 

    @classmethod
    def refresh_cache(cls):
        """
        ã€ç±»æ–¹æ³•ã€‘å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ (æ‰§è¡Œæ•°æ®åº“è¯»å–å’ŒçŸ©é˜µæ„å»º)
        """
        logger.info("  ğŸ”„ [å‘é‡å¼•æ“] å¼€å§‹åå°åˆ·æ–°å‘é‡ç¼“å­˜...")
        start_t = time.time()
        
        try:
            with connection.get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT tmdb_id, title, item_type, overview_embedding 
                    FROM media_metadata 
                    WHERE overview_embedding IS NOT NULL
                      AND item_type IN ('Movie', 'Series')
                      AND in_library = TRUE
                """)
                all_data = cursor.fetchall()
            
            if not all_data:
                logger.warning("  âš ï¸ [å‘é‡å¼•æ“] æ•°æ®åº“ä¸ºç©ºï¼Œæ— æ³•åˆ·æ–°ç¼“å­˜ã€‚")
                return

            ids, vectors, titles, types = [], [], [], []
            for row in all_data:
                vec = row.get('overview_embedding')
                if vec and len(vec) > 0:
                    ids.append(str(row['tmdb_id']))
                    titles.append(row['title'])
                    types.append(row['item_type'])
                    vectors.append(np.array(vec, dtype=np.float32))
            
            if not vectors:
                return

            matrix = np.stack(vectors)
            norm = np.linalg.norm(matrix, axis=1, keepdims=True)
            matrix = matrix / (norm + 1e-10)

            cls._cache_matrix = matrix
            cls._cache_ids = ids
            cls._cache_titles = titles
            cls._cache_types = types
            
            logger.info(f"  âœ… [å‘é‡å¼•æ“] ç¼“å­˜åˆ·æ–°å®Œæˆã€‚å…± {len(ids)} æ¡ï¼Œè€—æ—¶ {time.time() - start_t:.2f}sã€‚")

        except Exception as e:
            logger.error(f"  âŒ [å‘é‡å¼•æ“] åˆ·æ–°ç¼“å­˜å¤±è´¥: {e}", exc_info=True)

    @classmethod
    def start_auto_refresh_loop(cls):
        """
        ã€ç±»æ–¹æ³•ã€‘å¯åŠ¨è‡ªåŠ¨åˆ·æ–°å¾ªç¯
        """
        if cls._is_refreshing_loop_running:
            return
        
        cls._is_refreshing_loop_running = True
        
        def loop():
            logger.info("  ğŸš€ [å‘é‡å¼•æ“] è‡ªåŠ¨åˆ·æ–°å®ˆæŠ¤çº¿ç¨‹å·²å¯åŠ¨ã€‚")
            cls.refresh_cache()
            
            while True:
                gevent.sleep(cls._REFRESH_INTERVAL)
                cls.refresh_cache()
        
        gevent.spawn(loop)

    def _get_vector_data(self):
        """
        ã€å†…éƒ¨æ–¹æ³•ã€‘è·å–å‘é‡æ•°æ® (æé€Ÿç‰ˆ)
        """
        if RecommendationEngine._cache_matrix is None:
            RecommendationEngine.refresh_cache()
            
        return (RecommendationEngine._cache_matrix, 
                RecommendationEngine._cache_ids, 
                RecommendationEngine._cache_titles, 
                RecommendationEngine._cache_types)

    def _vector_search(self, user_history_items: List[Dict], exclusion_ids: set = None, limit: int = 10, allowed_types: List[str] = None) -> List[Dict]:
        """
        ã€å†…éƒ¨æ–¹æ³•ã€‘åŸºäºå‘é‡ç›¸ä¼¼åº¦æœç´¢æœ¬åœ°æ•°æ®åº“ã€‚
        """
        if exclusion_ids is None: exclusion_ids = set()
        if not allowed_types: allowed_types = ['Movie', 'Series']
        if exclusion_ids is None:
            exclusion_ids = set()

        history_tmdb_ids = set()
        history_titles = set()
        for item in user_history_items:
            if isinstance(item, dict):
                if item.get('tmdb_id'): history_tmdb_ids.add(str(item.get('tmdb_id')))
                if item.get('title'): history_titles.add(item.get('title'))
            elif isinstance(item, str):
                history_titles.add(item)

        if not history_tmdb_ids and not history_titles:
            return []

        matrix, ids, titles, types = self._get_vector_data()
        
        if matrix is None:
            logger.warning("  âœ [å‘é‡æœç´¢] æ— æ³•è·å–å‘é‡æ•°æ® (æ•°æ®åº“ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥)ã€‚")
            return []

        try:
            user_vectors = []
            
            for idx, db_tmdb_id in enumerate(ids):
                is_match = False
                if db_tmdb_id in history_tmdb_ids:
                    is_match = True
                elif titles[idx] and any(h_t in titles[idx] for h_t in history_titles):
                    is_match = True
                
                if is_match:
                    user_vectors.append(matrix[idx])
            
            if not user_vectors:
                logger.warning(f"  âœ [å‘é‡æœç´¢] åŒ¹é…å¤±è´¥ï¼šç”¨æˆ·çš„å†å²è®°å½•æœªåœ¨å‘é‡åº“ä¸­æ‰¾åˆ°å¯¹åº”æ•°æ®ã€‚")
                return []
            
            user_profile_vector = np.mean(user_vectors, axis=0)
            user_profile_vector = user_profile_vector / (np.linalg.norm(user_profile_vector) + 1e-10)

            scores = np.dot(matrix, user_profile_vector)
            top_indices = np.argsort(scores)[::-1]
            
            results = []
            count = 0
            
            for idx in top_indices:
                if count >= max(limit, 200): break
                
                if types[idx] not in allowed_types:
                    continue

                score = float(scores[idx])
                if score < 0.45: break 
                if score > 0.999: continue 
                
                current_id = ids[idx]
                
                if current_id in exclusion_ids: continue
                if current_id in history_tmdb_ids: continue
                if any(h_t in titles[idx] for h_t in history_titles): continue

                results.append({
                    'id': current_id,
                    'type': types[idx],
                    'title': titles[idx],
                    'score': score
                })
                count += 1
                
            return results

        except Exception as e:
            logger.error(f"  âœ [å‘é‡æœç´¢] è®¡ç®—è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return []
        
    def generate_user_vector(self, user_id: str, limit: int = 50, allowed_types: List[str] = None) -> List[Dict]:
        """
        åªä½¿ç”¨å‘é‡æœç´¢ï¼Œé€Ÿåº¦å¿«ï¼Œé€‚åˆå®æ—¶ç”Ÿæˆã€‚
        """
        logger.debug(f"  âœ [ä¸ªäººå‘é‡æ¨è] æ­£åœ¨ä¸ºç”¨æˆ· {user_id} å®æ—¶è®¡ç®—...")
        
        context_history_items = media_db.get_user_positive_history(user_id, limit=50)
        if not context_history_items:
            logger.warning(f"  âœ ç”¨æˆ· {user_id} å†å²è®°å½•ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆå‘é‡æ¨èã€‚")
            return []

        watched_tmdb_ids = set()
        full_interaction = media_db.get_user_all_interacted_history(user_id)
        for item in full_interaction:
            if item.get('tmdb_id'):
                watched_tmdb_ids.add(str(item.get('tmdb_id')))

        results = self._vector_search(
            user_history_items=context_history_items,
            exclusion_ids=watched_tmdb_ids,
            limit=limit,
            allowed_types=allowed_types 
        )
        
        return results

    def generate(self, definition: Dict) -> List[Dict[str, str]]:
        """
        æ¨èç”Ÿæˆå™¨ã€‚
        """
        ai_prompt = definition.get('ai_prompt')
        limit = definition.get('limit', 20)
        discovery_ratio = float(definition.get('ai_discovery_ratio', 0.2))
        allowed_types = definition.get('item_type', ['Movie', 'Series'])

        logger.debug("  âœ [æ™ºèƒ½æ¨è] å¯åŠ¨ (LLM + å‘é‡æ··åˆæ¨¡å¼)...")

        context_history_items = media_db.get_global_popular_items(limit=20)
        if not context_history_items:
            logger.warning("  âœ å…¨ç«™æ’­æ”¾æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆå…¨å±€æ¨èã€‚")
            return []

        watched_tmdb_ids = set()
        for item in context_history_items:
            if item.get('tmdb_id'):
                watched_tmdb_ids.add(str(item.get('tmdb_id')))

        final_items_map = {}

        logger.info(f"  âœ [æ™ºèƒ½æ¨è] æ­£åœ¨è°ƒç”¨ LLM åˆ†æå…¨ç«™å£å‘³...")
        history_titles_for_llm = []
        for item in context_history_items:
            title = item.get('title')
            year = item.get('release_year')
            if year:
                history_titles_for_llm.append(f"{title} ({year})")
            else:
                history_titles_for_llm.append(title)

        try:
            translator = AITranslator(config_manager.APP_CONFIG)
            request_limit = int(limit * discovery_ratio)
            request_limit = max(request_limit, 2) 

            system_prompt = "ä½ æ˜¯èµ„æ·±é€‰ç‰‡äººã€‚åŸºäºä»¥ä¸‹å¤§ä¼—å–œæ¬¢çš„å½±ç‰‡ï¼Œæ¨èåŒç±»é«˜åˆ†ä½œå“ã€‚ä¸è¦æ¨èåˆ—è¡¨ä¸­å·²æœ‰çš„ã€‚"
            if ai_prompt:
                system_prompt += f" é¢å¤–è¦æ±‚: {ai_prompt}"

            llm_recommendations = translator.get_recommendations(
                user_history=history_titles_for_llm, 
                user_instruction=system_prompt,
                allowed_types=allowed_types 
            )
                    
            if llm_recommendations:
                logger.info(f"  âœ [æ™ºèƒ½æ¨è] LLM è¿”å›äº† {len(llm_recommendations)} éƒ¨ä½œå“ï¼Œæ­£åœ¨åŒ¹é… TMDb ID...")
                with ThreadPoolExecutor(max_workers=5) as executor:
                    def resolve_item(rec_item):
                        try:
                            title = ""
                            original_title = ""
                            year = None
                            primary_type = 'Movie' 
                            
                            if isinstance(rec_item, dict):
                                title = rec_item.get('title')
                                original_title = rec_item.get('original_title')
                                year = str(rec_item.get('year')) if rec_item.get('year') else None
                                if rec_item.get('type'):
                                    primary_type = rec_item.get('type')
                            elif isinstance(rec_item, str):
                                title = rec_item
                            
                            if not title: return None

                            search_types = []
                            if 'Movie' in allowed_types and 'Series' in allowed_types:
                                search_types = [primary_type, 'Series' if primary_type == 'Movie' else 'Movie']
                            elif 'Movie' in allowed_types:
                                search_types = ['Movie']
                            elif 'Series' in allowed_types:
                                search_types = ['Series']
                            
                            match_result = None
                            
                            def has_chinese(text):
                                return any('\u4e00' <= char <= '\u9fff' for char in str(text))
                            search_query = original_title if original_title else title
                            if has_chinese(title): search_query = title

                            for try_type in search_types:
                                match_result = self.list_importer._match_title_to_tmdb(search_query, try_type, year)
                                if match_result: break
                                
                                if search_query != title:
                                    match_result = self.list_importer._match_title_to_tmdb(title, try_type, year)
                                    if match_result: break

                            if match_result:
                                tmdb_id, matched_type, season_num = match_result
                                tmdb_id = str(tmdb_id)
                                
                                if matched_type not in allowed_types:
                                    return None

                                if tmdb_id in watched_tmdb_ids:
                                    return None
                                return {
                                    'id': tmdb_id,
                                    'type': matched_type,
                                    'title': title, 
                                    'season': season_num,
                                    'release_date': None 
                                }
                            return None
                        except Exception:
                            return None

                    results = executor.map(resolve_item, llm_recommendations)
                    for res in results:
                        if res:
                            final_items_map[res['id']] = res
            else:
                logger.info("  âœ [æ™ºèƒ½æ¨è] ç”¨æˆ·è®¾ç½®æ¢ç´¢æ¯”ä¾‹ä¸º 0%...")
        except Exception as e:
            logger.error(f"  âœ [æ™ºèƒ½æ¨è] LLM è°ƒç”¨å¤±è´¥: {e}")

        if len(final_items_map) < limit:
            needed = limit - len(final_items_map)
            logger.info(f"  âœ [æ™ºèƒ½æ¨è] å¯ç”¨å‘é‡å¼•æ“è¡¥å…… {needed} éƒ¨ç›¸ä¼¼å½±ç‰‡...")
            vector_results = self._vector_search(
                user_history_items=context_history_items, 
                exclusion_ids=watched_tmdb_ids, 
                limit=needed + 10
            )
            for v in vector_results:
                if v['type'] in allowed_types and v['id'] not in final_items_map:
                    final_items_map[v['id']] = {
                        'id': v['id'], 'type': v['type'], 'title': v['title']
                    }

        final_items = list(final_items_map.values())[:limit]
        logger.info(f"  âœ [æ™ºèƒ½æ¨è] å®Œæˆï¼Œç”Ÿæˆ {len(final_items)} éƒ¨å½±ç‰‡ã€‚")
        return final_items