# handler/custom_collection.py
import logging
import requests
import xml.etree.ElementTree as ET
import re
import os
import time
import numpy as np
import sys
import gevent
from typing import List, Dict, Any, Optional, Tuple
import json
from datetime import datetime, timedelta, date
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
from gevent import subprocess, Timeout

import handler.tmdb as tmdb
import handler.emby as emby
import utils
import config_manager
from tasks.helpers import parse_series_title_and_season
from database import collection_db, watchlist_db, media_db, connection
from handler.douban import DoubanApi
from handler.tmdb import search_media, get_tv_details
from ai_translator import AITranslator

logger = logging.getLogger(__name__)


class ListImporter:
    """
    (V9.1 - æœ€ç»ˆå¼‚æ­¥ç‰ˆ)
    ä½¿ç”¨ gevent.subprocessï¼Œå¹¶ç¡®ä¿åœ¨ç‹¬ç«‹çš„ greenlet ä¸­è¿è¡Œï¼Œ
    ä»è€Œå®ç°çœŸæ­£çš„éé˜»å¡å¼‚æ­¥æ‰§è¡Œã€‚
    """
    
    SEASON_PATTERN = re.compile(r'(.*?)\s*[ï¼ˆ(]?\s*(ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+)\s*å­£\s*[)ï¼‰]?')
    
    # â–¼â–¼â–¼ ä¼˜åŒ–ï¼šæ‰©å±•æ•°å­—æ˜ å°„ï¼Œæ”¯æŒåˆ°äºŒåå­£ï¼Œå¢å¼ºå…¼å®¹æ€§ â–¼â–¼â–¼
    CHINESE_NUM_MAP = {
        'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
        'åä¸€': 11, 'åäºŒ': 12, 'åä¸‰': 13, 'åå››': 14, 'åäº”': 15, 'åå…­': 16, 'åä¸ƒ': 17, 'åå…«': 18, 'åä¹': 19, 'äºŒå': 20,
        'ç¬¬ä¸€': 1, 'ç¬¬äºŒ': 2, 'ç¬¬ä¸‰': 3, 'ç¬¬å››': 4, 'ç¬¬äº”': 5, 'ç¬¬å…­': 6, 'ç¬¬ä¸ƒ': 7, 'ç¬¬å…«': 8, 'ç¬¬ä¹': 9, 'ç¬¬å': 10,
        'ç¬¬åä¸€': 11, 'ç¬¬åäºŒ': 12, 'ç¬¬åä¸‰': 13, 'ç¬¬åå››': 14, 'ç¬¬åäº”': 15, 'ç¬¬åå…­': 16, 'ç¬¬åä¸ƒ': 17, 'ç¬¬åå…«': 18, 'ç¬¬åä¹': 19, 'ç¬¬äºŒå': 20
    }
    VALID_MAOYAN_PLATFORMS = {'tencent', 'iqiyi', 'youku', 'mango'}

    def __init__(self, tmdb_api_key: str):
        self.tmdb_api_key = tmdb_api_key
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})

    # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šè¿™ä¸ªå‡½æ•°ç°åœ¨æ˜¯çº¯ç²¹çš„é˜»å¡æ‰§è¡Œé€»è¾‘ â˜…â˜…â˜…
    def _execute_maoyan_fetch(self, definition: Dict) -> List[Dict[str, str]]:
        maoyan_url = definition.get('url', '')
        temp_output_file = os.path.join(config_manager.PERSISTENT_DATA_PATH, f"maoyan_temp_output_{hash(maoyan_url)}.json")
        
        content_key = maoyan_url.replace('maoyan://', '')
        parts = content_key.split('-')
        
        platform = 'all'
        if len(parts) > 1 and parts[-1] in self.VALID_MAOYAN_PLATFORMS:
            platform = parts[-1]
            type_part = '-'.join(parts[:-1])
        else:
            type_part = content_key

        types_to_fetch = [t.strip() for t in type_part.split(',') if t.strip()]
        
        if not types_to_fetch:
            logger.error(f"  âœ æ— æ³•ä»çŒ«çœ¼URL '{maoyan_url}' ä¸­è§£æå‡ºæœ‰æ•ˆçš„ç±»å‹ã€‚")
            return []
            
        limit = definition.get('limit')
        if not limit:
            limit = 50

        fetcher_script_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'maoyan_fetcher.py')
        if not os.path.exists(fetcher_script_path):
            logger.error(f"  âœ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•æ‰¾åˆ°çŒ«çœ¼è·å–è„šæœ¬ '{fetcher_script_path}'ã€‚")
            return []

        command = [
            sys.executable,
            fetcher_script_path,
            '--api-key', self.tmdb_api_key,
            '--output-file', temp_output_file,
            '--num', str(limit),
            '--platform', platform,
            '--types', *types_to_fetch
        ]
        
        try:
            logger.debug(f"  âœ (åœ¨ä¸€ä¸ªç‹¬ç«‹çš„ Greenlet ä¸­) æ‰§è¡Œå‘½ä»¤: {' '.join(command)}")
            
            result_bytes = subprocess.check_output(
                command, 
                stderr=subprocess.STDOUT, 
                timeout=600
            )
            
            result_output = result_bytes.decode('utf-8', errors='ignore')
            logger.info("  âœ çŒ«çœ¼è·å–è„šæœ¬æˆåŠŸå®Œæˆã€‚")
            if result_output:
                logger.debug(f"  âœ è„šæœ¬è¾“å‡º:\n{result_output}")
            
            with open(temp_output_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            return results

        except Timeout:
            logger.error("  âœ æ‰§è¡ŒçŒ«çœ¼è·å–è„šæœ¬è¶…æ—¶ï¼ˆè¶…è¿‡10åˆ†é’Ÿï¼‰ã€‚")
            return []
        except subprocess.CalledProcessError as e:
            error_output = e.output.decode('utf-8', errors='ignore') if e.output else "No output captured."
            logger.error(f"  âœ æ‰§è¡ŒçŒ«çœ¼è·å–è„šæœ¬å¤±è´¥ã€‚è¿”å›ç : {e.returncode}")
            logger.error(f"  âœ è„šæœ¬çš„å®Œæ•´é”™è¯¯è¾“å‡º:\n{error_output}")
            return []
        except Exception as e:
            logger.error(f"  âœ å¤„ç†çŒ«çœ¼æ¦œå•æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return []
        finally:
            if os.path.exists(temp_output_file):
                os.remove(temp_output_file)

    # ... å…¶ä»–æ‰€æœ‰æ–¹æ³• (_match_by_ids, process, FilterEngineç­‰) ä¿æŒå®Œå…¨ä¸å˜ ...
    def _match_by_ids(self, imdb_id: Optional[str], tmdb_id: Optional[str], item_type: str) -> Optional[str]:
        if tmdb_id:
            logger.debug(f"  âœ é€šè¿‡TMDb IDç›´æ¥åŒ¹é…ï¼š{tmdb_id}")
            return tmdb_id
        if imdb_id:
            logger.debug(f"  âœ é€šè¿‡IMDb IDæŸ¥æ‰¾TMDb IDï¼š{imdb_id}")
            try:
                tmdb_id_from_imdb = tmdb.get_tmdb_id_by_imdb_id(imdb_id, self.tmdb_api_key, item_type)
                if tmdb_id_from_imdb:
                    logger.debug(f"  âœ IMDb ID {imdb_id} å¯¹åº” TMDb ID: {tmdb_id_from_imdb}")
                    return str(tmdb_id_from_imdb)
                else:
                    logger.warning(f"  âœ æ— æ³•é€šè¿‡IMDb ID {imdb_id} æŸ¥æ‰¾åˆ°å¯¹åº”çš„TMDb IDã€‚")
            except Exception as e:
                logger.error(f"  âœ é€šè¿‡IMDb IDæŸ¥æ‰¾TMDb IDæ—¶å‡ºé”™: {e}")
        return None
    
    def _extract_ids_from_title_or_line(self, title_line: str) -> Tuple[Optional[str], Optional[str]]:
        imdb_id = None
        tmdb_id = None
        imdb_match = re.search(r'(tt\d{7,8})', title_line, re.I)
        if imdb_match:
            imdb_id = imdb_match.group(1)
        tmdb_match = re.search(r'tmdb://(\d+)', title_line, re.I)
        if tmdb_match:
            tmdb_id = tmdb_match.group(1)
        return imdb_id, tmdb_id
    
    def _get_items_from_douban_doulist(self, url: str) -> List[Dict[str, str]]:
        """ä¸“é—¨ç”¨äºè§£æå’Œåˆ†é¡µè·å–è±†ç“£è±†åˆ—å†…å®¹çš„å‡½æ•°"""
        all_items = []
        # ä»URLä¸­ç§»é™¤åˆ†é¡µå‚æ•°ï¼Œå¾—åˆ°åŸºç¡€URL
        base_url = url.split('?')[0]
        page_start = 0
        # è®¾ç½®ä¸€ä¸ªæœ€å¤§é¡µæ•°é™åˆ¶ï¼Œé˜²æ­¢æ„å¤–çš„æ— é™å¾ªç¯
        max_pages = 50 
        items_per_page = 25

        logger.info(f"  âœ æ£€æµ‹åˆ°è±†ç“£è±†åˆ—é“¾æ¥ï¼Œå¼€å§‹åˆ†é¡µè·å–: {base_url}")

        for page in range(max_pages):
            current_start = page * items_per_page
            paginated_url = f"{base_url}?start={current_start}&sort=seq&playable=0&sub_type="
            
            try:
                logger.debug(f"    âœ æ­£åœ¨è·å–ç¬¬ {page + 1} é¡µ: {paginated_url}")
                response = self.session.get(paginated_url, timeout=20)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'lxml')
                
                # æŸ¥æ‰¾é¡µé¢ä¸Šæ‰€æœ‰çš„æ¡ç›®å®¹å™¨
                doulist_items = soup.find_all('div', class_='doulist-item')

                # å¦‚æœå½“å‰é¡µæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¡ç›®ï¼Œè¯´æ˜åˆ°è¾¾äº†æœ€åä¸€é¡µ
                if not doulist_items:
                    logger.info(f"  âœ åœ¨ç¬¬ {page + 1} é¡µæœªå‘ç°æ›´å¤šé¡¹ç›®ï¼Œè·å–ç»“æŸã€‚")
                    break

                for item in doulist_items:
                    title_div = item.find('div', class_='title')
                    if not title_div: continue
                    
                    link_tag = title_div.find('a')
                    if not link_tag: continue
                    
                    # æå–æ ‡é¢˜
                    title = link_tag.get_text(strip=True)
                    # æå–è±†ç“£é“¾æ¥
                    douban_link = link_tag.get('href')
                    
                    # å°è¯•æå–å¹´ä»½
                    year = None
                    abstract_div = item.find('div', class_='abstract')
                    if abstract_div:
                        # å¹´ä»½é€šå¸¸åœ¨ abstract å†…å®¹ä¸­ä»¥ (YYYY) æˆ– YYYY-MM-DD çš„å½¢å¼å‡ºç°
                        year_match = re.search(r'\b(19\d{2}|20\d{2})\b', abstract_div.get_text())
                        if year_match:
                            year = year_match.group(1)
                    
                    if title:
                        all_items.append({
                            'title': title,
                            'imdb_id': None, # è±†åˆ—é¡µé¢ä¸ç›´æ¥æä¾›IMDb ID
                            'year': year,
                            'douban_link': douban_link # âœ¨ å…³é”®ä¿¡æ¯ï¼šæˆ‘ä»¬æ‹¿åˆ°äº†æ¯ä¸ªæ¡ç›®çš„è±†ç“£é“¾æ¥
                        })

            except Exception as e:
                logger.error(f"  âœ è·å–æˆ–è§£æè±†ç“£è±†åˆ—é¡µé¢ '{paginated_url}' æ—¶å‡ºé”™: {e}")
                # å‡ºç°é”™è¯¯æ—¶ï¼Œä¸­æ–­åç»­æ‰€æœ‰é¡µé¢çš„è·å–
                break
        
        logger.info(f"  âœ è±†ç“£è±†åˆ—è·å–å®Œæˆï¼Œä» {page} ä¸ªé¡µé¢ä¸­æ€»å…±è§£æå‡º {len(all_items)} ä¸ªé¡¹ç›®ã€‚")
        return all_items
    
    def _get_items_from_tmdb_list(self, url: str) -> List[Dict[str, str]]:
        """ä¸“é—¨ç”¨äºè§£æå’Œåˆ†é¡µè·å–TMDbç‰‡å•å†…å®¹çš„å‡½æ•°"""
        match = re.search(r'themoviedb\.org/list/(\d+)', url)
        if not match:
            logger.error(f"  âœ æ— æ³•ä»URL '{url}' ä¸­è§£æå‡ºTMDbç‰‡å•IDã€‚")
            return []

        list_id = int(match.group(1))
        all_items = []
        current_page = 1
        total_pages = 1 # å…ˆå‡è®¾åªæœ‰ä¸€é¡µ

        logger.info(f"  âœ æ£€æµ‹åˆ°TMDbç‰‡å•é“¾æ¥ï¼Œå¼€å§‹åˆ†é¡µè·å–: {url}")

        while current_page <= total_pages:
            try:
                logger.debug(f"    âœ æ­£åœ¨è·å–ç¬¬ {current_page} / {total_pages} é¡µ...")
                list_data = tmdb.get_list_details_tmdb(list_id, self.tmdb_api_key, page=current_page)

                if not list_data or not list_data.get('items'):
                    logger.warning(f"  âœ åœ¨ç¬¬ {current_page} é¡µæœªå‘ç°æ›´å¤šé¡¹ç›®ï¼Œè·å–ç»“æŸã€‚")
                    break

                # ä»ç¬¬ä¸€é¡µçš„è¿”å›ç»“æœä¸­æ›´æ–°æ€»é¡µæ•°
                if current_page == 1:
                    total_pages = list_data.get('total_pages', 1)

                for item in list_data['items']:
                    media_type = item.get('media_type')
                    tmdb_id = item.get('id')
                    
                    # å°†TMDbçš„ 'tv' æ˜ å°„ä¸ºæˆ‘ä»¬ç³»ç»Ÿå†…éƒ¨çš„ 'Series'
                    item_type_mapped = 'Series' if media_type == 'tv' else 'Movie'

                    title = item.get('title') if item_type_mapped == 'Movie' else item.get('name')

                    if tmdb_id:
                        all_items.append({
                            'id': str(tmdb_id), 
                            'type': item_type_mapped,
                            'title': title # æ–°å¢å­—æ®µ
                        })

                current_page += 1

            except Exception as e:
                logger.error(f"  âœ è·å–æˆ–è§£æTMDbç‰‡å•é¡µé¢ {current_page} æ—¶å‡ºé”™: {e}")
                break
        
        logger.info(f"  âœ TMDbç‰‡å•è·å–å®Œæˆï¼Œä» {total_pages} ä¸ªé¡µé¢ä¸­æ€»å…±è§£æå‡º {len(all_items)} ä¸ªé¡¹ç›®ã€‚")
        return all_items
    
    def _get_items_from_tmdb_discover(self, url: str) -> List[Dict[str, str]]:
        """ä¸“é—¨ç”¨äºè§£æTMDb Discover URLå¹¶è·å–ç»“æœçš„å‡½æ•°ï¼Œæ”¯æŒè‡ªåŠ¨åˆ†é¡µå¹¶è¿‡æ»¤æ— æµ·æŠ¥/æ— ä¸­æ–‡å…ƒæ•°æ®çš„é¡¹ç›®"""
        from urllib.parse import urlparse, parse_qs
        from datetime import datetime, timedelta
        import re

        logger.info(f"  âœ æ£€æµ‹åˆ°TMDb Discoveré“¾æ¥ï¼Œå¼€å§‹åŠ¨æ€è·å– (æ”¯æŒåˆ†é¡µå’Œè¿‡æ»¤): {url}")
        
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)
        params = {k: v[0] for k, v in query_params.items()}

        today = datetime.now()
        date_pattern = re.compile(r'{today([+-]\d+)?}')

        for key, value in params.items():
            match = date_pattern.search(value)
            if match:
                offset_str = match.group(1) 
                target_date = today
                if offset_str:
                    days = int(offset_str)
                    target_date = today + timedelta(days=days)
                params[key] = value.replace(match.group(0), target_date.strftime('%Y-%m-%d'))

        all_items = []
        current_page = 1
        total_pages = 1
        MAX_PAGES_TO_FETCH = 10

        while current_page <= total_pages and current_page <= MAX_PAGES_TO_FETCH:
            try:
                params['page'] = current_page
                logger.debug(f"    âœ æ­£åœ¨è·å–ç¬¬ {current_page} / {total_pages} é¡µ...")

                discover_data = None
                item_type_for_result = None

                if '/discover/movie' in url:
                    discover_data = tmdb.discover_movie_tmdb(self.tmdb_api_key, params)
                    item_type_for_result = 'Movie'
                elif '/discover/tv' in url:
                    discover_data = tmdb.discover_tv_tmdb(self.tmdb_api_key, params)
                    item_type_for_result = 'Series'
                else:
                    logger.warning(f"  ğŸš« æ— æ³•ä»URL '{url}' åˆ¤æ–­æ˜¯ç”µå½±è¿˜æ˜¯ç”µè§†å‰§ï¼Œdiscoverä»»åŠ¡ä¸­æ­¢ã€‚")
                    break

                if not discover_data or not discover_data.get('results'):
                    logger.info("    âœ åœ¨å½“å‰é¡µæœªå‘ç°æ›´å¤šé¡¹ç›®ï¼Œè·å–ç»“æŸã€‚")
                    break

                if current_page == 1:
                    total_pages = discover_data.get('total_pages', 1)

                for item in discover_data['results']:
                    # ç­›é€‰æ¡ä»¶ 1: å¿…é¡»æœ‰æµ·æŠ¥ (poster_pathä¸ä¸ºNoneæˆ–ç©º)
                    if not item.get('poster_path'):
                        logger.debug(f"  âœ ç­›é€‰TMDb Discoverç»“æœï¼šè·³è¿‡é¡¹ç›® '{item.get('title') or item.get('name')}' (ID: {item.get('id')})ï¼Œå› ä¸ºå®ƒæ²¡æœ‰æµ·æŠ¥ã€‚")
                        continue

                    # ç­›é€‰æ¡ä»¶ 2: å¿…é¡»æœ‰ä¸­æ–‡å…ƒæ•°æ® (ä»¥overviewå­—æ®µä¸ä¸ºç©ºä½œä¸ºåˆ¤æ–­ä¾æ®)
                    # TMDB APIåœ¨æŒ‡å®šlanguage=zh-CNæ—¶ï¼Œè‹¥æ— ä¸­æ–‡ç®€ä»‹ï¼Œæ­¤å­—æ®µé€šå¸¸ä¸ºç©º
                    if not item.get('overview'):
                        logger.debug(f"  âœ ç­›é€‰TMDb Discoverç»“æœï¼šè·³è¿‡é¡¹ç›® '{item.get('title') or item.get('name')}' (ID: {item.get('id')})ï¼Œå› ä¸ºå®ƒæ²¡æœ‰ä¸­æ–‡ç®€ä»‹ã€‚")
                        continue
                    
                    tmdb_id = item.get('id')
                    if tmdb_id and item_type_for_result:
                        title = item.get('title') if item_type_for_result == 'Movie' else item.get('name')
                        date_str = item.get('release_date') if item_type_for_result == 'Movie' else item.get('first_air_date')
                        year = date_str[:4] if date_str else None
                        all_items.append({
                            'id': str(tmdb_id), 
                            'type': item_type_for_result,
                            'title': title,
                            'release_date': date_str, 
                            'year': year
                        })
                
                current_page += 1

            except Exception as e:
                logger.error(f"  âœ è·å–æˆ–è§£æTMDb Discoveré“¾æ¥çš„ç¬¬ {current_page} é¡µæ—¶å‡ºé”™: {e}")
                break

        logger.info(f"  âœ TMDb Discover è·å–å®Œæˆï¼Œä» {total_pages} ä¸ªé¡µé¢ä¸­æ€»å…±è§£æå‡º {len(all_items)} ä¸ªé¡¹ç›®ã€‚")
        return all_items
    
    def _get_titles_and_imdbids_from_url(self, url: str) -> Tuple[List[Dict[str, str]], str]:
        source_type = 'list_rss' 
        items = []

        if 'themoviedb.org/discover/' in url:
            source_type = 'list_discover'
            items = self._get_items_from_tmdb_discover(url)
        elif 'themoviedb.org/list/' in url:
            source_type = 'list_tmdb'
            items = self._get_items_from_tmdb_list(url)
        elif 'douban.com/doulist' in url:
            source_type = 'list_douban'
            items = self._get_items_from_douban_doulist(url)
        else:
            logger.info(f"  âœ å¼€å§‹è·å–æ ‡å‡†RSSæ¦œå•: {url}")
            try:
                response = self.session.get(url, timeout=20)
                response.raise_for_status()
                content = response.text
                if 'encoding="gb2312"' in content.lower():
                     content = response.content.decode('gb2312', errors='ignore')
                
                root = ET.fromstring(content)
                channel = root.find('channel')
                if channel is None: return [], source_type

                for item in channel.findall('item'):
                    title_elem = item.find('title')
                    guid_elem = item.find('guid')
                    link_elem = item.find('link')
                    description_elem = item.find('description')
                    
                    title = title_elem.text if title_elem is not None else None
                    description = description_elem.text if description_elem is not None else ''
                    
                    douban_link = None
                    if link_elem is not None and link_elem.text and 'douban.com' in link_elem.text:
                        douban_link = link_elem.text
                    elif guid_elem is not None and guid_elem.text and 'douban.com' in guid_elem.text:
                        douban_link = guid_elem.text

                    year = None
                    year_match = re.search(r'\b(20\d{2})\b', description)
                    if year_match: year = year_match.group(1)

                    imdb_id = None
                    if guid_elem is not None and guid_elem.text:
                        match = re.search(r'tt\d{7,8}', guid_elem.text)
                        if match: imdb_id = match.group(0)
                    if not imdb_id and link_elem is not None and link_elem.text:
                        match = re.search(r'tt\d{7,8}', link_elem.text)
                        if match: imdb_id = match.group(0)
                    
                    if title:
                        items.append({'title': title.strip(), 'imdb_id': imdb_id, 'year': year, 'douban_link': douban_link})
            except Exception as e:
                logger.error(f"ä»RSS URL '{url}' è·å–æ¦œå•æ—¶å‡ºé”™: {e}")
        
        return items, source_type

    def _match_title_to_tmdb(self, title: str, item_type: str, year: Optional[str] = None) -> Optional[Tuple[str, str, Optional[int]]]:
        """
        - å¢åŠ äº†å¯¹å‰§é›†æœç´¢ç»“æœçš„ç²¾ç¡®éªŒè¯é€»è¾‘ï¼Œé¿å…å› TMDbå­˜åœ¨ä¸è§„èŒƒæ¡ç›®ï¼ˆå¦‚ "å‰§å2"ï¼‰æ—¶ï¼Œé”™è¯¯åœ°åŒ¹é…åˆ°éåŸºç¡€å‰§é›†ã€‚
        - ç»Ÿä¸€äº†è¿”å›å€¼æ ¼å¼ä¸ºå…ƒç»„ (tmdb_id, item_type, season_number)ã€‚
        """
        def normalize_string(s: str) -> str:
            if not s: return ""
            # ç§»é™¤äº†ä¸­æ–‡å¥å·ï¼Œå¢åŠ äº†å¯¹æ›´å¤šåˆ†éš”ç¬¦çš„å…¼å®¹
            return re.sub(r'[\s:ï¼šÂ·\-*\'!,?.ã€‚]+', '', s).lower()

        if item_type == 'Movie':
            # --- ç”µå½±åŒ¹é…é€»è¾‘ä¿æŒä¸å˜ ---
            titles_to_try = set([title.strip()])
            match = re.match(r'([\u4e00-\u9fa5\sÂ·0-9]+)[\s:ï¼š*]*(.*)', title.strip())
            if match:
                part1 = match.group(1).strip()
                part2 = match.group(2).strip()
                if part1: titles_to_try.add(part1)
                if part2: titles_to_try.add(part2)

            num_map = {'1': 'ä¸€', '2': 'äºŒ', '3': 'ä¸‰', '4': 'å››', '5': 'äº”', '6': 'å…­', '7': 'ä¸ƒ', '8': 'å…«', '9': 'ä¹'}
            current_titles = list(titles_to_try) 
            for t in current_titles:
                if any(num in t for num in num_map.keys()):
                    new_title = t
                    for num, char in num_map.items():
                        new_title = new_title.replace(num, char)
                    titles_to_try.add(new_title)
            
            final_titles = list(titles_to_try)
            logger.debug(f"  âœ ä¸º '{title}' ç”Ÿæˆçš„æœ€ç»ˆå€™é€‰æœç´¢æ ‡é¢˜: {final_titles}")

            first_search_results = None
            year_info = f" (å¹´ä»½: {year})" if year else ""

            for title_variation in final_titles:
                if not title_variation: continue
                
                results = search_media(title_variation, self.tmdb_api_key, 'Movie', year=year)
                
                if first_search_results is None:
                    first_search_results = results

                if not results:
                    continue

                norm_variation = normalize_string(title_variation)

                for result in results:
                    norm_title = normalize_string(result.get('title'))
                    norm_original_title = normalize_string(result.get('original_title'))

                    if norm_variation == norm_title or norm_variation == norm_original_title:
                        tmdb_id = str(result.get('id'))
                        logger.info(f"  âœ ç”µå½±æ ‡é¢˜ '{title}'{year_info} é€šè¿‡ã€ç²¾ç¡®è§„èŒƒåŒ¹é…ã€‘(ä½¿ç”¨'{title_variation}') æˆåŠŸåŒ¹é…åˆ°: {result.get('title')} (ID: {tmdb_id})")
                        return tmdb_id, 'Movie', None
                
                for result in results:
                    norm_title = normalize_string(result.get('title'))
                    norm_original_title = normalize_string(result.get('original_title'))

                    if norm_variation in norm_title or norm_variation in norm_original_title:
                        tmdb_id = str(result.get('id'))
                        logger.info(f"  âœ ç”µå½±æ ‡é¢˜ '{title}'{year_info} é€šè¿‡ã€åŒ…å«åŒ¹é…ã€‘(ä½¿ç”¨'{title_variation}') æˆåŠŸåŒ¹é…åˆ°: {result.get('title')} (ID: {tmdb_id})")
                        return tmdb_id, 'Movie', None

            if first_search_results:
                first_result = first_search_results[0]
                tmdb_id = str(first_result.get('id'))
                logger.warning(f"  âœ ç”µå½±æ ‡é¢˜ '{title}'{year_info} æ‰€æœ‰ç²¾ç¡®åŒ¹é…å’ŒåŒ…å«åŒ¹é…å‡å¤±è´¥ã€‚å°†ã€å›é€€ä½¿ç”¨ã€‘æœ€ç›¸å…³çš„æœç´¢ç»“æœ: {first_result.get('title')} (ID: {tmdb_id})")
                return tmdb_id, 'Movie', None

            logger.error(f"  âœ ç”µå½±æ ‡é¢˜ '{title}'{year_info} æœªèƒ½åœ¨TMDbä¸Šæ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœã€‚")
            return None
        
        elif item_type == 'Series':
            # 1. è§£ææ ‡é¢˜
            show_name_parsed, season_number_to_validate = parse_series_title_and_season(title, api_key=self.tmdb_api_key)
            show_name = show_name_parsed if show_name_parsed else title
            
            # 2. æœç´¢
            results = search_media(show_name, self.tmdb_api_key, 'Series', year=year)

            # å›é€€æœç´¢é€»è¾‘ 
            if not results and year and season_number_to_validate is not None:
                logger.debug(f"  âœ å¸¦å¹´ä»½ '{year}' æœç´¢å‰§é›† '{show_name}' æœªæ‰¾åˆ°ç»“æœï¼Œå¯èƒ½æ˜¯åç»­å­£ã€‚å°è¯•ä¸å¸¦å¹´ä»½è¿›è¡Œå›é€€æœç´¢...")
                results = search_media(show_name, self.tmdb_api_key, 'Series', year=None)

            if not results:
                year_info = f" (å¹´ä»½: {year})" if year else ""
                logger.warning(f"  âœ å‰§é›†æ ‡é¢˜ '{title}' (æœç´¢è¯: '{show_name}'){year_info} æœªèƒ½åœ¨TMDbä¸Šæ‰¾åˆ°åŒ¹é…é¡¹ã€‚")
                return None
            
            # æƒ…å†µ A: ä¸éœ€è¦éªŒè¯å­£å· (ç›´æ¥æ‰¾æœ€åƒçš„)
            if season_number_to_validate is None:
                series_result = None
                norm_show_name = normalize_string(show_name)
                
                # ä¼˜å…ˆæ‰¾ç²¾ç¡®åŒ¹é…
                for result in results:
                    if normalize_string(result.get('name', '')) == norm_show_name:
                        series_result = result
                        logger.debug(f"  âœ å‰§é›† '{show_name}' é€šè¿‡ã€ç²¾ç¡®è§„èŒƒåŒ¹é…ã€‘æ‰¾åˆ°äº†: {result.get('name')} (ID: {result.get('id')})")
                        break 
                
                # æ²¡æ‰¾åˆ°ç²¾ç¡®çš„å°±ç”¨ç¬¬ä¸€ä¸ª
                if not series_result:
                    series_result = results[0]
                    logger.warning(f"  âœ å‰§é›† '{show_name}' æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨é¦–ä¸ªç»“æœ: {series_result.get('name')} (ID: {series_result.get('id')})")

                return str(series_result.get('id')), 'Series', None

            # æƒ…å†µ B: éœ€è¦éªŒè¯å­£å· (éå†å‰ 5 ä¸ªç»“æœï¼Œè°æœ‰è¿™ä¸€å­£å°±ç®—è°çš„)
            else:
                # å®šä¹‰ä¸€ä¸ªå†…éƒ¨å‡½æ•°æ¥æ‰§è¡ŒéªŒè¯é€»è¾‘ï¼Œé¿å…ä»£ç é‡å¤
                def verify_season_in_results(candidates_list, source_desc=""):
                    if not candidates_list:
                        return None
                    
                    # æ’åºï¼šåå­—å®Œå…¨åŒ¹é…çš„æ’åœ¨æœ€å‰é¢
                    norm_show_name = normalize_string(show_name)
                    candidates_list.sort(key=lambda x: 0 if normalize_string(x.get('name', '')) == norm_show_name else 1)
                    
                    logger.info(f"  âœ å‰§é›† '{show_name}'{source_desc} éœ€è¦éªŒè¯ç¬¬ {season_number_to_validate} å­£ï¼Œæ­£åœ¨æ‰«æ {len(candidates_list)} ä¸ªå€™é€‰ç»“æœ...")

                    for candidate in candidates_list:
                        candidate_id = str(candidate.get('id'))
                        candidate_name = candidate.get('name')
                        
                        # è·å–è¯¥å‰§é›†çš„è¯¦æƒ…ï¼ˆåŒ…å«å­£ä¿¡æ¯ï¼‰
                        series_details = get_tv_details(int(candidate_id), self.tmdb_api_key, append_to_response="seasons")
                        
                        if series_details and 'seasons' in series_details:
                            # æ£€æŸ¥æ˜¯å¦æœ‰ç›®æ ‡å­£
                            has_season = False
                            for season in series_details['seasons']:
                                if season.get('season_number') == season_number_to_validate:
                                    has_season = True
                                    break
                            
                            if has_season:
                                logger.info(f"  âœ åŒ¹é…æˆåŠŸï¼åœ¨å€™é€‰ç»“æœ '{candidate_name}' (ID: {candidate_id}) ä¸­æ‰¾åˆ°äº†ç¬¬ {season_number_to_validate} å­£ã€‚")
                                return candidate_id
                            else:
                                logger.debug(f"    - å€™é€‰ '{candidate_name}' (ID: {candidate_id}) æ²¡æœ‰ç¬¬ {season_number_to_validate} å­£ï¼Œè·³è¿‡ã€‚")
                    return None

                # 1. ç¬¬ä¸€æ¬¡å°è¯•ï¼šä½¿ç”¨å½“å‰çš„æœç´¢ç»“æœï¼ˆå¯èƒ½å¸¦å¹´ä»½ï¼‰
                matched_id = verify_season_in_results(results[:5])
                if matched_id:
                    return matched_id, 'Series', season_number_to_validate

                # 2. å¦‚æœå¸¦å¹´ä»½éªŒè¯å¤±è´¥ï¼Œå°è¯•å»æ‰å¹´ä»½é‡æœ 
                if year:
                    logger.info(f"  âœ å‰§é›† '{show_name}' å¸¦å¹´ä»½ ({year}) æœç´¢ç»“æœä¸­æœªæ‰¾åˆ°ç¬¬ {season_number_to_validate} å­£ï¼Œå°è¯•ç§»é™¤å¹´ä»½é‡æœ...")
                    results_no_year = search_media(show_name, self.tmdb_api_key, 'Series', year=None)
                    
                    if results_no_year:
                        # æ’é™¤æ‰å·²ç»åœ¨ç¬¬ä¸€æ¬¡æœç´¢ä¸­éªŒè¯è¿‡çš„IDï¼Œé¿å…é‡å¤APIè¯·æ±‚
                        checked_ids = set(str(r.get('id')) for r in results[:5])
                        candidates_no_year = [r for r in results_no_year if str(r.get('id')) not in checked_ids][:5]
                        
                        if candidates_no_year:
                            matched_id = verify_season_in_results(candidates_no_year, source_desc=" (æ— å¹´ä»½é‡æœ)")
                            if matched_id:
                                return matched_id, 'Series', season_number_to_validate

                # 3. å¦‚æœå¾ªç¯å®Œäº†éƒ½æ²¡æ‰¾åˆ°
                logger.warning(f"  âœ éªŒè¯å¤±è´¥ï¼åœ¨ '{show_name}' çš„æ‰€æœ‰æœç´¢ç»“æœä¸­ï¼Œå‡æœªæ‰¾åˆ°ç¬¬ {season_number_to_validate} å­£ã€‚")
                    
                # ==============================================================================
                # â˜…â˜…â˜… å…œåº•å›é€€æœºåˆ¶ â˜…â˜…â˜…
                # å¦‚æœè§£æåçš„æœç´¢+éªŒè¯å¤±è´¥äº†ï¼Œå°è¯•ç”¨â€œåŸå§‹æ ‡é¢˜â€ç›´æ¥æœä¸€æ¬¡
                # ==============================================================================
                if show_name != title:
                    logger.info(f"  âœ [å…œåº•æœºåˆ¶] å°è¯•ä½¿ç”¨åŸå§‹æ ‡é¢˜ '{title}' è¿›è¡Œå›é€€æœç´¢...")
                    fallback_results = search_media(title, self.tmdb_api_key, 'Series', year=None)
                    
                    if fallback_results:
                        # å¦‚æœåŸå§‹æ ‡é¢˜èƒ½æœåˆ°ç»“æœï¼Œé€šå¸¸ç¬¬ä¸€ä¸ªå°±æ˜¯æœ€åŒ¹é…çš„
                        # è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨ï¼šè§£æå™¨é”™è¯¯åœ°æŠŠå‰§åçš„ä¸€éƒ¨åˆ†å½“æˆäº†å­£å·
                        # ä¾‹å¦‚ï¼š"Love 101" è¢«è§£ææˆäº† "Love" ç¬¬ 101 å­£ï¼ŒéªŒè¯å¤±è´¥ -> å›é€€æœ "Love 101" -> æˆåŠŸ
                        best_match = fallback_results[0]
                        logger.info(f"  âœ [å…œåº•æˆåŠŸ] åŸå§‹æ ‡é¢˜ '{title}' åŒ¹é…åˆ°äº†: {best_match.get('name')} (ID: {best_match.get('id')})")
                        return str(best_match.get('id')), 'Series', None
            
            return None
                
        return None

    def process(self, definition: Dict) -> Tuple[List[Dict[str, str]], str]:
        raw_url = definition.get('url')
        urls = []
        if isinstance(raw_url, list):
            urls = [u for u in raw_url if u]
        elif isinstance(raw_url, str) and raw_url:
            urls = [raw_url]
            
        if not urls: return [], 'empty'
        
        all_items = []
        last_source_type = 'mixed'
        
        # å¾ªç¯è°ƒç”¨æ—§é€»è¾‘
        total_urls = len(urls)
        for i, url in enumerate(urls):
            # æ„é€ ä¸´æ—¶å®šä¹‰ï¼ŒåªåŒ…å«å•ä¸ª URL
            temp_def = definition.copy()
            temp_def['url'] = url
            
            # è°ƒç”¨åŸé€»è¾‘
            items, source_type = self._process_single_url(url, temp_def)
            all_items.extend(items)
            last_source_type = source_type
            
            # â˜…â˜…â˜… æ–°å¢ï¼šå¤šæ¦œå•é—´çš„é˜²å°æ§ä¼‘çœ  â˜…â˜…â˜…
            # å¦‚æœå½“å‰æ˜¯çŒ«çœ¼é“¾æ¥ï¼Œä¸”ä¸æ˜¯åˆ—è¡¨ä¸­çš„æœ€åä¸€ä¸ªï¼Œåˆ™å¼ºåˆ¶ä¼‘çœ 
            if isinstance(url, str) and url.startswith('maoyan://'):
                if i < total_urls - 1:
                    logger.info(f"  âœ [é˜²å°æ§] å•ä¸ªçŒ«çœ¼æ¦œå•é‡‡é›†å®Œæ¯•ï¼Œä¸ºå®‰å…¨èµ·è§ï¼Œå¼ºåˆ¶ä¼‘çœ  10 ç§’åå†é‡‡é›†ä¸‹ä¸€ä¸ª...")
                    time.sleep(10)
            
        # ç»Ÿä¸€å»é‡
        unique_items = []
        seen_keys = set()
        for item in all_items:
            tmdb_id = item.get('id')
            item_type = item.get('type')
            title = item.get('title')
            season = item.get('season')
            
            if tmdb_id:
                key = f"{item_type}-{tmdb_id}-{season}"
            else:
                key = f"unidentified-{title}"
            
            if key not in seen_keys:
                seen_keys.add(key)
                unique_items.append(item)
        
        # ç»Ÿä¸€é™åˆ¶æ•°é‡
        limit = definition.get('limit')
        if limit and isinstance(limit, int) and limit > 0:
            unique_items = unique_items[:limit]
            
        # ==================== â˜…â˜…â˜… AI è¿‡æ»¤æ’ä»¶ (æœ€ç»ˆç‰ˆ) â˜…â˜…â˜… ====================
        if definition.get('ai_enabled') and definition.get('ai_prompt'):
            ai_prompt = definition.get('ai_prompt')
            logger.info(f"  âœ [AIå®¡é˜…] æ£€æµ‹åˆ° AI é€‰ç‰‡æŒ‡ä»¤ï¼Œæ­£åœ¨ç­›é€‰ {len(unique_items)} ä¸ªå€™é€‰é¡¹ç›®...")
            
            try:
                # 1. å®ä¾‹åŒ– AI (æ³¨æ„è¿™é‡Œå¿…é¡»æ˜¯ APP_CONFIG)
                translator = AITranslator(config_manager.APP_CONFIG)
                
                # 2. å‡†å¤‡ç²¾ç®€æ•°æ® (å…³é”®ï¼šä¸€å®šè¦å¸¦ä¸Š release_date å’Œ year)
                candidates_for_ai = []
                for item in unique_items:
                    candidates_for_ai.append({
                        "id": str(item.get('id')),
                        "title": item.get('title'),
                        "type": item.get('type'),
                        "year": item.get('year'),                 # <--- åˆ«æ¼äº†
                        "release_date": item.get('release_date')  # <--- æ ¸å¿ƒï¼åˆ«æ¼äº†
                    })
                
                # 3. è°ƒç”¨ AI è¿‡æ»¤
                filtered_ids = translator.filter_candidates(
                    candidates=candidates_for_ai, 
                    user_instruction=ai_prompt
                )
                
                # 4. åº”ç”¨è¿‡æ»¤ç»“æœ
                if filtered_ids:
                    original_count = len(unique_items)
                    unique_items = [item for item in unique_items if str(item.get('id')) in filtered_ids]
                    logger.info(f"  âœ [AIå®¡é˜…] å®Œæˆã€‚ä» {original_count} éƒ¨ä¸­ç­›é€‰å‡º {len(unique_items)} éƒ¨ã€‚")
                else:
                    logger.warning("  âœ [AIå®¡é˜…] AI è¿”å›äº†ç©ºåˆ—è¡¨æˆ–è¿‡æ»¤å¤±è´¥ï¼Œå°†ä¿ç•™åŸå§‹åˆ—è¡¨ã€‚")
            
            except Exception as e_ai:
                logger.error(f"  âœ [AIå®¡é˜…] æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œè·³è¿‡ç­›é€‰: {e_ai}", exc_info=True)
        # ===================================================================
        
        return unique_items, last_source_type

    def _process_single_url(self, url: str, definition: Dict) -> Tuple[List[Dict[str, str]], str]:
        definition = definition.copy()
        definition['url'] = url
        source_type = 'list_rss'
        
        if not url:
            return [], source_type
            

        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ­£ï¼šåœ¨è¿™é‡Œç›´æ¥å¤„ç†çŒ«çœ¼é€»è¾‘ â˜…â˜…â˜…
        if url.startswith('maoyan://'):
            source_type = 'list_maoyan'
            logger.info(f"  âœ æ£€æµ‹åˆ°çŒ«çœ¼æ¦œå•ï¼Œå°†å¯åŠ¨å¼‚æ­¥åå°è„šæœ¬...")
            # ä½¿ç”¨ gevent å¼‚æ­¥æ‰§è¡Œè€—æ—¶çš„å­è¿›ç¨‹è°ƒç”¨
            greenlet = gevent.spawn(self._execute_maoyan_fetch, definition)
            # .get() ä¼šç­‰å¾… greenlet æ‰§è¡Œå®Œæ¯•å¹¶è¿”å›ç»“æœ
            tmdb_items = greenlet.get()
            return tmdb_items, source_type

        # --- å¯¹äºéçŒ«çœ¼æ¦œå•ï¼Œä¿æŒåŸæœ‰é€»è¾‘ä¸å˜ ---
        item_types = definition.get('item_type', ['Movie'])
        if isinstance(item_types, str): item_types = [item_types]
        limit = definition.get('limit')
        
        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 2/2: æ¥æ”¶ _get_titles_and_imdbids_from_url è¿”å›çš„ source_type â˜…â˜…â˜…
        items, source_type = self._get_titles_and_imdbids_from_url(url)
        
        if not items: return [], source_type
        
        if items and 'id' in items[0] and 'type' in items[0]:
            logger.info(f"  âœ æ£€æµ‹åˆ°æ¥è‡ªTMDbæº ({source_type}) çš„é¢„åŒ¹é…IDï¼Œå°†è·³è¿‡æ ‡é¢˜åŒ¹é…ã€‚")
            if limit and isinstance(limit, int) and limit > 0:
                items = items[:limit]
            return items, source_type # ç›´æ¥è¿”å›ç»“æœå’Œç±»å‹

        if limit and isinstance(limit, int) and limit > 0:
            items = items[:limit]
        
        tmdb_items = []
        douban_api = DoubanApi()

        with ThreadPoolExecutor(max_workers=5) as executor:
            def find_first_match(item: Dict[str, str], types_to_check):
                original_source_title = item.get('title', '').strip()
                year = item.get('year')
                rss_imdb_id = item.get('imdb_id')
                douban_link = item.get('douban_link')

                # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 1ï¼šå®šä¹‰ä¸€ä¸ªåŒ…å«åŸå§‹æ ‡é¢˜çš„è¾…åŠ©å‡½æ•° â˜…â˜…â˜…
                def create_result(tmdb_id, item_type, confirmed_season=None):
                    result = {
                        'id': tmdb_id, 
                        'type': item_type, 
                        'title': original_source_title,
                        'year': year
                    }
                    # åªæœ‰å½“ä¼ å…¥äº†æœ‰æ•ˆçš„å­£å·æ—¶ï¼Œæ‰æ·»åŠ 
                    if item_type == 'Series' and confirmed_season is not None:
                        result['season'] = confirmed_season
                    return result

                # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 2ï¼šé»˜è®¤çš„ fallback ä¹Ÿè¦å¸¦ä¸ŠåŸå§‹æ ‡é¢˜ â˜…â˜…â˜…
                fallback_result = {
                    'id': None, 
                    'type': types_to_check[0] if types_to_check else 'Movie', 
                    'title': original_source_title,
                    'year': year
                }

                # 1. å°è¯• IMDb ID
                if rss_imdb_id:
                    for item_type in types_to_check:
                        tmdb_id = self._match_by_ids(rss_imdb_id, None, item_type)
                        if tmdb_id:
                            _, s_num = parse_series_title_and_season(original_source_title, api_key=self.tmdb_api_key)
                            return create_result(tmdb_id, item_type, s_num)

                # 2. å°è¯• æ ‡é¢˜åŒ¹é…
                cleaned_title = re.sub(r'^\s*\d+\.\s*', '', original_source_title)
                cleaned_title = re.sub(r'\s*\(\d{4}\)$', '', cleaned_title).strip()
                
                for item_type in types_to_check:
                    match_result = self._match_title_to_tmdb(cleaned_title, item_type, year=year)
                    
                    if match_result:
                        tmdb_id, matched_type, matched_season = match_result
                        return create_result(tmdb_id, matched_type, matched_season)
                
                # 3. å°è¯• è±†ç“£é“¾æ¥
                if douban_link:
                    logger.info(f"  âœ ç‰‡å+å¹´ä»½åŒ¹é… '{original_source_title}' å¤±è´¥ï¼Œå¯åŠ¨å¤‡ç”¨æ–¹æ¡ˆï¼šé€šè¿‡è±†ç“£é“¾æ¥è·å–æ›´å¤šä¿¡æ¯...")
                    douban_details = douban_api.get_details_from_douban_link(douban_link, mtype=types_to_check[0] if types_to_check else None)
                    
                    if douban_details:
                        imdb_id_from_douban = douban_details.get("imdb_id")
                        if not imdb_id_from_douban and douban_details.get("attrs", {}).get("imdb"):
                            imdb_ids = douban_details["attrs"]["imdb"]
                            if isinstance(imdb_ids, list) and len(imdb_ids) > 0:
                                imdb_id_from_douban = imdb_ids[0]

                        if imdb_id_from_douban:
                            logger.info(f"  âœ è±†ç“£å¤‡ç”¨æ–¹æ¡ˆ(3a)æˆåŠŸï¼æ‹¿åˆ°IMDb ID: {imdb_id_from_douban}ï¼Œç°åœ¨ç”¨å®ƒåŒ¹é…TMDb...")
                            for item_type in types_to_check:
                                tmdb_id = self._match_by_ids(imdb_id_from_douban, None, item_type)
                                if tmdb_id:
                                    return create_result(tmdb_id, item_type)
                        
                        logger.info(f"  âœ è±†ç“£å¤‡ç”¨æ–¹æ¡ˆ(3a)å¤±è´¥ï¼Œå°è¯•æ–¹æ¡ˆ(3b): ä½¿ç”¨ original_title...")
                        original_title = douban_details.get("original_title")
                        if original_title:
                            for item_type in types_to_check:
                                match_result = self._match_title_to_tmdb(original_title, item_type, year=year)
                                if match_result:
                                    tmdb_id, matched_type, matched_season = match_result
                                    logger.info(f"  âœ è±†ç“£å¤‡ç”¨æ–¹æ¡ˆ(3b)æˆåŠŸï¼é€šè¿‡ original_title '{original_title}' åŒ¹é…æˆåŠŸã€‚")
                                    return create_result(tmdb_id, matched_type, matched_season)

                logger.debug(f"  âœ æ‰€æœ‰ä¼˜å…ˆæ–¹æ¡ˆå‡å¤±è´¥ï¼Œå°è¯•ä¸å¸¦å¹´ä»½è¿›è¡Œæœ€åçš„å›é€€æœç´¢: '{original_source_title}'")
                for item_type in types_to_check:
                    match_result = self._match_title_to_tmdb(cleaned_title, item_type, year=None)
                    if match_result:
                        tmdb_id, matched_type, matched_season = match_result
                        logger.warning(f"  âœ æ³¨æ„ï¼š'{original_source_title}' åœ¨æœ€åçš„å›é€€æœç´¢ä¸­åŒ¹é…æˆåŠŸï¼Œä½†å¹´ä»½å¯èƒ½ä¸å‡†ã€‚")
                        return create_result(tmdb_id, matched_type, matched_season)

                logger.error(f"  âœ å½»åº•å¤±è´¥ï¼šæ‰€æœ‰æ–¹æ¡ˆéƒ½æ— æ³•ä¸º '{original_source_title}' æ‰¾åˆ°åŒ¹é…é¡¹ã€‚")
                return fallback_result

            results_in_order = executor.map(lambda item: find_first_match(item, item_types), items)
            tmdb_items = [result for result in results_in_order if result is not None]
        
        douban_api.close()
        logger.info(f"  âœ RSSåŒ¹é…å®Œæˆï¼ŒæˆåŠŸè·å¾— {len(tmdb_items)} ä¸ªTMDbé¡¹ç›®ã€‚")
        
        unique_items = []
        seen_keys = set()
        
        for item in tmdb_items:
            tmdb_id = item.get('id')
            item_type = item.get('type')
            title = item.get('title')
            season = item.get('season')
            
            if tmdb_id:
                # 1. å¦‚æœæœ‰ IDï¼Œä¼˜å…ˆç”¨ ID + å­£å·å»é‡ (é˜²æ­¢åŒä¸€å‰§é›†ä¸åŒå­£è¢«å»é‡)
                # ä¾‹å¦‚: Series-12345-1, Series-12345-2
                key = f"{item_type}-{tmdb_id}-{season}"
            else:
                # 2. å¦‚æœæ²¡æœ‰ IDï¼Œå¿…é¡»ç”¨ æ ‡é¢˜ å»é‡ï¼
                # ä¾‹å¦‚: unidentified-æ€ªå¥‡ç‰©è¯­ ç¬¬äº”å­£
                # è¿™æ ·ã€Šæ€ªå¥‡ç‰©è¯­ã€‹å’Œã€Šé»‘è¢çº å¯Ÿé˜Ÿã€‹å°±ä¸ä¼šå› ä¸ºéƒ½æ˜¯ None è€Œæ‰“æ¶äº†
                key = f"unidentified-{title}"
            
            if key not in seen_keys:
                seen_keys.add(key)
                unique_items.append(item)
                
        logger.info(f"  âœ å»é‡åå‰©ä½™ {len(unique_items)} ä¸ªæœ‰æ•ˆé¡¹ç›®ã€‚")

        return unique_items, source_type

class FilterEngine:
    """
    ã€V4 - PG JSON å…¼å®¹æœ€ç»ˆç‰ˆã€‘
    - ä¿®å¤äº† _item_matches_rules æ–¹æ³•ä¸­å›  psycopg2 è‡ªåŠ¨è§£æ JSON å­—æ®µè€Œå¯¼è‡´çš„ TypeErrorã€‚
    - ç§»é™¤äº†æ‰€æœ‰å¯¹ _json å­—æ®µçš„å¤šä½™ json.loads() è°ƒç”¨ï¼Œè§£å†³äº†ç­›é€‰è§„åˆ™é™é»˜å¤±æ•ˆçš„é—®é¢˜ã€‚
    """
    def __init__(self):
        self.airing_series_ids = None
        self.series_runtime_cache = {}

    def _get_airing_ids(self): # â—€â—€â—€ å‡½æ•°åä¹Ÿæ”¹äº†
        """è¾…åŠ©å‡½æ•°ï¼Œå¸¦ç¼“å­˜åœ°è·å–è¿è½½ä¸­ID"""
        if self.airing_series_ids is None:
            logger.debug("  âœ ç­›é€‰å¼•æ“ï¼šé¦–æ¬¡éœ€è¦â€œè¿è½½ä¸­â€æ•°æ®ï¼Œæ­£åœ¨ä»æ•°æ®åº“æŸ¥è¯¢...")
            self.airing_series_ids = watchlist_db.get_airing_series_tmdb_ids()
            logger.debug(f"  âœ ç¼“å­˜äº† {len(self.airing_series_ids)} ä¸ªâ€œè¿è½½ä¸­â€å‰§é›†IDã€‚")
        return self.airing_series_ids

    def _item_matches_rules(self, item_metadata: Dict[str, Any], rules: List[Dict[str, Any]], logic: str) -> bool:
        if not rules: return True
        
        results = []
        for rule in rules:
            field, op, value = rule.get("field"), rule.get("operator"), rule.get("value")
            match = False
            
            # 1. å¤„ç†åˆ—è¡¨å­—æ®µ
            if field in ['actors', 'directors', 'genres', 'countries', 'studios', 'tags', 'keywords']:
                item_value_list = item_metadata.get(f"{field}_json")
                if not item_value_list or not isinstance(item_value_list, list):
                    results.append(False)
                    continue

                values_to_check = item_value_list
                if op == 'is_primary':
                    if field == 'actors':
                        values_to_check = item_value_list[:3] #æ¼”å‘˜åªå–å‰3
                    else:
                        values_to_check = item_value_list[:1]
                
                try:
                    if field in ['actors', 'directors']:
                        if not isinstance(value, list):
                            results.append(False); continue
                        
                        rule_person_ids = set(str(p['id']) for p in value if isinstance(p, dict) and 'id' in p)
                        if not rule_person_ids:
                            results.append(False); continue

                        item_person_ids = set()
                        for p in values_to_check:
                            person_id = p.get('tmdb_id') or p.get('id') # <-- ä¿®æ­£ç‚¹ï¼
                            if person_id is not None:
                                item_person_ids.add(str(person_id))
                        
                        if op in ['is_one_of', 'contains', 'is_primary']:
                            if not rule_person_ids.isdisjoint(item_person_ids):
                                match = True
                        elif op == 'is_none_of':
                            if rule_person_ids.isdisjoint(item_person_ids):
                                match = True
                    
                    # å¤„ç†å…¶ä»–æ™®é€šåˆ—è¡¨å­—æ®µ
                    else:
                        if op == 'is_primary':
                            if values_to_check and values_to_check[0] == value:
                                match = True
                        elif op == 'is_one_of':
                            if isinstance(value, list) and any(v in values_to_check for v in value):
                                match = True
                        elif op == 'is_none_of':
                            if isinstance(value, list) and not any(v in values_to_check for v in value):
                                match = True
                        elif op == 'contains':
                            if value in values_to_check:
                                match = True

                except (TypeError, KeyError) as e:
                    logger.warning(f"  âœ å¤„ç† {field}_json æ—¶é‡åˆ°æ„å¤–çš„æ ¼å¼é”™è¯¯: {e}, å†…å®¹: {item_value_list}")

            # 2. å¤„ç†å…¶ä»–æ‰€æœ‰éåˆ—è¡¨å­—æ®µ
            elif field in ['release_date', 'date_added']:
                item_date_val = item_metadata.get(field)
                if item_date_val and str(value).isdigit():
                    try:
                        if isinstance(item_date_val, datetime):
                            item_date = item_date_val.date()
                        elif isinstance(item_date_val, date):
                            item_date = item_date_val
                        else:
                            item_date = datetime.strptime(str(item_date_val), '%Y-%m-%d').date()

                        today = datetime.now().date()
                        days = int(value)
                        cutoff_date = today - timedelta(days=days)

                        if op == 'in_last_days':
                            if cutoff_date <= item_date <= today:
                                match = True
                        elif op == 'not_in_last_days':
                            if item_date < cutoff_date:
                                match = True
                    except (ValueError, TypeError):
                        pass

            # 3. å¤„ç†åˆ†çº§å­—æ®µ
            elif field == 'unified_rating':
                item_unified_rating = item_metadata.get('unified_rating')
                if item_unified_rating:
                    if op == 'is_one_of':
                        if isinstance(value, list) and item_unified_rating in value:
                            match = True
                    elif op == 'is_none_of':
                        if isinstance(value, list) and item_unified_rating not in value:
                            match = True
                    elif op == 'eq':
                        if str(value) == item_unified_rating:
                            match = True

            # 5. å¤„ç†è¿è½½å‰§é›†
            elif field == 'is_in_progress':
                if item_metadata.get('item_type') == 'Series':
                    airing_ids = self._get_airing_ids()
                    is_item_airing = str(item_metadata.get('tmdb_id')) in airing_ids

                    if (op == 'is' and value is True) or (op == 'is_not' and value is False):
                        if is_item_airing:
                            match = True
                    elif (op == 'is' and value is False) or (op == 'is_not' and value is True):
                        if not is_item_airing:
                            match = True

            elif field == 'title':
                item_title = item_metadata.get('title')
                if item_title and isinstance(value, str):
                    item_title_lower = item_title.lower()
                    value_lower = value.lower()
                    if op == 'contains':
                        if value_lower in item_title_lower: match = True
                    elif op == 'does_not_contain':
                        if value_lower not in item_title_lower: match = True
                    elif op == 'starts_with':
                        if item_title_lower.startswith(value_lower): match = True
                    elif op == 'ends_with':
                        if item_title_lower.endswith(value_lower): match = True
            
            # å¤„ç†æ—¶é•¿ç­›é€‰ 
            elif field == 'runtime':
                try:
                    threshold_minutes = float(value)
                    item_runtime = 0.0
                    
                    # 1. ç”µå½±å¤„ç†é€»è¾‘ (ä¸å˜)
                    if item_metadata.get('item_type') == 'Movie':
                        item_runtime = float(item_metadata.get('runtime_minutes') or 0)
                        if item_runtime <= 0:
                            assets = item_metadata.get('asset_details_json')
                            if assets and isinstance(assets, list) and len(assets) > 0:
                                item_runtime = float(assets[0].get('runtime_minutes') or 0)

                    # 2. å‰§é›†å¤„ç†é€»è¾‘ (â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ â˜…â˜…â˜…)
                    elif item_metadata.get('item_type') == 'Series':
                        tmdb_id = str(item_metadata.get('tmdb_id'))
                        
                        # A. ä¼˜å…ˆå°è¯•ä»ç¼“å­˜è·å– (ç”¨äºæ‰¹é‡ç”Ÿæˆä»»åŠ¡ï¼Œæé€Ÿ)
                        if self.series_runtime_cache and tmdb_id in self.series_runtime_cache:
                            item_runtime = self.series_runtime_cache[tmdb_id]
                        
                        # B. å¦‚æœç¼“å­˜æœªå‘½ä¸­ï¼Œè¯´æ˜æ˜¯å®æ—¶å…¥åº“åŒ¹é… (å•æ¬¡æŸ¥è¯¢ï¼Œæ€§èƒ½æ— æŸ)
                        else:
                            # è°ƒç”¨æˆ‘ä»¬åœ¨ç¬¬ä¸€æ­¥ä¸­æ·»åŠ çš„å•é¡¹æŸ¥è¯¢å‡½æ•°
                            item_runtime = media_db.get_series_average_runtime(tmdb_id)
                            logger.debug(f"    âœ [å®æ—¶ç­›é€‰] å‰§é›† {tmdb_id} å®æ—¶è®¡ç®—å¹³å‡æ—¶é•¿: {item_runtime} åˆ†é’Ÿ")
                    
                    # 3. æ‰§è¡Œæ¯”è¾ƒ (ä¸å˜)
                    if op == 'gte': 
                        if item_runtime >= threshold_minutes: match = True
                    elif op == 'lte': 
                        if item_runtime > 0 and item_runtime <= threshold_minutes: match = True
                        
                except (ValueError, TypeError):
                    pass

            else:
                actual_item_value = item_metadata.get(field)
                if actual_item_value is not None:
                    try:
                        if op == 'gte' and float(actual_item_value) >= float(value): match = True
                        elif op == 'lte' and float(actual_item_value) <= float(value): match = True
                        elif op == 'eq' and str(actual_item_value) == str(value): match = True
                    except (ValueError, TypeError): pass

            results.append(match)

        if logic.upper() == 'AND': return all(results)
        else: return any(results)

    def execute_filter(self, definition: Dict[str, Any]) -> List[Dict[str, str]]:
        logger.info("  âœ ç­›é€‰å¼•æ“ï¼šå¼€å§‹æ‰§è¡Œåˆé›†ç”Ÿæˆ...")
        rules = definition.get('rules', [])
        logic = definition.get('logic', 'AND')
        item_types_to_process = definition.get('item_type', ['Movie'])
        if isinstance(item_types_to_process, str):
            item_types_to_process = [item_types_to_process]
        if not rules:
            logger.warning("  âœ åˆé›†å®šä¹‰ä¸­æ²¡æœ‰ä»»ä½•è§„åˆ™ï¼Œå°†è¿”å›ç©ºåˆ—è¡¨ã€‚")
            return []

        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šæ ¹æ®å®šä¹‰åˆ¤æ–­æ•°æ®æº â˜…â˜…â˜…
        library_ids = definition.get('library_ids') # åœ¨æ–°ç‰ˆUIä¸­ï¼Œè¿™ä¸ªå­—æ®µå« target_library_idsï¼Œä½†æˆ‘ä»¬å…¼å®¹æ—§çš„
        if not library_ids:
            library_ids = definition.get('target_library_ids')

        all_media_metadata = []

        # æŒ‡å®šåª’ä½“åº“
        if library_ids and isinstance(library_ids, list) and len(library_ids) > 0:
            logger.info(f"  âœ å·²æŒ‡å®š {len(library_ids)} ä¸ªåª’ä½“åº“ï¼Œæ­£åœ¨ä»æœ¬åœ°æ•°æ®åº“ç­›é€‰...")
            
            # 1. è·å–ç¬¦åˆåº“æ¡ä»¶çš„ TMDB ID é›†åˆ
            tmdb_ids_in_libs = collection_db.get_tmdb_ids_by_library_ids(library_ids)
            
            if not tmdb_ids_in_libs:
                logger.warning("  âœ æŒ‡å®šçš„åª’ä½“åº“ä¸­æœªæ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¡ä»¶çš„åª’ä½“é¡¹ã€‚")
                return []

            # 2. æ‰¹é‡è·å–è¿™äº› ID çš„è¯¦ç»†å…ƒæ•°æ®
            # æ³¨æ„ï¼šget_media_details_by_tmdb_ids è¿”å›çš„æ˜¯ {tmdb_id: metadata} å­—å…¸
            media_metadata_map = media_db.get_media_details_by_tmdb_ids(list(tmdb_ids_in_libs))
            
            # 3. æŒ‰éœ€è¦çš„ç±»å‹è¿‡æ»¤å¹¶æ·»åŠ åˆ°æ€»åˆ—è¡¨
            for item_type in item_types_to_process:
                metadata_for_type = [
                    meta for meta in media_metadata_map.values() 
                    if meta.get('item_type') == item_type
                ]
                all_media_metadata.extend(metadata_for_type)
        # æœªæŒ‡å®šåª’ä½“åº“
        else:
            # --- åˆ†æ”¯2ï¼šä¿æŒåŸæœ‰é€»è¾‘ï¼Œæ‰«æå…¨åº“ ---
            logger.info("  âœ æœªæŒ‡å®šåª’ä½“åº“ï¼Œå°†æ‰«ææ‰€æœ‰åª’ä½“åº“çš„å…ƒæ•°æ®ç¼“å­˜...")
            for item_type in item_types_to_process:
                all_media_metadata.extend(media_db.get_all_media_metadata(item_type=item_type))

        if all_media_metadata:
            # 1. æŒ‘å‡ºæ‰€æœ‰å‰§é›†çš„ TMDB ID
            series_ids_to_fetch = [
                str(m['tmdb_id']) 
                for m in all_media_metadata 
                if m.get('item_type') == 'Series' and m.get('tmdb_id')
            ]
            
            # 2. å¦‚æœæœ¬æ¬¡ç­›é€‰åŒ…å«å‰§é›†ï¼Œä¸”è§„åˆ™é‡Œæœ‰æ—¶é•¿ç­›é€‰ï¼Œåˆ™è¿›è¡Œç²¾å‡†é¢„å–
            # (ä¸ºäº†ç®€å•ç¨³å¥ï¼Œåªè¦æœ‰å‰§é›†å°±é¢„å–ï¼Œå¼€é”€æå°)
            if series_ids_to_fetch:
                logger.info(f"  âœ æ­£åœ¨ä¸ºæœ¬æ¬¡ç­›é€‰èŒƒå›´å†…çš„ {len(series_ids_to_fetch)} éƒ¨å‰§é›†ç²¾å‡†è®¡ç®—å¹³å‡æ—¶é•¿...")
                start_time = datetime.now()
                self.series_runtime_cache = media_db.get_runtimes_for_series_list(series_ids_to_fetch)
                duration = (datetime.now() - start_time).total_seconds()
                logger.info(f"  âœ æ—¶é•¿è®¡ç®—å®Œæˆï¼Œè€—æ—¶ {duration:.3f}ç§’ã€‚")
        
        # --- åç»­çš„ç­›é€‰é€»è¾‘ä¿æŒä¸å˜ ---
        matched_items = []
        if not all_media_metadata:
            logger.warning("  âœ æœªèƒ½åŠ è½½ä»»ä½•åª’ä½“å…ƒæ•°æ®è¿›è¡Œç­›é€‰ã€‚")
            return []
        
        logger.info(f"  âœ å·²åŠ è½½ {len(all_media_metadata)} æ¡å…ƒæ•°æ®ï¼Œå¼€å§‹åº”ç”¨ç­›é€‰è§„åˆ™...")
        for media_metadata in all_media_metadata:
            if self._item_matches_rules(media_metadata, rules, logic):
                tmdb_id = media_metadata.get('tmdb_id')
                item_type = media_metadata.get('item_type')
                if tmdb_id and item_type:
                    emby_ids = media_metadata.get('emby_item_ids_json')
                    first_emby_id = emby_ids[0] if emby_ids and isinstance(emby_ids, list) else None
                    
                    matched_items.append({
                        'id': str(tmdb_id), 
                        'type': item_type,
                        'emby_id': first_emby_id 
                    })
                    
        unique_items = list({f"{item['type']}-{item['id']}": item for item in matched_items}.values())
        logger.info(f"  âœ ç­›é€‰å®Œæˆï¼å…±æ‰¾åˆ° {len(unique_items)} éƒ¨åŒ¹é…çš„åª’ä½“é¡¹ç›®ã€‚")
        return unique_items
    
    def find_matching_collections(self, item_metadata: Dict[str, Any], media_library_id: Optional[str] = None) -> List[Dict[str, Any]]:
        media_item_type = item_metadata.get('item_type')
        media_type_cn = "å‰§é›†" if media_item_type == "Series" else "å½±ç‰‡"
        logger.info(f"  âœ æ­£åœ¨ä¸º{media_type_cn}ã€Š{item_metadata.get('title')}ã€‹å®æ—¶åŒ¹é…è‡ªå®šä¹‰åˆé›†...")
        matched_collections = []
        all_filter_collections = [
            c for c in collection_db.get_all_custom_collections() 
            if c['type'] == 'filter' and c['status'] == 'active' and c['emby_collection_id']
        ]
        if not all_filter_collections:
            logger.debug("  âœ æ²¡æœ‰å‘ç°ä»»ä½•å·²å¯ç”¨çš„ç­›é€‰ç±»åˆé›†ï¼Œè·³è¿‡åŒ¹é…ã€‚")
            return []
        for collection_def in all_filter_collections:
            try:
                definition = collection_def['definition_json']
                defined_library_ids = definition.get('library_ids')
                if defined_library_ids and media_library_id and media_library_id not in defined_library_ids:
                    logger.debug(f"  âœ è·³è¿‡åˆé›†ã€Š{collection_def['name']}ã€‹ï¼Œå› ä¸ºåª’ä½“åº“ä¸åŒ¹é… (åˆé›†è¦æ±‚: {defined_library_ids}, å®é™…æ¥è‡ª: '{media_library_id}')ã€‚")
                    continue 
                collection_item_types = definition.get('item_type', ['Movie'])
                if isinstance(collection_item_types, str):
                    collection_item_types = [collection_item_types]
                if media_item_type not in collection_item_types:
                    logger.debug(f"  âœ è·³è¿‡åˆé›†ã€Š{collection_def['name']}ã€‹ï¼Œå› ä¸ºå†…å®¹ç±»å‹ä¸åŒ¹é… (åˆé›†éœ€è¦: {collection_item_types}, å®é™…æ˜¯: '{media_item_type}')ã€‚")
                    continue
                rules = definition.get('rules', [])
                logic = definition.get('logic', 'AND')
                if self._item_matches_rules(item_metadata, rules, logic):
                    logger.info(f"  âœ åŒ¹é…æˆåŠŸï¼{media_type_cn}ã€Š{item_metadata.get('title')}ã€‹å±äºåˆé›†ã€Š{collection_def['name']}ã€‹ã€‚")
                    matched_collections.append({
                        'id': collection_def['id'],
                        'name': collection_def['name'],
                        'emby_collection_id': collection_def['emby_collection_id']
                    })
            except TypeError as e:
                logger.warning(f"  âœ è§£æåˆé›†ã€Š{collection_def['name']}ã€‹çš„å®šä¹‰æ—¶å‡ºé”™: {e}ï¼Œè·³è¿‡ã€‚")
                continue
        return matched_collections
    
class RecommendationEngine:
    """
    ã€AI æ¨èå¼•æ“ (åŒæ¨¡ç‰ˆ)ã€‘
    æ¨¡å¼ A (LLM): åŸºäºå¤§æ¨¡å‹çŸ¥è¯†åº“æ¨è (é€‚åˆå‘ç°æ–°ç‰‡)ã€‚
    æ¨¡å¼ B (Vector): åŸºäºæœ¬åœ°æ•°æ®åº“å‘é‡ç›¸ä¼¼åº¦æ¨è (é€‚åˆç²¾å‡†åŒ¹é…å£å‘³)ã€‚
    """
    def __init__(self, tmdb_api_key: str):
        self.tmdb_api_key = tmdb_api_key
        self.list_importer = ListImporter(tmdb_api_key) # å¤ç”¨æœç´¢åŒ¹é…é€»è¾‘

    def _vector_search(self, user_history_items: List[Dict], limit: int = 10) -> List[Dict]:
        """
        ã€å†…éƒ¨æ–¹æ³•ã€‘åŸºäºå‘é‡ç›¸ä¼¼åº¦æœç´¢æœ¬åœ°æ•°æ®åº“ã€‚
        æ”¹è¿›ç‰ˆï¼šä¼˜å…ˆä½¿ç”¨ TMDb ID è¿›è¡Œç²¾ç¡®åŒ¹é…ï¼Œè€Œéä¸é è°±çš„æ ‡é¢˜åŒ¹é…ã€‚
        """
        logger.info("  âœ [å‘é‡æœç´¢] å¼€å§‹åŠ è½½å‘é‡æ•°æ®å¹¶è®¡ç®—ç›¸ä¼¼åº¦...")
        
        # 1. æå–å†å²è®°å½•ä¸­çš„ ID é›†åˆ (ç”¨äºå¿«é€ŸæŸ¥æ‰¾)
        history_tmdb_ids = set()
        history_titles = set()
        
        # å…¼å®¹å¤„ç†ï¼šuser_history_items å¯èƒ½æ˜¯å­—å…¸åˆ—è¡¨ï¼Œä¹Ÿå¯èƒ½æ˜¯çº¯æ ‡é¢˜åˆ—è¡¨(æ—§é€»è¾‘)
        for item in user_history_items:
            if isinstance(item, dict):
                if item.get('tmdb_id'):
                    history_tmdb_ids.add(str(item.get('tmdb_id')))
                if item.get('title'):
                    history_titles.add(item.get('title'))
            elif isinstance(item, str):
                history_titles.add(item)

        if not history_tmdb_ids and not history_titles:
            logger.warning("  âœ [å‘é‡æœç´¢] ç”¨æˆ·å†å²è®°å½•ä¸ºç©ºæˆ–æ ¼å¼æ— æ³•è§£æï¼Œè·³è¿‡ã€‚")
            return []

        try:
            with connection.get_db_connection() as conn:
                cursor = conn.cursor()
                # è·å–æ‰€æœ‰å·²ç”Ÿæˆå‘é‡çš„åª’ä½“
                cursor.execute("""
                    SELECT tmdb_id, title, item_type, overview_embedding 
                    FROM media_metadata 
                    WHERE overview_embedding IS NOT NULL
                """)
                all_data = cursor.fetchall()
                
            if not all_data:
                logger.warning("  âœ [å‘é‡æœç´¢] æ•°æ®åº“ä¸­æ²¡æœ‰å‘é‡æ•°æ®ã€‚è¯·å…ˆè¿è¡Œâ€œç”Ÿæˆåª’ä½“å‘é‡â€ä»»åŠ¡ã€‚")
                return []

            # 2. æ„å»ºçŸ©é˜µ
            ids = []
            vectors = []
            titles = []
            types = []
            
            for row in all_data:
                vec = row.get('overview_embedding')
                if vec and len(vec) > 0:
                    ids.append(str(row['tmdb_id']))
                    titles.append(row['title'])
                    types.append(row['item_type'])
                    vectors.append(np.array(vec, dtype=np.float32))
            
            if not vectors:
                return []

            matrix = np.stack(vectors)
            # å½’ä¸€åŒ–
            norm = np.linalg.norm(matrix, axis=1, keepdims=True)
            matrix = matrix / (norm + 1e-10)

            # 3. å®šä½ç”¨æˆ·å£å‘³ (User Profile)
            user_vectors = []
            matched_count = 0
            
            # â˜…â˜…â˜… æ”¹è¿›çš„æ ¸å¿ƒï¼šä¼˜å…ˆåŒ¹é… ID â˜…â˜…â˜…
            for idx, db_tmdb_id in enumerate(ids):
                is_match = False
                
                # A. ID ç²¾ç¡®åŒ¹é…
                if db_tmdb_id in history_tmdb_ids:
                    is_match = True
                
                # B. æ ‡é¢˜å…œåº•åŒ¹é… (å¦‚æœ ID æ²¡å¯¹ä¸Šï¼Œå†è¯•æ ‡é¢˜)
                elif titles[idx] and any(h_t in titles[idx] for h_t in history_titles):
                    is_match = True
                
                if is_match:
                    user_vectors.append(matrix[idx])
                    matched_count += 1
            
            if not user_vectors:
                logger.warning(f"  âœ [å‘é‡æœç´¢] åŒ¹é…å¤±è´¥ï¼šç”¨æˆ·çš„ {len(user_history_items)} æ¡å†å²è®°å½•å‡æœªåœ¨æœ¬åœ°å‘é‡åº“ä¸­æ‰¾åˆ°å¯¹åº”æ•°æ®ã€‚")
                logger.warning(f"    (æç¤ºï¼šè¯·æ£€æŸ¥è¿™äº›å½±ç‰‡æ˜¯å¦å·²å…¥åº“ï¼Œä¸”æ˜¯å¦å·²è¿è¡Œ'ç”Ÿæˆåª’ä½“å‘é‡'ä»»åŠ¡)")
                return []
            
            logger.info(f"  âœ [å‘é‡æœç´¢] æˆåŠŸåŒ¹é…åˆ° {matched_count} éƒ¨å†å²å½±ç‰‡çš„å‘é‡ï¼Œæ­£åœ¨è®¡ç®—æ¨è...")

            # è®¡ç®—ç”¨æˆ·å¹³å‡å‘é‡
            user_profile_vector = np.mean(user_vectors, axis=0)
            user_profile_vector = user_profile_vector / (np.linalg.norm(user_profile_vector) + 1e-10)

            # 4. è®¡ç®—ç›¸ä¼¼åº¦
            scores = np.dot(matrix, user_profile_vector)
            top_indices = np.argsort(scores)[::-1]
            
            results = []
            count = 0
            for idx in top_indices:
                if count >= limit: break
                
                score = float(scores[idx])
                if score < 0.45: break 
                if score > 0.999: continue # æ’é™¤è‡ªå·±
                
                # æ’é™¤å·²çœ‹è¿‡çš„ (ID æˆ– æ ‡é¢˜)
                if ids[idx] in history_tmdb_ids: continue
                if any(h_t in titles[idx] for h_t in history_titles): continue

                results.append({
                    'id': ids[idx],
                    'type': types[idx],
                    'title': titles[idx],
                    'score': score
                })
                count += 1
                
            logger.info(f"  âœ [å‘é‡æœç´¢] è®¡ç®—å®Œæˆï¼Œè´¡çŒ®äº† {len(results)} éƒ¨ç›¸ä¼¼å½±ç‰‡ã€‚")
            return results

        except Exception as e:
            logger.error(f"  âœ [å‘é‡æœç´¢] è®¡ç®—è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return []
        
    def generate(self, definition: Dict) -> List[Dict[str, str]]:
        """
        ç”Ÿæˆæ¨èåˆ—è¡¨çš„ä¸»å…¥å£ (é…åˆ media_db è¿”å›å­—å…¸åˆ—è¡¨çš„ä¼˜åŒ–ç‰ˆ)ã€‚
        """
        target_user_id = definition.get('target_user_id')
        ai_prompt = definition.get('ai_prompt')
        limit = definition.get('limit', 20)

        if not target_user_id:
            logger.error("  âœ [AIæ¨è] æœªæŒ‡å®šç›®æ ‡ç”¨æˆ·ï¼Œæ— æ³•ç”Ÿæˆæ¨èã€‚")
            return []

        # 1. è·å–ç”¨æˆ·å†å² (ç°åœ¨ç›´æ¥è¿”å›åŒ…å« tmdb_id çš„å­—å…¸åˆ—è¡¨)
        history_items = media_db.get_user_positive_history(target_user_id, limit=20)
        
        if not history_items:
            logger.warning(f"  âœ [AIæ¨è] ç”¨æˆ· {target_user_id} æ²¡æœ‰è¶³å¤Ÿçš„è§‚çœ‹å†å²ã€‚")
            return []

        # å‡†å¤‡ç»™ LLM çœ‹çš„çº¯æ–‡æœ¬æ ‡é¢˜åˆ—è¡¨
        history_titles_for_llm = []
        for item in history_items:
            title = item.get('title')
            year = item.get('release_year')
            if year:
                history_titles_for_llm.append(f"{title} ({year})")
            else:
                history_titles_for_llm.append(title)

        final_items_map = {} 

        # ==================================================
        # ç­–ç•¥ A: LLM æ¨è
        # ==================================================
        logger.info(f"  âœ [AIæ¨è] æ­£åœ¨è°ƒç”¨ LLM è¿›è¡Œæ¨ç†...")
        try:
            translator = AITranslator(config_manager.APP_CONFIG)
            request_limit = min(int(limit * 1.5), 50) 
            instruction_with_limit = f"{ai_prompt or ''} (Please recommend at least {request_limit} items)"
            
            # ä¼ ç»™ LLM çº¯æ–‡æœ¬æ ‡é¢˜
            llm_recommendations = translator.get_recommendations(history_titles_for_llm, instruction_with_limit)
            
            if llm_recommendations:
                logger.info(f"  âœ [AIæ¨è] LLM è¿”å›äº† {len(llm_recommendations)} éƒ¨ä½œå“ï¼Œæ­£åœ¨åŒ¹é… TMDb ID...")
                
                with ThreadPoolExecutor(max_workers=5) as executor:
                    def resolve_item(rec_item):
                        try:
                            # 1. æ•°æ®æ ‡å‡†åŒ–
                            title = ""
                            original_title = ""
                            year = None
                            # é»˜è®¤å…ˆä¿¡ AI çš„ï¼ŒAI æ²¡è¯´å°±é»˜è®¤ä¸º Movie
                            primary_type = 'Movie' 
                            
                            if isinstance(rec_item, dict):
                                title = rec_item.get('title')
                                original_title = rec_item.get('original_title')
                                year = str(rec_item.get('year')) if rec_item.get('year') else None
                                if rec_item.get('type'):
                                    primary_type = rec_item.get('type')
                            elif isinstance(rec_item, str):
                                title = rec_item
                            
                            if not title: return None

                            # å®šä¹‰åå‘ç±»å‹ï¼ˆå¦‚æœè¿™æ¬¡æœMovieå¤±è´¥ï¼Œä¸‹æ¬¡å°±æœSeriesï¼‰
                            secondary_type = 'Series' if primary_type == 'Movie' else 'Movie'

                            # ç®€å•çš„ä¸­æ–‡åˆ¤æ–­
                            def has_chinese(text):
                                return any('\u4e00' <= char <= '\u9fff' for char in str(text))

                            # 2. ç¡®å®šæœç´¢å…³é”®è¯
                            # é»˜è®¤ç”¨åŸåæœ
                            search_query = original_title if original_title else title
                            
                            # ç‰¹æ®Šä¼˜åŒ–ï¼šå¦‚æœæ˜¯å›½äº§å‰§ï¼ˆæ ‡é¢˜å«ä¸­æ–‡ï¼‰ï¼Œå¼ºè¡Œç”¨ä¸­æ–‡åæœï¼Œå‡†ç¡®ç‡æœ€é«˜
                            if has_chinese(title):
                                search_query = title

                            # --- æ ¸å¿ƒä¿®æ”¹ï¼šå››é‡æœç´¢ç­–ç•¥ ---
                            
                            # ç¬¬ 1 è¯•ï¼šç”¨ã€é¦–é€‰ç±»å‹ã€‘+ã€é¦–é€‰å…³é”®è¯ã€‘æœ
                            match_result = self.list_importer._match_title_to_tmdb(search_query, primary_type, year)
                            
                            # ç¬¬ 2 è¯•ï¼šå¦‚æœæ²¡æœåˆ°ï¼Œå°è¯•ã€åå‘ç±»å‹ã€‘+ã€é¦–é€‰å…³é”®è¯ã€‘
                            # (è§£å†³ AI æŠŠç”µè§†å‰§æ ‡æˆç”µå½±ï¼Œæˆ–è€…æ²¡æ ‡ç±»å‹çš„æƒ…å†µ)
                            if not match_result:
                                # logger.debug(f"  âœ [AIæ¨è] '{search_query}' æŒ‰ {primary_type} æœªæ‰¾åˆ°ï¼Œå°è¯•æŒ‰ {secondary_type} æœç´¢...")
                                match_result = self.list_importer._match_title_to_tmdb(search_query, secondary_type, year)

                            # ç¬¬ 3 è¯•ï¼šå¦‚æœè¿˜æ²¡æœåˆ°ï¼Œä¸”å…³é”®è¯ä¸æ˜¯ä¸­æ–‡æ ‡é¢˜ï¼Œå°è¯•ç”¨ã€ä¸­æ–‡æ ‡é¢˜ã€‘+ã€é¦–é€‰ç±»å‹ã€‘æœ
                            # (è§£å†³è‹±æ–‡åŸåæœä¸åˆ°çš„æƒ…å†µ)
                            if not match_result and search_query != title:
                                match_result = self.list_importer._match_title_to_tmdb(title, primary_type, year)

                            # ç¬¬ 4 è¯•ï¼šæœ€åè¯•ä¸€æ¬¡ã€ä¸­æ–‡æ ‡é¢˜ã€‘+ã€åå‘ç±»å‹ã€‘
                            if not match_result and search_query != title:
                                match_result = self.list_importer._match_title_to_tmdb(title, secondary_type, year)

                            # --- ç»“æœå¤„ç† ---
                            if match_result:
                                tmdb_id, matched_type, season_num = match_result
                                return {
                                    'id': tmdb_id,
                                    'type': matched_type, # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯å®é™…åŒ¹é…åˆ°çš„ç±»å‹
                                    'title': title, 
                                    'season': season_num,
                                    'release_date': None 
                                }
                            else:
                                logger.debug(f"  âœ [AIæ¨è] æœªèƒ½æ‰¾åˆ° '{title}' (æœ: {search_query}) çš„ TMDb ID (å·²å°è¯•ç”µå½±/å‰§é›†è·¨ç±»å‹æœç´¢)ã€‚")
                                return None
                        except Exception as e:
                            logger.error(f"  âœ [AIæ¨è] å¤„ç†å•é¡¹ '{rec_item}' æ—¶å‡ºé”™: {e}")
                            return None

                    results = executor.map(resolve_item, llm_recommendations)
                    for res in results:
                        if res:
                            final_items_map[res['id']] = res
            else:
                logger.warning("  âœ [AIæ¨è] LLM æœªè¿”å›ä»»ä½•æœ‰æ•ˆç»“æœã€‚")

        except Exception as e:
            logger.error(f"  âœ [AIæ¨è] LLM è°ƒç”¨å¤±è´¥: {e}")

        # ==================================================
        # ç­–ç•¥ B: å‘é‡æ¨è
        # ==================================================
        if len(final_items_map) < limit:
            try:
                needed = limit - len(final_items_map)
                # â˜…â˜…â˜… å…³é”®ï¼šç›´æ¥æŠŠåŒ…å« tmdb_id çš„å­—å…¸åˆ—è¡¨ä¼ ç»™å‘é‡æœç´¢ â˜…â˜…â˜…
                # (å‰ææ˜¯ä½ å·²ç»åº”ç”¨äº†æˆ‘ä¸Šä¸€æ¡å›å¤ä¸­å¯¹ _vector_search çš„ä¿®æ”¹)
                vector_results = self._vector_search(history_items, limit=needed + 5)
                
                if vector_results:
                    logger.info(f"  âœ [AIæ¨è] å¯ç”¨å‘é‡å¼•æ“è¡¥å……äº† {len(vector_results)} éƒ¨ç›¸ä¼¼å½±ç‰‡ã€‚")
                    for v in vector_results:
                        if v['id'] not in final_items_map:
                            final_items_map[v['id']] = {
                                'id': v['id'],
                                'type': v['type'],
                                'title': v['title'],
                                'release_date': None
                            }
            except Exception as e:
                logger.error(f"  âœ [AIæ¨è] å‘é‡æ¨èå¤±è´¥: {e}", exc_info=True)

        # ==================================================
        # æœ€ç»ˆæ±‡æ€»
        # ==================================================
        final_items = list(final_items_map.values())
        if limit and isinstance(limit, int):
            final_items = final_items[:limit]

        logger.info(f"  âœ [AIæ¨è] å…¨éƒ¨å®Œæˆã€‚æœ€ç»ˆç”Ÿæˆ {len(final_items)} éƒ¨å½±ç‰‡ã€‚")
        return final_items