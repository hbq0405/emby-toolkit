# handler/nullbr.py
import logging
import requests
import re
import time  
import threading 
from datetime import datetime
from database import settings_db, media_db, request_db
import config_manager

import constants
import utils
try:
    # åªå¯¼å…¥ä¸»ç±»ï¼Œä¸å¯¼å…¥å·¥å…·ç±»ï¼Œé˜²æ­¢æŠ¥é”™
    from p115client import P115Client
except ImportError:
    P115Client = None

logger = logging.getLogger(__name__)

# â˜…â˜…â˜… ç¡¬ç¼–ç é…ç½®ï¼šNullbr â˜…â˜…â˜…
NULLBR_APP_ID = "7DqRtfNX3"
NULLBR_API_BASE = "https://api.nullbr.com"

# çº¿ç¨‹é”ï¼Œé˜²æ­¢å¹¶å‘è¯·æ±‚å¯¼è‡´è®¡æ•°å™¨é”™ä¹±
_rate_limit_lock = threading.Lock()

def get_config():
    return settings_db.get_setting('nullbr_config') or {}

def _get_headers():
    config = get_config()
    api_key = config.get('api_key')
    headers = {
        "Content-Type": "application/json",
        "X-APP-ID": NULLBR_APP_ID,
        "User-Agent": f"EmbyToolkit/{constants.APP_VERSION}"
    }
    if api_key:
        headers["X-API-KEY"] = api_key
    return headers

def _parse_size_to_gb(size_str):
    """å°†å¤§å°å­—ç¬¦ä¸² (å¦‚ '83.03 GB', '500 MB') è½¬æ¢ä¸º GB (float)"""
    if not size_str:
        return 0.0
    
    size_str = size_str.upper().replace(',', '')
    match = re.search(r'([\d\.]+)\s*(TB|GB|MB|KB)', size_str)
    if not match:
        return 0.0
    
    num = float(match.group(1))
    unit = match.group(2)
    
    if unit == 'TB':
        return num * 1024
    elif unit == 'GB':
        return num
    elif unit == 'MB':
        return num / 1024
    elif unit == 'KB':
        return num / 1024 / 1024
    return 0.0

def _is_resource_valid(item, filters, media_type='movie'):
    """æ ¹æ®é…ç½®è¿‡æ»¤èµ„æº"""
    if not filters:
        return True

    # 1. åˆ†è¾¨ç‡è¿‡æ»¤ (å¦‚æœé…ç½®äº†åˆ—è¡¨ï¼Œåˆ™å¿…é¡»åœ¨åˆ—è¡¨ä¸­)
    allowed_resolutions = filters.get('resolutions', [])
    if allowed_resolutions:
        res = item.get('resolution')
        # å¦‚æœèµ„æºæ²¡æ ‡åˆ†è¾¨ç‡ï¼Œæˆ–è€…åˆ†è¾¨ç‡ä¸åœ¨å…è®¸åˆ—è¡¨ä¸­ï¼Œåˆ™è¿‡æ»¤
        if not res or res not in allowed_resolutions:
            return False

    # 2. è´¨é‡è¿‡æ»¤ (åªè¦åŒ…å«å…¶ä¸­ä¸€ä¸ªå…³é”®è¯å³å¯)
    allowed_qualities = filters.get('qualities', [])
    if allowed_qualities:
        item_quality = item.get('quality')
        # item_quality å¯èƒ½æ˜¯å­—ç¬¦ä¸²ä¹Ÿå¯èƒ½æ˜¯åˆ—è¡¨
        if not item_quality:
            return False
        
        if isinstance(item_quality, str):
            q_list = [item_quality]
        else:
            q_list = item_quality
            
        # æ£€æŸ¥æ˜¯å¦æœ‰äº¤é›†
        has_match = any(q in q_list for q in allowed_qualities)
        if not has_match:
            return False

    # 3. å¤§å°è¿‡æ»¤ (GB)
    if media_type == 'tv':
        # å¦‚æœé…ç½®äº† tv_min_sizeï¼Œä¼˜å…ˆä½¿ç”¨ï¼Œå¦åˆ™å›é€€åˆ°æ—§çš„ min_size (å…¼å®¹æ—§é…ç½®)
        min_size = float(filters.get('tv_min_size') or filters.get('min_size') or 0)
        max_size = float(filters.get('tv_max_size') or filters.get('max_size') or 0)
    else:
        # é»˜è®¤ä¸ºç”µå½±
        min_size = float(filters.get('movie_min_size') or filters.get('min_size') or 0)
        max_size = float(filters.get('movie_max_size') or filters.get('max_size') or 0)
    
    if min_size > 0 or max_size > 0:
        size_gb = _parse_size_to_gb(item.get('size'))
        if min_size > 0 and size_gb < min_size:
            return False
        if max_size > 0 and size_gb > max_size:
            return False

    # 4. ä¸­å­—è¿‡æ»¤
    if filters.get('require_zh'):
        # 1. ä¼˜å…ˆçœ‹ API è¿”å›çš„ç¡¬æŒ‡æ ‡ (zh_sub: 1)
        if item.get('is_zh_sub'):
            return True
            
        # 2. API æ²¡æ ‡è®°ï¼Œå°è¯•ä»æ ‡é¢˜çŒœæµ‹
        title = item.get('title', '').upper()
        
        # å¸¸è§çš„ä¸­å­—/å›½è¯­æ ‡è¯†
        zh_keywords = [
            'ä¸­å­—', 'ä¸­è‹±', 'å­—å¹•', 
            'CHS', 'CHT', 'CN', 
            'DIY', 'å›½è¯­', 'å›½ç²¤'
        ]
        
        # åªè¦åŒ…å«ä»»æ„ä¸€ä¸ªå…³é”®è¯å³å¯
        is_zh_guess = any(k in title for k in zh_keywords)
        
        if not is_zh_guess:
            return False

    # 5. å°è£…å®¹å™¨è¿‡æ»¤ (åç¼€å)
    allowed_containers = filters.get('containers', [])
    if allowed_containers:
        # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœæ˜¯å‰§é›† (TV)ï¼Œé€šå¸¸æ˜¯ç›®å½•æˆ–åˆé›†ï¼Œæ— æ³•ä»æ ‡é¢˜åˆ¤æ–­å®¹å™¨ï¼Œç›´æ¥æ”¾è¡Œ â˜…â˜…â˜…
        # å¦åˆ™ä¼šå¯¼è‡´æ–‡ä»¶å¤¹å½¢å¼çš„èµ„æºè¢«è¯¯æ€
        if media_type == 'tv':
            return True

        title = item.get('title', '').lower()
        # æ£€æŸ¥æ ‡é¢˜ç»“å°¾æˆ–é“¾æ¥ç»“å°¾
        link = item.get('link', '').lower()
        
        # æå–æ‰©å±•åé€»è¾‘ç®€å•ç‰ˆ
        ext = None
        if 'mkv' in title or link.endswith('.mkv'): ext = 'mkv'
        elif 'mp4' in title or link.endswith('.mp4'): ext = 'mp4'
        elif 'iso' in title or link.endswith('.iso'): ext = 'iso'
        elif 'ts' in title or link.endswith('.ts'): ext = 'ts'
        
        if not ext or ext not in allowed_containers:
            return False

    return True

def _check_and_update_rate_limit():
    """
    æ£€æŸ¥ API è°ƒç”¨é™åˆ¶ï¼š
    1. æ¯æ—¥é™é¢æ£€æŸ¥
    2. è¯·æ±‚é—´éš”å¼ºåˆ¶ç¡çœ 
    """
    with _rate_limit_lock:
        config = get_config()
        # è·å–é…ç½®ï¼Œé»˜è®¤é™åˆ¶ 100 æ¬¡ï¼Œé—´éš” 5 ç§’
        daily_limit = int(config.get('daily_limit', 100))
        interval = float(config.get('request_interval', 5.0))
        
        # è·å–ç»Ÿè®¡æ•°æ®
        stats = settings_db.get_setting('nullbr_usage_stats') or {}
        today_str = datetime.now().strftime('%Y-%m-%d')
        
        # 1. æ£€æŸ¥æ—¥æœŸï¼Œå¦‚æœæ˜¯æ–°çš„ä¸€å¤©åˆ™é‡ç½®
        if stats.get('date') != today_str:
            stats = {
                'date': today_str,
                'count': 0,
                'last_request_ts': 0
            }
        
        # 2. æ£€æŸ¥æ¯æ—¥é™é¢
        current_count = stats.get('count', 0)
        if current_count >= daily_limit:
            logger.warning(f"NULLBR API ä»Šæ—¥è°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™ ({current_count}/{daily_limit})")
            raise Exception(f"ä»Šæ—¥ API è°ƒç”¨æ¬¡æ•°å·²è¾¾ä¸Šé™ ({daily_limit}æ¬¡)ï¼Œè¯·æ˜æ—¥å†è¯•æˆ–å¢åŠ é…é¢ã€‚")
            
        # 3. æ£€æŸ¥è¯·æ±‚é—´éš” (å¼ºåˆ¶ç¡çœ )
        last_ts = stats.get('last_request_ts', 0)
        now_ts = time.time()
        elapsed = now_ts - last_ts
        
        if elapsed < interval:
            sleep_time = interval - elapsed
            logger.info(f"  â³ è§¦å‘æµæ§ï¼Œå¼ºåˆ¶ç­‰å¾… {sleep_time:.2f} ç§’...")
            time.sleep(sleep_time)
            
        # 4. æ›´æ–°ç»Ÿè®¡
        stats['count'] = current_count + 1
        stats['last_request_ts'] = time.time()
        settings_db.save_setting('nullbr_usage_stats', stats)
        
        logger.debug(f"NULLBR API è°ƒç”¨ç»Ÿè®¡: {stats['count']}/{daily_limit}")

def _enrich_items_with_status(items):
    """
    æ‰¹é‡æŸ¥è¯¢æœ¬åœ°æ•°æ®åº“ï¼Œä¸º NULLBR çš„ç»“æœæ³¨å…¥ in_library å’Œ subscription_status çŠ¶æ€
    """
    if not items:
        return items

    # 1. æå– ID åˆ—è¡¨
    # NULLBR è¿”å›çš„ ID å¯èƒ½æ˜¯ 'id' æˆ– 'tmdbid'
    tmdb_ids = []
    for item in items:
        tid = item.get('tmdbid') or item.get('id')
        if tid:
            tmdb_ids.append(str(tid))
    
    if not tmdb_ids:
        return items

    # 2. æ‰¹é‡æŸ¥è¯¢æ•°æ®åº“
    # å‡è®¾å¤§éƒ¨åˆ†æ˜¯ç”µå½±ï¼Œæ··åˆæŸ¥è¯¢æ¯”è¾ƒéº»çƒ¦ï¼Œè¿™é‡Œç®€å•å¤„ç†ï¼š
    # åˆ†åˆ«æŸ¥ Movie å’Œ Seriesï¼Œæˆ–è€…æ ¹æ® item è‡ªèº«çš„ media_type åˆ¤æ–­
    # ä¸ºäº†æ•ˆç‡ï¼Œæˆ‘ä»¬ä¸€æ¬¡æ€§æŸ¥å‡ºæ¥ï¼Œåœ¨å†…å­˜é‡ŒåŒ¹é…
    
    # è·å–æ‰€æœ‰ç›¸å…³ ID çš„åº“å†…çŠ¶æ€ (Movie å’Œ Series éƒ½æŸ¥)
    library_map_movie = media_db.check_tmdb_ids_in_library(tmdb_ids, 'Movie')
    library_map_series = media_db.check_tmdb_ids_in_library(tmdb_ids, 'Series')
    
    # è·å–è®¢é˜…çŠ¶æ€
    sub_status_movie = request_db.get_global_subscription_statuses_by_tmdb_ids(tmdb_ids, 'Movie')
    sub_status_series = request_db.get_global_subscription_statuses_by_tmdb_ids(tmdb_ids, 'Series')

    # 3. æ³¨å…¥çŠ¶æ€
    for item in items:
        tid = str(item.get('tmdbid') or item.get('id') or '')
        mtype = item.get('media_type', 'movie') # é»˜è®¤ä¸º movie
        
        if not tid:
            continue

        in_lib = False
        sub_stat = None

        if mtype == 'tv':
            if f"{tid}_Series" in library_map_series:
                in_lib = True
            sub_stat = sub_status_series.get(tid)
        else:
            if f"{tid}_Movie" in library_map_movie:
                in_lib = True
            sub_stat = sub_status_movie.get(tid)
            
        item['in_library'] = in_lib
        item['subscription_status'] = sub_stat

    return items

def get_preset_lists():
    """è·å–ç‰‡å•åˆ—è¡¨"""
    custom_presets = settings_db.get_setting('nullbr_presets')
    if custom_presets and isinstance(custom_presets, list) and len(custom_presets) > 0:
        return custom_presets
    return utils.DEFAULT_NULLBR_PRESETS

def fetch_list_items(list_id, page=1):
    """è·å–æŒ‡å®šç‰‡å•çš„è¯¦ç»†å†…å®¹"""
    url = f"{NULLBR_API_BASE}/list/{list_id}"
    params = {"page": page}
    try:
        logger.info(f"  âœ æ­£åœ¨è·å–ç‰‡å•åˆ—è¡¨: {list_id} (Page {page})")
        response = requests.get(url, params=params, headers=_get_headers(), timeout=15)
        response.raise_for_status()
        data = response.json()
        items = data.get('items', [])
        enriched_items = _enrich_items_with_status(items)
        return {"code": 200, "data": {"list": enriched_items, "total": data.get('total_results', 0)}}
    except Exception as e:
        logger.error(f"  âœ è·å–ç‰‡å•å¤±è´¥: {e}")
        raise e

def search_media(keyword, page=1):
    """æœç´¢èµ„æº """
    url = f"{NULLBR_API_BASE}/search"
    params = { "query": keyword, "page": page }
    try:
        proxies = config_manager.get_proxies_for_requests()
        response = requests.get(url, params=params, headers=_get_headers(), timeout=15, proxies=proxies)
        response.raise_for_status()
        data = response.json()
        items = data.get('items', [])
        enriched_items = _enrich_items_with_status(items)
        return { "code": 200, "data": { "list": enriched_items, "total": data.get('total_results', 0) } }
    except Exception as e:
        logger.error(f"  âœ NULLBR æœç´¢å¤±è´¥: {e}")
        raise e

def _fetch_single_source(tmdb_id, media_type, source_type, season_number=None):
    # 1. æµæ§æ£€æŸ¥
    try:
        _check_and_update_rate_limit()
    except Exception as e:
        logger.warning(f"  âš ï¸ {e}")
        return []

    # 2. æ„é€  URL
    url = ""
    if media_type == 'movie':
        url = f"{NULLBR_API_BASE}/movie/{tmdb_id}/{source_type}"
    elif media_type == 'tv':
        if season_number:
            # â˜… å…³é”®ï¼šå¦‚æœæœ‰å­£å·ï¼Œè¯·æ±‚å•å­£æ¥å£
            url = f"{NULLBR_API_BASE}/tv/{tmdb_id}/season/{season_number}/{source_type}"
        else:
            # æ²¡æœ‰å­£å·ï¼Œè¯·æ±‚æ•´å‰§æ¥å£ (115) æˆ– S1 (Magnet)
            if source_type == '115':
                url = f"{NULLBR_API_BASE}/tv/{tmdb_id}/115"
            elif source_type == 'magnet':
                url = f"{NULLBR_API_BASE}/tv/{tmdb_id}/season/1/magnet"
            else:
                return []
    else:
        return []

    # â˜… æ‰“å°æ—¥å¿—ï¼Œæ–¹ä¾¿ä½ åœ¨åå°çœ‹æ˜¯å¦çœŸçš„å¸¦ä¸Šäº†å­£å·
    logger.info(f"  âœ [DEBUG] NULLBRè¯·æ±‚: {url} (Season: {season_number})")

    try:
        proxies = config_manager.get_proxies_for_requests()
        response = requests.get(url, headers=_get_headers(), timeout=10, proxies=proxies)
        
        if response.status_code == 404:
            return []
        
        response.raise_for_status()
        data = response.json()
        raw_list = data.get(source_type, [])
        
        cleaned_list = []
        for item in raw_list:
            link = item.get('share_link') or item.get('magnet') or item.get('ed2k')
            title = item.get('title') or item.get('name')
            
            if link and title:
                # ç£åŠ›é“¾å¦‚æœæ²¡æœ‰å­£å·ï¼Œé»˜è®¤æ ‡è®°ä¸º S1 (ä»…æ˜¾ç¤ºç”¨)
                if media_type == 'tv' and source_type == 'magnet' and not season_number:
                    title = f"[S1] {title}"
                
                # ä¸­å­—åˆ¤æ–­
                is_zh = item.get('zh_sub') == 1
                if not is_zh:
                    t_upper = title.upper()
                    zh_keywords = ['ä¸­å­—', 'ä¸­è‹±', 'å­—å¹•', 'CHS', 'CHT', 'CN', 'DIY', 'å›½è¯­', 'å›½ç²¤']
                    if any(k in t_upper for k in zh_keywords):
                        is_zh = True
                
                # -------------------------------------------------
                # â˜…â˜…â˜… å¼ºåŠ›æ¸…æ´—ï¼šå†æ¬¡æ ¸å¯¹å­£å·ï¼Œé˜²æ­¢ API è¿”å›è„æ•°æ® â˜…â˜…â˜…
                # -------------------------------------------------
                if media_type == 'tv' and season_number:
                    try:
                        target_season = int(season_number)
                        title_upper = title.upper()
                        
                        # 1. åŒ¹é… Sxx æ ¼å¼ (å¦‚ S04, .S04., [S04])
                        # æ’é™¤ S01-S05 è¿™ç§åˆé›†èŒƒå›´ï¼ŒåªåŒ¹é…å•ç‹¬çš„å­£å·æ ‡è¯†
                        match = re.search(r'(?:^|\.|\[|\s|-)S(\d{1,2})(?:\.|\]|\s|E|-|$)', title_upper)
                        if match:
                            found_season = int(match.group(1))
                            if found_season != target_season:
                                # å­£å·ä¸åŒ¹é…ï¼Œè·³è¿‡
                                continue
                        
                        # 2. åŒ¹é…ä¸­æ–‡ "ç¬¬xå­£"
                        match_zh = re.search(r'ç¬¬(\d{1,2})å­£', title)
                        if match_zh:
                            found_season_zh = int(match_zh.group(1))
                            if found_season_zh != target_season:
                                continue
                    except Exception:
                        pass # æ­£åˆ™å‡ºé”™ä¸å½±å“ä¸»æµç¨‹
                # -------------------------------------------------

                resource_obj = {
                    "title": title,
                    "size": item.get('size', 'æœªçŸ¥'),
                    "resolution": item.get('resolution'),
                    "quality": item.get('quality'),
                    "link": link,
                    "source_type": source_type.upper(),
                    "is_zh_sub": is_zh
                }
                cleaned_list.append(resource_obj)
        return cleaned_list
    except Exception as e:
        logger.warning(f"  âœ è·å– {source_type} èµ„æºå¤±è´¥: {e}")
        return []

def fetch_resource_list(tmdb_id, media_type='movie', specific_source=None, season_number=None):
    """
    è·å–èµ„æºåˆ—è¡¨
    """
    config = get_config()
    
    if specific_source:
        sources_to_fetch = [specific_source]
    else:
        sources_to_fetch = config.get('enabled_sources', ['115', 'magnet', 'ed2k'])
    
    all_resources = []
    
    # 1. 115
    if '115' in sources_to_fetch:
        try:
            res_115 = _fetch_single_source(tmdb_id, media_type, '115', season_number)
            all_resources.extend(res_115)
        except Exception as e:
            # å¯ä»¥ä¸´æ—¶åŠ ä¸ªæ—¥å¿—çœ‹æŠ¥é”™
            logger.error(f"115 fetch error: {e}")
            pass

    # 2. Magnet
    if 'magnet' in sources_to_fetch:
        try:
            res_mag = _fetch_single_source(tmdb_id, media_type, 'magnet', season_number)
            all_resources.extend(res_mag)
        except Exception: pass

    # 3. Ed2k (ä»…ç”µå½±)
    if media_type == 'movie' and 'ed2k' in sources_to_fetch:
        try:
            # ç”µå½±ä¸éœ€è¦å­£å·ï¼Œä¿æŒåŸæ ·å³å¯ï¼Œæˆ–è€…ä¼  None
            res_ed2k = _fetch_single_source(tmdb_id, media_type, 'ed2k')
            all_resources.extend(res_ed2k)
        except Exception: pass
    
    # 4. è·å–è¿‡æ»¤é…ç½®
    config = get_config()
    filters = config.get('filters', {})
    
    # 5. æ‰§è¡Œè¿‡æ»¤
    # å¦‚æœ filters å…¨ä¸ºç©ºå€¼ï¼Œåˆ™ä¸è¿‡æ»¤
    has_filter = any(filters.values())
    if not has_filter:
        return all_resources
        
    filtered_list = [res for res in all_resources if _is_resource_valid(res, filters, media_type)]
    
    logger.info(f"  âœ èµ„æºè¿‡æ»¤: åŸå§‹ {len(all_resources)} -> è¿‡æ»¤å {len(filtered_list)}")
    return filtered_list

# ==============================================================================
# â˜…â˜…â˜… 115 æ¨é€é€»è¾‘  â˜…â˜…â˜…
# ==============================================================================

def _clean_link(link):
    """
    æ¸…æ´—é“¾æ¥ï¼šå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œå¹¶å®‰å…¨å»é™¤æœ«å°¾çš„ HTML è„å­—ç¬¦ (&#)
    """
    if not link:
        return ""
    link = link.strip()
    while link.endswith('&#') or link.endswith('&') or link.endswith('#'):
        if link.endswith('&#'):
            link = link[:-2]
        elif link.endswith('&') or link.endswith('#'):
            link = link[:-1]
    return link

def notify_cms_scan():
    """
    é€šçŸ¥ CMS æ‰§è¡Œç›®å½•æ•´ç† (ç”Ÿæˆ strm)
    æ¥å£: /api/sync/lift_by_token?type=auto_organize&token=...
    """
    config = get_config()
    cms_url = config.get('cms_url')
    cms_token = config.get('cms_token')

    if not cms_url or not cms_token:
        # ç”¨æˆ·æ²¡é…ç½® CMSï¼Œç›´æ¥å¿½ç•¥ï¼Œä¸æŠ¥é”™
        return

    cms_url = cms_url.rstrip('/')
    # æ„é€ é€šçŸ¥æ¥å£ URL
    api_url = f"{cms_url}/api/sync/lift_by_token"
    params = {
        "type": "auto_organize",
        "token": cms_token
    }

    try:
        logger.info(f"  âœ æ­£åœ¨é€šçŸ¥ CMS æ‰§è¡Œæ•´ç†...")
        # CMS é€šå¸¸åœ¨å†…ç½‘ï¼Œä¸èµ°ä»£ç†
        response = requests.get(api_url, params=params, timeout=5)
        response.raise_for_status()
        
        res_json = response.json()
        if res_json.get('code') == 200 or res_json.get('success'):
            logger.info(f"  âœ… CMS é€šçŸ¥æˆåŠŸ: {res_json.get('msg', 'OK')}")
        else:
            logger.warning(f"  âš ï¸ CMS é€šçŸ¥è¿”å›å¼‚å¸¸: {res_json}")

    except Exception as e:
        # é€šçŸ¥å¤±è´¥ä¸åº”è¯¥å½±å“ä¸»æµç¨‹çš„æˆåŠŸçŠ¶æ€ï¼Œåªè®°å½•æ—¥å¿—
        logger.warning(f"  âš ï¸ CMS é€šçŸ¥å‘é€å¤±è´¥: {e}")
        raise e

def push_to_115(resource_link, title):
    """
    æ™ºèƒ½æ¨é€ï¼šæ”¯æŒ 115/115cdn/anxia è½¬å­˜ å’Œ ç£åŠ›ç¦»çº¿
    â˜… ä¿®å¤ï¼šæ”¹ç”¨ã€æ–‡ä»¶æŒ‡çº¹(PickCode)å¯¹æ¯”æ³•ã€‘æ£€æµ‹æ–°æ–‡ä»¶/æ–‡ä»¶å¤¹ï¼Œå¹¶å»¶é•¿ç­‰å¾…æ—¶é—´
    """
    if P115Client is None:
        raise ImportError("æœªå®‰è£… p115 åº“")

    config = get_config()
    cookies = config.get('p115_cookies')
    
    try:
        cid_val = config.get('p115_save_path_cid', 0)
        save_path_cid = int(cid_val) if cid_val else 0
    except:
        save_path_cid = 0

    if not cookies:
        raise ValueError("æœªé…ç½® 115 Cookies")

    clean_url = _clean_link(resource_link)
    logger.info(f"  âœ [DEBUG] å¾…å¤„ç†é“¾æ¥: {clean_url}")
    
    client = P115Client(cookies)
    
    try:
        # æ”¯æŒ 115.com, 115cdn.com, anxia.com
        target_domains = ['115.com', '115cdn.com', 'anxia.com']
        is_115_share = any(d in clean_url for d in target_domains) and ('magnet' not in clean_url)
        
        if is_115_share:
            # ... (115 åˆ†äº«é“¾æ¥è½¬å­˜é€»è¾‘ä¿æŒä¸å˜) ...
            logger.info(f"  âœ [æ¨¡å¼] è¯†åˆ«ä¸º 115 è½¬å­˜ä»»åŠ¡ -> CID: {save_path_cid}")
            share_code = None
            match = re.search(r'/s/([a-z0-9]+)', clean_url)
            if match: share_code = match.group(1)
            if not share_code: raise Exception("æ— æ³•ä»é“¾æ¥ä¸­æå–åˆ†äº«ç ")
            receive_code = ''
            pwd_match = re.search(r'password=([a-z0-9]+)', clean_url)
            if pwd_match: receive_code = pwd_match.group(1)
            
            resp = {} 
            try:
                if hasattr(client, 'fs_share_import_to_dir'):
                     resp = client.fs_share_import_to_dir(share_code, receive_code, save_path_cid)
                elif hasattr(client, 'fs_share_import'):
                    resp = client.fs_share_import(share_code, receive_code, save_path_cid)
                elif hasattr(client, 'share_import'):
                    resp = client.share_import(share_code, receive_code, save_path_cid)
                else:
                    api_url = "https://webapi.115.com/share/receive"
                    payload = {'share_code': share_code, 'receive_code': receive_code, 'cid': save_path_cid}
                    r = client.request(api_url, method='POST', data=payload)
                    resp = r.json() if hasattr(r, 'json') else r
            except Exception as e:
                raise Exception(f"è°ƒç”¨è½¬å­˜æ¥å£å¤±è´¥: {e}")

            if resp and resp.get('state'):
                logger.info(f"  âœ… 115 è½¬å­˜æˆåŠŸ: {title}")
                return True
            else:
                err = resp.get('error_msg') if resp else 'æ— å“åº”'
                err = err or resp.get('msg') or str(resp)
                raise Exception(f"è½¬å­˜å¤±è´¥: {err}")

        else:
            # ==================================================
            # â˜…â˜…â˜… ç£åŠ›/Ed2k ç¦»çº¿ä¸‹è½½ (æŒ‡çº¹å¯¹æ¯”ç‰ˆ) â˜…â˜…â˜…
            # ==================================================
            logger.info(f"  âœ [æ¨¡å¼] è¯†åˆ«ä¸ºç£åŠ›/ç¦»çº¿ä»»åŠ¡ -> CID: {save_path_cid}")
            
            # 1. ã€å…³é”®æ­¥éª¤ã€‘å»ºç«‹å¿«ç…§ï¼šè®°å½•å½“å‰ç›®å½•ä¸‹å·²å­˜åœ¨æ–‡ä»¶çš„ pick_code
            existing_pick_codes = set()
            try:
                # è·å–å‰50ä¸ªæ–‡ä»¶/æ–‡ä»¶å¤¹ (æŒ‰ä¸Šä¼ æ—¶é—´å€’åº)
                # æ³¨æ„ï¼š115 API è¿”å›çš„ pc (pick_code) æ˜¯å”¯ä¸€æ ‡è¯†
                files_res = client.fs_files({'cid': save_path_cid, 'limit': 50, 'o': 'user_ptime', 'asc': 0})
                if files_res.get('data'):
                    for item in files_res['data']:
                        if item.get('pc'):
                            existing_pick_codes.add(item.get('pc'))
            except Exception as e:
                logger.warning(f"  âš ï¸ è·å–ç›®å½•å¿«ç…§å¤±è´¥(å¯èƒ½æ˜¯ç©ºç›®å½•): {e}")
            
            logger.info(f"  âœ [å¿«ç…§] å½“å‰ç›®å½•å·²æœ‰ {len(existing_pick_codes)} ä¸ªé¡¹ç›®")

            # 2. æ·»åŠ ä»»åŠ¡
            payload = {'url[0]': clean_url, 'wp_path_id': save_path_cid}
            resp = client.offline_add_urls(payload)
            
            if resp.get('state'):
                # è·å– info_hash ç”¨äºè¾…åŠ©æ£€æŸ¥æ­»é“¾
                result_list = resp.get('result', [])
                info_hash = None
                if result_list and isinstance(result_list, list):
                    info_hash = result_list[0].get('info_hash')

                # 3. è½®è¯¢æ£€æµ‹ç›®å½• (å»¶é•¿åˆ° 45ç§’)
                # æ–‡ä»¶å¤¹ç”Ÿæˆæ¯”è¾ƒæ…¢ï¼Œç»™è¶³æ—¶é—´
                max_retries = 3  # 15æ¬¡ * 3ç§’ = 45ç§’
                success_found = False
                
                logger.info(f"  âœ ä»»åŠ¡å·²æäº¤ï¼Œæ­£åœ¨æ‰«ææ–°é¡¹ç›®...")

                for i in range(max_retries):
                    time.sleep(3) 
                    
                    # --- A. æ£€æŸ¥ç›®å½•æ˜¯å¦æœ‰ã€ä¸åœ¨å¿«ç…§é‡Œã€‘çš„æ–°é¡¹ç›® ---
                    try:
                        check_res = client.fs_files({'cid': save_path_cid, 'limit': 50, 'o': 'user_ptime', 'asc': 0})
                        if check_res.get('data'):
                            for item in check_res['data']:
                                current_pc = item.get('pc')
                                # å¦‚æœå‘ç°ä¸€ä¸ª pick_code ä¸åœ¨æ—§é›†åˆé‡Œï¼Œè¯´æ˜æ˜¯æ–°ç”Ÿæˆçš„
                                if current_pc and (current_pc not in existing_pick_codes):
                                    item_name = item.get('n', 'æœªçŸ¥')
                                    logger.info(f"  âœ… [ç¬¬{i+1}æ¬¡æ£€æŸ¥] å‘ç°æ–°é¡¹ç›®: {item_name}")
                                    success_found = True
                                    break
                        if success_found:
                            break
                    except Exception as e:
                        pass # ç½‘ç»œæ³¢åŠ¨å¿½ç•¥

                    # --- B. è¾…åŠ©æ£€æŸ¥ï¼šä»»åŠ¡æ˜¯å¦æŒ‚äº† ---
                    try:
                        list_resp = client.offline_list(page=1)
                        tasks = list_resp.get('tasks', [])
                        for task in tasks[:10]:
                            if info_hash and task.get('info_hash') == info_hash:
                                if task.get('status') == -1:
                                    try: client.offline_delete([task.get('info_hash')])
                                    except: pass
                                    raise Exception("115ä»»åŠ¡çŠ¶æ€å˜ä¸º[ä¸‹è½½å¤±è´¥]")
                    except Exception as task_err:
                        if "ä¸‹è½½å¤±è´¥" in str(task_err): raise task_err
                        pass

                if success_found:
                    logger.info(f"  âœ… 115 ç¦»çº¿æˆåŠŸ: {title}")
                    return True
                else:
                    # è¶…æ—¶æœªå‘ç°æ–°æ–‡ä»¶
                    try: 
                        if info_hash: client.offline_delete([info_hash])
                    except: pass
                    
                    logger.warning(f"  âŒ æœªåœ¨ç›®å½•å‘ç°æ–°é¡¹ç›®ï¼Œåˆ¤å®šä¸ºæ­»é“¾æˆ–ä¸‹è½½è¿‡æ…¢")
                    raise Exception("èµ„æºæ— æ•ˆï¼Œè¯·æ¢ä¸ªæºè¯•è¯•")

            else:
                err = resp.get('error_msg') or resp.get('msg') or 'æœªçŸ¥é”™è¯¯'
                if 'å·²å­˜åœ¨' in str(err):
                    logger.info(f"  âœ… ä»»åŠ¡å·²å­˜åœ¨: {title}")
                    return True
                raise Exception(f"ç¦»çº¿å¤±è´¥: {err}")

    except Exception as e:
        logger.error(f"  âœ 115 æ¨é€å¼‚å¸¸: {e}")
        if "Login" in str(e) or "cookie" in str(e).lower():
            raise Exception("115 Cookie æ— æ•ˆ")
        raise e

def get_115_account_info():
    """
    æç®€çŠ¶æ€æ£€æŸ¥ï¼šåªéªŒè¯ Cookie æ˜¯å¦æœ‰æ•ˆï¼Œä¸è·å–ä»»ä½•è¯¦æƒ…
    """
    if P115Client is None:
        raise Exception("æœªå®‰è£… p115client")
        
    config = get_config()
    cookies = config.get('p115_cookies')
    
    if not cookies:
        raise Exception("æœªé…ç½® Cookies")
        
    try:
        client = P115Client(cookies)
        
        # å°è¯•åˆ—å‡º 1 ä¸ªæ–‡ä»¶ï¼Œè¿™æ˜¯éªŒè¯ Cookie æœ€å¿«æœ€å‡†çš„æ–¹æ³•
        resp = client.fs_files({'limit': 1})
        
        if not resp.get('state'):
            raise Exception("Cookie å·²å¤±æ•ˆ")
            
        # åªè¦æ²¡æŠ¥é”™ï¼Œå°±æ˜¯æœ‰æ•ˆ
        return {
            "valid": True,
            "msg": "Cookie çŠ¶æ€æ­£å¸¸ï¼Œå¯æ­£å¸¸æ¨é€"
        }

    except Exception as e:
        # logger.error(f"115 çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}") # å«Œçƒ¦å¯ä»¥æ³¨é‡Šæ‰æ—¥å¿—
        raise Exception("Cookie æ— æ•ˆæˆ–ç½‘ç»œä¸é€š")

def handle_push_request(link, title):
    """
    ç»Ÿä¸€æ¨é€å…¥å£
    """
    # 1. æ¨é€åˆ° 115 (å¦‚æœå¤±è´¥æˆ–æ­»é“¾ï¼Œè¿™é‡Œä¼šç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸­æ–­æµç¨‹)
    push_to_115(link, title)
    
    # 2. 115 æˆåŠŸåï¼Œé€šçŸ¥ CMS æ•´ç†
    # (è¿™ä¸ªå‡½æ•°å†…éƒ¨ä¼šæ£€æŸ¥æ˜¯å¦æœ‰é…ç½®ï¼Œæ²¡é…ç½®å°±é™é»˜è·³è¿‡)
    notify_cms_scan()
    
    return True

def auto_download_best_resource(tmdb_id, media_type, title, season_number=None):
    """
    [è‡ªåŠ¨ä»»åŠ¡ä¸“ç”¨] æœç´¢å¹¶ä¸‹è½½æœ€ä½³èµ„æº
    :param season_number: å­£å· (ä»… media_type='tv' æ—¶æœ‰æ•ˆ)
    """
    try:
        config = get_config()
        if not config.get('api_key'):
            logger.warning("NULLBR æœªé…ç½® API Keyï¼Œæ— æ³•æ‰§è¡Œè‡ªåŠ¨å…œåº•ã€‚")
            return False

        priority_sources = ['115', 'magnet', 'ed2k']
        user_enabled = config.get('enabled_sources', priority_sources)
        
        # æ„é€ æ—¥å¿—æ ‡é¢˜
        log_title = title
        if media_type == 'tv' and season_number:
            log_title = f"{title} S{season_number}"

        logger.info(f"  âœ [è‡ªåŠ¨ä»»åŠ¡] å¼€å§‹æœç´¢èµ„æº: {log_title} (ID: {tmdb_id})")

        for source in priority_sources:
            if source not in user_enabled: continue
            if media_type == 'tv' and source == 'ed2k': continue

            # â˜…â˜…â˜… ä¿®æ”¹ï¼šé€ä¼  season_number â˜…â˜…â˜…
            resources = fetch_resource_list(tmdb_id, media_type, specific_source=source, season_number=season_number)
            
            if not resources:
                continue

            logger.info(f"  âœ [{source.upper()}] æ‰¾åˆ° {len(resources)} ä¸ªèµ„æºï¼Œå¼€å§‹å°è¯•æ¨é€...")

            for index, res in enumerate(resources):
                try:
                    logger.info(f"  ğŸ‘‰ å°è¯•ç¬¬ {index + 1} ä¸ªèµ„æº: {res['title']}")
                    
                    # è°ƒç”¨ç»Ÿä¸€æ¨é€å…¥å£ (115 -> CMS Notify)
                    handle_push_request(res['link'], title)
                    
                    logger.info(f"  âœ… èµ„æºæ¨é€æˆåŠŸï¼Œåœæ­¢åç»­å°è¯•ã€‚")
                    return True
                    
                except Exception as e:
                    logger.warning(f"  âŒ ç¬¬ {index + 1} ä¸ªèµ„æºæ¨é€å¤±è´¥: {e}")
                    logger.info("  ğŸ”„ æ­£åœ¨å°è¯•ä¸‹ä¸€ä¸ªèµ„æº...")
                    continue
            
            logger.info(f"  âš ï¸ [{source.upper()}] æ‰€æœ‰èµ„æºå‡å°è¯•å¤±è´¥ï¼Œåˆ‡æ¢ä¸‹ä¸€æº...")

        logger.info(f"  âŒ æ‰€æœ‰æºçš„æ‰€æœ‰èµ„æºå‡å°è¯•å¤±è´¥: {log_title}")
        return False

    except Exception as e:
        logger.error(f"  âœ NULLBR è‡ªåŠ¨å…œåº•å¤±è´¥: {e}")
        return False